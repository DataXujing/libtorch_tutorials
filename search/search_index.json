{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u524d\u8a00 \u7b2c\u4e00\u7ae0 \u5f00\u53d1\u73af\u5883\u642d\u5efa(vs,opencv,libtorch) \u7b2c\u4e8c\u7ae0 \u5f20\u91cf\u7684\u5e38\u89c4\u64cd\u4f5c \u7b2c\u4e09\u7ae0 \u6a21\u578b\u642d\u5efa \u7b2c\u56db\u7ae0 \u6570\u636e\u52a0\u8f7d\u6a21\u5757 \u7b2c\u4e94\u7ae0 \u5206\u7c7b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u7b2c\u516d\u7ae0 \u5206\u5272\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u7b2c\u4e03\u7ae0 \u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u7b2c\u516b\u7ae0 libtorch\u90e8\u7f72\u4f8b\u5b50 \u7b2c\u4e5d\u7ae0 \u603b\u7ed3\u548c\u5c55\u671b","title":"\u524d\u8a00"},{"location":"#_1","text":"\u7b2c\u4e00\u7ae0 \u5f00\u53d1\u73af\u5883\u642d\u5efa(vs,opencv,libtorch) \u7b2c\u4e8c\u7ae0 \u5f20\u91cf\u7684\u5e38\u89c4\u64cd\u4f5c \u7b2c\u4e09\u7ae0 \u6a21\u578b\u642d\u5efa \u7b2c\u56db\u7ae0 \u6570\u636e\u52a0\u8f7d\u6a21\u5757 \u7b2c\u4e94\u7ae0 \u5206\u7c7b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u7b2c\u516d\u7ae0 \u5206\u5272\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u7b2c\u4e03\u7ae0 \u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u7b2c\u516b\u7ae0 libtorch\u90e8\u7f72\u4f8b\u5b50 \u7b2c\u4e5d\u7ae0 \u603b\u7ed3\u548c\u5c55\u671b","title":"\u524d\u8a00 "},{"location":"about/","text":"\u5173\u4e8e\u6211 \u5f90\u9759 \uff1a\uff08 dataxujing \uff09 \u5c0f\u767d\u4e00\u4e2a\uff0cAI\u56fe\u50cf\u7b97\u6cd5\u7814\u53d1\u5de5\u7a0b\u5e08\uff0c\u6570\u636e\u79d1\u5b66\u7231\u597d\u8005\uff0c\u559c\u6b22R, Python, C++ \u5173\u6ce8\u673a\u5668\u5b66\u4e60\uff0c\u6df1\u5ea6\u5b66\u4e60\u7b49AI\u9886\u57df\uff1b \u559c\u6b22\u7f51\u7edc\u722c\u866b\uff0c\u5173\u6ce8\u524d\u7aef\u53ef\u89c6\u5316; \u5bf9ASR(\u8bed\u97f3\u8bc6\u522b)\uff0cNLP(\u81ea\u7136\u8bed\u8a00\u5904\u7406)\uff0cCV(\u8ba1\u7b97\u673a\u89c6\u89c9)\u5747\u6709\u6d89\u730e\uff1b \u76ee\u524d\u4ece\u4e8b\u533b\u7597\u5f71\u50cfAI\u7b97\u6cd5\u7684\u7814\u7a76\u548c\u843d\u5730\u5de5\u4f5c\u3002 \u8be5\u6559\u7a0b\u4e3b\u8981\u662f\u5bf9\u53e6\u4e00\u4e2a\u5927\u4f6c\u7684libtorch\u6559\u7a0b\u7684\u6574\u7406\uff0c\u611f\u8c22\u5927\u4f6c\u5f00\u6e90\uff1a https://github.com/AllentDan","title":"\u5173\u4e8e"},{"location":"about/#_1","text":"\u5f90\u9759 \uff1a\uff08 dataxujing \uff09 \u5c0f\u767d\u4e00\u4e2a\uff0cAI\u56fe\u50cf\u7b97\u6cd5\u7814\u53d1\u5de5\u7a0b\u5e08\uff0c\u6570\u636e\u79d1\u5b66\u7231\u597d\u8005\uff0c\u559c\u6b22R, Python, C++ \u5173\u6ce8\u673a\u5668\u5b66\u4e60\uff0c\u6df1\u5ea6\u5b66\u4e60\u7b49AI\u9886\u57df\uff1b \u559c\u6b22\u7f51\u7edc\u722c\u866b\uff0c\u5173\u6ce8\u524d\u7aef\u53ef\u89c6\u5316; \u5bf9ASR(\u8bed\u97f3\u8bc6\u522b)\uff0cNLP(\u81ea\u7136\u8bed\u8a00\u5904\u7406)\uff0cCV(\u8ba1\u7b97\u673a\u89c6\u89c9)\u5747\u6709\u6d89\u730e\uff1b \u76ee\u524d\u4ece\u4e8b\u533b\u7597\u5f71\u50cfAI\u7b97\u6cd5\u7684\u7814\u7a76\u548c\u843d\u5730\u5de5\u4f5c\u3002 \u8be5\u6559\u7a0b\u4e3b\u8981\u662f\u5bf9\u53e6\u4e00\u4e2a\u5927\u4f6c\u7684libtorch\u6559\u7a0b\u7684\u6574\u7406\uff0c\u611f\u8c22\u5927\u4f6c\u5f00\u6e90\uff1a https://github.com/AllentDan","title":"\u5173\u4e8e\u6211"},{"location":"chapter1/","text":"\u7b2c\u4e00\u7ae0 \u5f00\u53d1\u73af\u5883\u7684\u642d\u5efa 1. visual studio\u7684\u5b89\u88c5\u548c\u914d\u7f6e visual studio\u7248\u672c\u6700\u597d\u57282015\u53ca\u4ee5\u4e0a\uff0c\u672c\u6587\u4f7f\u7528\u7684\u7248\u672c\u662f2017\u3002\u4e0b\u8f7d\u94fe\u63a5\uff1a https://docs.microsoft.com/zh-cn/visualstudio/productinfo/vs2017-system-requirements-vs \u5177\u4f53\u5b89\u88c5\u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003 https://www.jianshu.com/p/320aefbc582d .\u6253\u5f00\u4e0b\u8f7d\u94fe\u63a5\u4e0b\u8f7d\u793e\u533a\u7248\u672c\u5373\u53ef\uff0c\u5b89\u88c5\u65f6\u5bf9\u4e8ec++\u7a0b\u5e8f\u8bbe\u8ba1\u53ea\u9700\u5b89\u88c5\u5bf9\u5e94\u90e8\u5206\uff0c\u52fe\u9009\u5982\u4e0b \u5173\u4e8evisual studio\u7684\u5b89\u88c5\u7f51\u4e0a\u6709\u592a\u591a\u6559\u7a0b\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002 2. win10 \u4e0bopencv\u7684\u5b89\u88c5 1.opencv\u7684\u4e0b\u8f7d \u8fdb\u5165opencv\u5b98\u7f51 https://opencv.org/releases.html# \u9009\u62e9\u5bf9\u5e94\u7248\u672c\u4e0b\u8f7d. \u8fd0\u884c\u4e0b\u8f7d\u540e\u7684exe\u6587\u4ef6\uff0c\u5373\u53ef\u89e3\u538b\u6587\u4ef6\uff0c\u76ee\u5f55\u4e0b\u5c06\u4f1a\u51fa\u73b0\u540d\u4e3aopencv\u7684\u6587\u4ef6\u5939. 2.\u73af\u5883\u53d8\u91cf\u7684\u8bbe\u7f6e \u53f3\u51fb\u201c\u6b64\u7535\u8111\u201d\uff0c\u5de6\u51fb\u201c\u5c5e\u6027\u201d\uff0c\u201c\u9ad8\u7ea7\u7cfb\u7edf\u914d\u7f6e\u201d\uff0c\u201c\u73af\u5883\u53d8\u91cf\u201d\uff0c\u7f16\u8f91\u540d\u4e3a\u201cPath\u201d\u7684\u73af\u5883\u53d8\u91cf\uff0c\u65b0\u5efa\u4ee5\u4e0b\u8def\u5f84\u540e\u70b9\u51fb\u786e\u5b9a\uff0c\u91cd\u542f\u8ba1\u7b97\u673a\u3002 3.VS2017\u4e2d\u914d\u7f6eopencv \u521b\u5efa\u5de5\u7a0b\uff0c\u5de5\u7a0b\u4e0b\u5efa\u4e00\u4e2a\u6e90\u6587\u4ef6 \u56e0\u4e3a\u6211\u662f64\u4f4d\u673a\uff0c\u6240\u4ee5\u9009\u62e9Debug x64 \u53f3\u952e\u9879\u76ee\uff0c\u4f9d\u6b21\u70b9\u51fb\u201c\u5c5e\u6027\u201d\uff0c\u201cVC++\u76ee\u5f55\u201d\uff0c\u201c\u5305\u542b\u76ee\u5f55\u201d\uff0c\u5c06\u4e0b\u56fe\u8def\u5f84\u6dfb\u52a0\u8fdb\u53bb\u540e\u70b9\u51fb\u786e\u5b9a \u540c\u4e0a\u8ff0\u6b65\u9aa4\uff0c\u5728\u201c\u5e93\u76ee\u5f55\u201d\uff0c\u5c06\u4e0b\u56fe\u8def\u5f84\u6dfb\u52a0\u8fdb\u53bb\u540e\u70b9\u51fb\u786e\u5b9a \u70b9\u51fb\u201cVC++\u76ee\u5f55\u201d\u4e0b\u65b9\u7684\u201c\u94fe\u63a5\u5668\u201d\uff0c\u201c\u8f93\u5165\u201d\uff0c\u201c\u9644\u52a0\u4f9d\u8d56\u9879\u201d\uff0c\u6dfb\u52a0dll\u6587\u4ef6\u540e\u70b9\u51fb\u786e\u5b9a 4.\u6d4b\u8bd5opencv\u662f\u5426\u5b89\u88c5\u6210\u529f \u5728\u65b0\u5efa\u7684\u9879\u76ee\u4e2d\u8f93\u5165\u5982\u4e0bC++\u4ee3\u7801(cv_demo\u9879\u76ee) #include <opencv2/opencv.hpp> #include <iostream> using namespace cv; using namespace std; int main() { Mat image = imread(\"X:\\\\medicon\\\\medicon_book\\\\TensorFlow_book\\\\libtorch-tutorials\\\\libtorch-tutorials\\\\docs\\\\img\\\\ch1\\\\Lenna.jpg\"); //\u56fe\u7247\u8def\u5f84 double scale = 0.9; //\u663e\u793a\u7684\u56fe\u50cf\u5927\u5c0f\u4e3a\u539f\u6765\u5927\u5c0f\u76840.3 Size dsize = Size(image.cols*scale, image.rows*scale); Mat image2 = Mat(dsize, CV_32S); resize(image, image2, dsize); imshow(\"Read Image\", image2); waitKey(0); return 0; \u5b89\u88c5\u914d\u7f6e\u6210\u529f\uff01 3. win10\u4e0b libtorch\u7684\u5b89\u88c5\u548c\u914d\u7f6e 1.libtorch\u7684\u4e0b\u8f7d \u7531\u4e8e\u5386\u53f2\u539f\u56e0\u7535\u8111\u5b89\u88c5\u7684cuda\u7248\u672c\u662f9.0\uff0c\u8fd9\u91cc\u6d4b\u8bd5cuda9.0\u4e0blibtorch\u7684\u5b89\u88c5\u548c\u914d\u7f6e\uff0c\u4e0b\u8f7d\u5730\u5740\uff1a https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-1.0.0.zip \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0clibtorch\u7684\u7248\u672c\u5e94\u8be5\u5927\u4e8e\u7b49\u4e8epytorch\u7684\u8bad\u7ec3\u6846\u67b6\u7684\u7248\u672c\uff0c\u5982\u679c\u4e0b\u8f7d\u6700\u65b0\u7684libtorch\uff0c\u53ef\u4ee5\u5728\u5b98\u7f51\u4e0b\u8f7d 2.\u89e3\u538b\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf \u89e3\u538b\u540e\uff0c\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u53f3\u952e\u6211\u7684\u7535\u8111->\u5c5e\u6027->\u9ad8\u7ea7\u7cfb\u7edf\u8bbe\u7f6e->\u9ad8\u7ea7\u4e2d\u7684\u73af\u5883\u53d8\u91cf->\u70b9\u51fb\u7cfb\u7edf\u53d8\u91cf\u4e2d\u7684Path->\u6dfb\u52a0dll\u8def\u5f84\uff1a \u65b0\u5efa\u9879\u76ee\u914d\u7f6e\u73af\u5883 \u6dfb\u52a0\u5305\u542b\u76ee\u5f55 \u8fd9\u4e24\u4e2a\u5934\u6587\u4ef6\u8def\u5f84\u4e2d\u5e38\u7528\u7684\u5934\u6587\u4ef6\u5206\u522b\u662f\uff1a #include \"torch/script.h\" \u548c #include \"torch/torch.h\" \u7f51\u4e0a\u5f88\u591a\u7684\u793a\u4f8b\u4ee3\u7801\u6dfb\u52a0\u7684\u662f\u7b2c\u4e8c\u4e2a\u5934\u6587\u4ef6\uff0c\u4f46\u662f\u4e00\u822c\u90fd\u6ca1\u6709\u8bf4\u8fd9\u4e2a\u5934\u6587\u4ef6\u6240\u5728\u8def\u5f84\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u627e\u4e0d\u5230\u5f88\u591a\u5b9a\u4e49\uff0c\u8fd9\u4e2a\u95ee\u9898\u7f51\u4e0a\u63d0\u5230\u7684\u5f88\u5c11\uff0c\u6240\u4ee5\u5728\u8fd9\u91cc\u7279\u522b\u8bf4\u660e\u4e00\u4e0b\u3002 \u8bbe\u7f6e\u94fe\u63a5\u5e93 \u6dfb\u52a0libtorch\u5305\u542blib\u7684\u6587\u4ef6\u5939\u8def\u5f84 \u6dfb\u52a0\u6240\u9700\u8981\u7684lib\u6587\u4ef6 # \u6211\u4eec\u5b89\u88c5\u6700\u65b0\u7684CPU Debug\u7248\u672c\uff01 opencv_world452d.lib caffe2_detectron_ops.lib pytorch_jni.lib caffe2_module_test_dynamic.lib c10d.lib torch.lib torch_cpu.lib fbjni.lib mkldnn.lib dnnl.lib c10.lib gloo.lib fbgemm.lib asmjit.lib XNNPACK.lib libprotocd.lib libprotobufd.lib pthreadpool.lib libprotobuf-lited.lib cpuinfo.lib clog.lib \u4fee\u6539C/C++ --> \u5e38\u89c4 --> SDL\u68c0\u67e5\uff08\u5426\uff09 \u4fee\u6539C/C++ --> \u8bed\u8a00 --> \u7b26\u5408\u6a21\u5f0f(\u5426) \u8fd0\u884c\u6d4b\u8bd5\u4ee3\u7801 #include \"torch/torch.h\" #include \"torch/script.h\" # include <iostream> int main() { torch::Tensor output = torch::randn({ 3,2 }); std::cout << output; return 0; } libtorch\u914d\u7f6e\u6210\u529f\uff01 4.ResNet32\u5206\u7c7b\u7f51\u7edc libtorch\u90e8\u7f72\u6d4b\u8bd5 python\u4e0b\u8bad\u7ec3\u7684\u6a21\u578b\u8f6ctorchscript from torchvision.models import resnet34 import torch.nn.functional as F import torch.nn as nn import torch import cv2 #\u8bfb\u53d6\u4e00\u5f20\u56fe\u7247\uff0c\u5e76\u8f6c\u6362\u6210[1,3,224,224]\u7684float\u5f20\u91cf\u5e76\u5f52\u4e00\u5316 image = cv2.imread(\"Lenna.jpg\") image = cv2.resize(image,(224,224)) input_tensor = torch.tensor(image).permute(2,0,1).unsqueeze(0).float()/225.0 #\u5b9a\u4e49\u5e76\u52a0\u8f7dresnet34\u6a21\u578b\u5728imagenet\u9884\u8bad\u7ec3\u7684\u6743\u91cd model = resnet34(pretrained=True) model.eval() #\u67e5\u770b\u6a21\u578b\u9884\u6d4b\u8be5\u4ed8\u56fe\u7684\u7ed3\u679c output = model(input_tensor) output = F.softmax(output,1) print(\"\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e3a\u7b2c{}\u7c7b\uff0c\u7f6e\u4fe1\u5ea6\u4e3a{}\".format(torch.argmax(output),output.max())) #\u751f\u6210pt\u6a21\u578b\uff0c\u6309\u7167\u5b98\u7f51\u6765\u5373\u53ef model=model.to(torch.device(\"cpu\")) model.eval() var=torch.ones((1,3,224,224)) traced_script_module = torch.jit.trace(model, var) traced_script_module.save(\"resnet34.pt\") python\u4e0b\u7684\u8bc6\u522b\u7ed3\u679c #include<opencv2/opencv.hpp> #include <torch/torch.h> #include <torch/script.h> int main() { //\u5b9a\u4e49\u4f7f\u7528cuda //auto device = torch::Device(torch::kCUDA, 0); //\u8bfb\u53d6\u56fe\u7247 auto image = cv::imread(\"D:\\\\libtorch_install\\\\libtorch_code\\\\resnet32_demo\\\\python\\\\lenna.jpg\"); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(224, 224)); //\u8f6c\u6210\u5f20\u91cf auto input_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat32) / 225.0; //\u52a0\u8f7d\u6a21\u578b auto model = torch::jit::load(\"D:\\\\libtorch_install\\\\libtorch_code\\\\resnet32_demo\\\\python\\\\resnet34.pt\"); //model.to(device); model.eval(); //\u524d\u5411\u4f20\u64ad //auto output = model.forward({ input_tensor.to(device) }).toTensor(); auto output = model.forward({ input_tensor }).toTensor(); output = torch::softmax(output, 1); std::cout << \"\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e3a\u7b2c\" << torch::argmax(output) << \"\u7c7b\uff0c\u7f6e\u4fe1\u5ea6\u4e3a\" << output.max() << std::endl; return 0; } OK\uff0c\u6a21\u578b\u8bc6\u522b\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\uff0c\u6210\u529f\uff01","title":"\u7b2c\u4e00\u7ae0 \u5f00\u53d1\u73af\u5883\u642d\u5efa(vs,opencv,libtorch)"},{"location":"chapter1/#_1","text":"","title":"\u7b2c\u4e00\u7ae0 \u5f00\u53d1\u73af\u5883\u7684\u642d\u5efa"},{"location":"chapter1/#1-visual-studio","text":"visual studio\u7248\u672c\u6700\u597d\u57282015\u53ca\u4ee5\u4e0a\uff0c\u672c\u6587\u4f7f\u7528\u7684\u7248\u672c\u662f2017\u3002\u4e0b\u8f7d\u94fe\u63a5\uff1a https://docs.microsoft.com/zh-cn/visualstudio/productinfo/vs2017-system-requirements-vs \u5177\u4f53\u5b89\u88c5\u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003 https://www.jianshu.com/p/320aefbc582d .\u6253\u5f00\u4e0b\u8f7d\u94fe\u63a5\u4e0b\u8f7d\u793e\u533a\u7248\u672c\u5373\u53ef\uff0c\u5b89\u88c5\u65f6\u5bf9\u4e8ec++\u7a0b\u5e8f\u8bbe\u8ba1\u53ea\u9700\u5b89\u88c5\u5bf9\u5e94\u90e8\u5206\uff0c\u52fe\u9009\u5982\u4e0b \u5173\u4e8evisual studio\u7684\u5b89\u88c5\u7f51\u4e0a\u6709\u592a\u591a\u6559\u7a0b\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002","title":"1. visual studio\u7684\u5b89\u88c5\u548c\u914d\u7f6e"},{"location":"chapter1/#2-win10-opencv","text":"1.opencv\u7684\u4e0b\u8f7d \u8fdb\u5165opencv\u5b98\u7f51 https://opencv.org/releases.html# \u9009\u62e9\u5bf9\u5e94\u7248\u672c\u4e0b\u8f7d. \u8fd0\u884c\u4e0b\u8f7d\u540e\u7684exe\u6587\u4ef6\uff0c\u5373\u53ef\u89e3\u538b\u6587\u4ef6\uff0c\u76ee\u5f55\u4e0b\u5c06\u4f1a\u51fa\u73b0\u540d\u4e3aopencv\u7684\u6587\u4ef6\u5939. 2.\u73af\u5883\u53d8\u91cf\u7684\u8bbe\u7f6e \u53f3\u51fb\u201c\u6b64\u7535\u8111\u201d\uff0c\u5de6\u51fb\u201c\u5c5e\u6027\u201d\uff0c\u201c\u9ad8\u7ea7\u7cfb\u7edf\u914d\u7f6e\u201d\uff0c\u201c\u73af\u5883\u53d8\u91cf\u201d\uff0c\u7f16\u8f91\u540d\u4e3a\u201cPath\u201d\u7684\u73af\u5883\u53d8\u91cf\uff0c\u65b0\u5efa\u4ee5\u4e0b\u8def\u5f84\u540e\u70b9\u51fb\u786e\u5b9a\uff0c\u91cd\u542f\u8ba1\u7b97\u673a\u3002 3.VS2017\u4e2d\u914d\u7f6eopencv \u521b\u5efa\u5de5\u7a0b\uff0c\u5de5\u7a0b\u4e0b\u5efa\u4e00\u4e2a\u6e90\u6587\u4ef6 \u56e0\u4e3a\u6211\u662f64\u4f4d\u673a\uff0c\u6240\u4ee5\u9009\u62e9Debug x64 \u53f3\u952e\u9879\u76ee\uff0c\u4f9d\u6b21\u70b9\u51fb\u201c\u5c5e\u6027\u201d\uff0c\u201cVC++\u76ee\u5f55\u201d\uff0c\u201c\u5305\u542b\u76ee\u5f55\u201d\uff0c\u5c06\u4e0b\u56fe\u8def\u5f84\u6dfb\u52a0\u8fdb\u53bb\u540e\u70b9\u51fb\u786e\u5b9a \u540c\u4e0a\u8ff0\u6b65\u9aa4\uff0c\u5728\u201c\u5e93\u76ee\u5f55\u201d\uff0c\u5c06\u4e0b\u56fe\u8def\u5f84\u6dfb\u52a0\u8fdb\u53bb\u540e\u70b9\u51fb\u786e\u5b9a \u70b9\u51fb\u201cVC++\u76ee\u5f55\u201d\u4e0b\u65b9\u7684\u201c\u94fe\u63a5\u5668\u201d\uff0c\u201c\u8f93\u5165\u201d\uff0c\u201c\u9644\u52a0\u4f9d\u8d56\u9879\u201d\uff0c\u6dfb\u52a0dll\u6587\u4ef6\u540e\u70b9\u51fb\u786e\u5b9a 4.\u6d4b\u8bd5opencv\u662f\u5426\u5b89\u88c5\u6210\u529f \u5728\u65b0\u5efa\u7684\u9879\u76ee\u4e2d\u8f93\u5165\u5982\u4e0bC++\u4ee3\u7801(cv_demo\u9879\u76ee) #include <opencv2/opencv.hpp> #include <iostream> using namespace cv; using namespace std; int main() { Mat image = imread(\"X:\\\\medicon\\\\medicon_book\\\\TensorFlow_book\\\\libtorch-tutorials\\\\libtorch-tutorials\\\\docs\\\\img\\\\ch1\\\\Lenna.jpg\"); //\u56fe\u7247\u8def\u5f84 double scale = 0.9; //\u663e\u793a\u7684\u56fe\u50cf\u5927\u5c0f\u4e3a\u539f\u6765\u5927\u5c0f\u76840.3 Size dsize = Size(image.cols*scale, image.rows*scale); Mat image2 = Mat(dsize, CV_32S); resize(image, image2, dsize); imshow(\"Read Image\", image2); waitKey(0); return 0; \u5b89\u88c5\u914d\u7f6e\u6210\u529f\uff01","title":"2. win10 \u4e0bopencv\u7684\u5b89\u88c5"},{"location":"chapter1/#3-win10-libtorch","text":"1.libtorch\u7684\u4e0b\u8f7d \u7531\u4e8e\u5386\u53f2\u539f\u56e0\u7535\u8111\u5b89\u88c5\u7684cuda\u7248\u672c\u662f9.0\uff0c\u8fd9\u91cc\u6d4b\u8bd5cuda9.0\u4e0blibtorch\u7684\u5b89\u88c5\u548c\u914d\u7f6e\uff0c\u4e0b\u8f7d\u5730\u5740\uff1a https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-1.0.0.zip \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0clibtorch\u7684\u7248\u672c\u5e94\u8be5\u5927\u4e8e\u7b49\u4e8epytorch\u7684\u8bad\u7ec3\u6846\u67b6\u7684\u7248\u672c\uff0c\u5982\u679c\u4e0b\u8f7d\u6700\u65b0\u7684libtorch\uff0c\u53ef\u4ee5\u5728\u5b98\u7f51\u4e0b\u8f7d 2.\u89e3\u538b\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf \u89e3\u538b\u540e\uff0c\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u53f3\u952e\u6211\u7684\u7535\u8111->\u5c5e\u6027->\u9ad8\u7ea7\u7cfb\u7edf\u8bbe\u7f6e->\u9ad8\u7ea7\u4e2d\u7684\u73af\u5883\u53d8\u91cf->\u70b9\u51fb\u7cfb\u7edf\u53d8\u91cf\u4e2d\u7684Path->\u6dfb\u52a0dll\u8def\u5f84\uff1a \u65b0\u5efa\u9879\u76ee\u914d\u7f6e\u73af\u5883 \u6dfb\u52a0\u5305\u542b\u76ee\u5f55 \u8fd9\u4e24\u4e2a\u5934\u6587\u4ef6\u8def\u5f84\u4e2d\u5e38\u7528\u7684\u5934\u6587\u4ef6\u5206\u522b\u662f\uff1a #include \"torch/script.h\" \u548c #include \"torch/torch.h\" \u7f51\u4e0a\u5f88\u591a\u7684\u793a\u4f8b\u4ee3\u7801\u6dfb\u52a0\u7684\u662f\u7b2c\u4e8c\u4e2a\u5934\u6587\u4ef6\uff0c\u4f46\u662f\u4e00\u822c\u90fd\u6ca1\u6709\u8bf4\u8fd9\u4e2a\u5934\u6587\u4ef6\u6240\u5728\u8def\u5f84\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u627e\u4e0d\u5230\u5f88\u591a\u5b9a\u4e49\uff0c\u8fd9\u4e2a\u95ee\u9898\u7f51\u4e0a\u63d0\u5230\u7684\u5f88\u5c11\uff0c\u6240\u4ee5\u5728\u8fd9\u91cc\u7279\u522b\u8bf4\u660e\u4e00\u4e0b\u3002 \u8bbe\u7f6e\u94fe\u63a5\u5e93 \u6dfb\u52a0libtorch\u5305\u542blib\u7684\u6587\u4ef6\u5939\u8def\u5f84 \u6dfb\u52a0\u6240\u9700\u8981\u7684lib\u6587\u4ef6 # \u6211\u4eec\u5b89\u88c5\u6700\u65b0\u7684CPU Debug\u7248\u672c\uff01 opencv_world452d.lib caffe2_detectron_ops.lib pytorch_jni.lib caffe2_module_test_dynamic.lib c10d.lib torch.lib torch_cpu.lib fbjni.lib mkldnn.lib dnnl.lib c10.lib gloo.lib fbgemm.lib asmjit.lib XNNPACK.lib libprotocd.lib libprotobufd.lib pthreadpool.lib libprotobuf-lited.lib cpuinfo.lib clog.lib \u4fee\u6539C/C++ --> \u5e38\u89c4 --> SDL\u68c0\u67e5\uff08\u5426\uff09 \u4fee\u6539C/C++ --> \u8bed\u8a00 --> \u7b26\u5408\u6a21\u5f0f(\u5426) \u8fd0\u884c\u6d4b\u8bd5\u4ee3\u7801 #include \"torch/torch.h\" #include \"torch/script.h\" # include <iostream> int main() { torch::Tensor output = torch::randn({ 3,2 }); std::cout << output; return 0; } libtorch\u914d\u7f6e\u6210\u529f\uff01","title":"3. win10\u4e0b libtorch\u7684\u5b89\u88c5\u548c\u914d\u7f6e"},{"location":"chapter1/#4resnet32-libtorch","text":"python\u4e0b\u8bad\u7ec3\u7684\u6a21\u578b\u8f6ctorchscript from torchvision.models import resnet34 import torch.nn.functional as F import torch.nn as nn import torch import cv2 #\u8bfb\u53d6\u4e00\u5f20\u56fe\u7247\uff0c\u5e76\u8f6c\u6362\u6210[1,3,224,224]\u7684float\u5f20\u91cf\u5e76\u5f52\u4e00\u5316 image = cv2.imread(\"Lenna.jpg\") image = cv2.resize(image,(224,224)) input_tensor = torch.tensor(image).permute(2,0,1).unsqueeze(0).float()/225.0 #\u5b9a\u4e49\u5e76\u52a0\u8f7dresnet34\u6a21\u578b\u5728imagenet\u9884\u8bad\u7ec3\u7684\u6743\u91cd model = resnet34(pretrained=True) model.eval() #\u67e5\u770b\u6a21\u578b\u9884\u6d4b\u8be5\u4ed8\u56fe\u7684\u7ed3\u679c output = model(input_tensor) output = F.softmax(output,1) print(\"\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e3a\u7b2c{}\u7c7b\uff0c\u7f6e\u4fe1\u5ea6\u4e3a{}\".format(torch.argmax(output),output.max())) #\u751f\u6210pt\u6a21\u578b\uff0c\u6309\u7167\u5b98\u7f51\u6765\u5373\u53ef model=model.to(torch.device(\"cpu\")) model.eval() var=torch.ones((1,3,224,224)) traced_script_module = torch.jit.trace(model, var) traced_script_module.save(\"resnet34.pt\") python\u4e0b\u7684\u8bc6\u522b\u7ed3\u679c #include<opencv2/opencv.hpp> #include <torch/torch.h> #include <torch/script.h> int main() { //\u5b9a\u4e49\u4f7f\u7528cuda //auto device = torch::Device(torch::kCUDA, 0); //\u8bfb\u53d6\u56fe\u7247 auto image = cv::imread(\"D:\\\\libtorch_install\\\\libtorch_code\\\\resnet32_demo\\\\python\\\\lenna.jpg\"); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(224, 224)); //\u8f6c\u6210\u5f20\u91cf auto input_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat32) / 225.0; //\u52a0\u8f7d\u6a21\u578b auto model = torch::jit::load(\"D:\\\\libtorch_install\\\\libtorch_code\\\\resnet32_demo\\\\python\\\\resnet34.pt\"); //model.to(device); model.eval(); //\u524d\u5411\u4f20\u64ad //auto output = model.forward({ input_tensor.to(device) }).toTensor(); auto output = model.forward({ input_tensor }).toTensor(); output = torch::softmax(output, 1); std::cout << \"\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u4e3a\u7b2c\" << torch::argmax(output) << \"\u7c7b\uff0c\u7f6e\u4fe1\u5ea6\u4e3a\" << output.max() << std::endl; return 0; } OK\uff0c\u6a21\u578b\u8bc6\u522b\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\uff0c\u6210\u529f\uff01","title":"4.ResNet32\u5206\u7c7b\u7f51\u7edc libtorch\u90e8\u7f72\u6d4b\u8bd5"},{"location":"chapter2/","text":"\u7b2c\u4e8c\u7ae0 \u5f20\u91cf\u7684\u5e38\u89c4\u64cd\u4f5c ibtorch(pytorch c++)\u7684\u5927\u591a\u6570api\u548cpytorch\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\uff0clibtorch\u4e2d\u5f20\u91cf\u7684\u521d\u59cb\u5316\u4e5f\u548cpytorch\u4e2d\u7684\u7c7b\u4f3c\u3002\u672c\u6587\u4ecb\u7ecd\u56db\u79cd\u6df1\u5ea6\u56fe\u50cf\u7f16\u7a0b\u9700\u8981\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002 1.Tensor\u7684\u521d\u59cb\u5316 \u7b2c\u4e00\u79cd\uff0c\u56fa\u5b9a\u5c3a\u5bf8\u548c\u503c\u7684\u521d\u59cb\u5316\u3002 //\u5e38\u89c1\u56fa\u5b9a\u503c\u7684\u521d\u59cb\u5316\u65b9\u5f0f auto b = torch::zeros({3,4}); b = torch::ones({3,4}); b= torch::eye(4); b = torch::full({3,4},10); b = torch::tensor({33,22,11}); pytorch\u4e2d\u7528 [] \u8868\u793a\u5c3a\u5bf8\uff0c\u800ccpp\u4e2d\u7528 {} \u8868\u793a\u3002zeros\u4ea7\u751f\u503c\u5168\u4e3a0\u7684\u5f20\u91cf\u3002ones\u4ea7\u751f\u503c\u5168\u4e3a1\u7684\u5f20\u91cf\u3002eye\u4ea7\u751f\u5355\u4f4d\u77e9\u9635\u5f20\u91cf\u3002full\u4ea7\u751f\u6307\u5b9a\u503c\u548c\u5c3a\u5bf8\u7684\u5f20\u91cf\u3002 torch::tensor({}) \u4e5f\u53ef\u4ee5\u4ea7\u751f\u5f20\u91cf\uff0c\u6548\u679c\u548cpytorch\u7684 torch.Tensor([]) \u6216\u8005 torch.tensor([]) \u4e00\u6837\u3002 \u7b2c\u4e8c\u79cd\uff0c\u56fa\u5b9a\u5c3a\u5bf8\uff0c\u968f\u673a\u503c\u7684\u521d\u59cb\u5316\u65b9\u6cd5 //\u968f\u673a\u521d\u59cb\u5316 auto r = torch::rand({3,4}); r = torch::randn({3, 4}); r = torch::randint(0, 4,{3,3}); rand\u4ea7\u751f0-1\u4e4b\u95f4\u7684\u968f\u673a\u503c\uff0crandn\u53d6\u6b63\u6001\u5206\u5e03N(0,1)\u7684\u968f\u673a\u503c\uff0crandint\u53d6 [min,max) \u7684\u968f\u673a\u6574\u578b\u6570\u503c\u3002 \u7b2c\u4e09\u79cd\uff0c\u4ecec++\u7684\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u800c\u6765 int aa[10] = {3,4,6}; std::vector<float> aaaa = {3,4,6}; auto aaaaa = torch::from_blob(aa,{3},torch::kFloat); auto aaa = torch::from_blob(aaaa.data(),{3},torch::kFloat); pytorch\u53ef\u4ee5\u63a5\u53d7\u4ece\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u5982numpy\u548clist\u7684\u6570\u636e\u8f6c\u5316\u6210\u5f20\u91cf\u3002libtorch\u540c\u6837\u53ef\u4ee5\u63a5\u53d7\u5176\u4ed6\u6570\u636e\u6307\u9488\uff0c\u901a\u8fc7 from_blob \u51fd\u6570\u5373\u53ef\u8f6c\u6362\u3002\u8fd9\u4e2a\u65b9\u5f0f\u5728\u90e8\u7f72\u4e2d\u7ecf\u5e38\u7528\u5230\uff0c\u5982\u679c\u56fe\u50cf\u662fopencv\u52a0\u8f7d\u7684\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7 from_blob \u5c06\u56fe\u50cf\u6307\u9488\u8f6c\u6210\u5f20\u91cf\u3002 \u7b2c\u56db\u79cd\uff0c\u6839\u636e\u5df2\u6709\u5f20\u91cf\u521d\u59cb\u5316 auto b = torch::zeros({3,4}); auto d = torch::Tensor(b); d = torch::zeros_like(b); d = torch::ones_like(b); d = torch::rand_like(b,torch::kFloat); d = b.clone(); \u8fd9\u91cc\uff0c auto d = torch::Tensor(b) \u7b49\u4ef7\u4e8e auto d = b \uff0c\u4e24\u8005\u521d\u59cb\u5316\u7684\u5f20\u91cfd\u5747\u53d7\u539f\u5f20\u91cfb\u7684\u5f71\u54cd\uff0cb\u4e2d\u7684\u503c\u53d1\u751f\u6539\u53d8\uff0cd\u4e5f\u5c06\u53d1\u751f\u6539\u53d8\uff0c\u4f46\u662fb\u5982\u679c\u53ea\u662f\u5f20\u91cf\u53d8\u5f62\uff0cd\u5374\u4e0d\u4f1a\u8ddf\u7740\u53d8\u5f62\uff0c\u4ecd\u65e7\u4fdd\u6301\u521d\u59cb\u5316\u65f6\u7684\u5f62\u72b6\uff0c\u8fd9\u79cd\u8868\u73b0\u79f0\u4e3a\u6d45\u62f7\u8d1d\u3002zeros_like\u548cones_like\u987e\u540d\u601d\u4e49\u5c06\u4ea7\u751f\u548c\u539f\u5f20\u91cfb\u76f8\u540c\u5f62\u72b6\u76840\u5f20\u91cf\u548c1\u5f20\u91cf\uff0crandlike\u540c\u7406\u3002\u6700\u540e\u4e00\u4e2aclone\u51fd\u6570\u5219\u662f\u5b8c\u5168\u62f7\u8d1d\u6210\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u539f\u5f20\u91cfb\u7684\u53d8\u5316\u4e0d\u4f1a\u5f71\u54cdd\uff0c\u8fd9\u88ab\u79f0\u4f5c \u6df1\u62f7\u8d1d \u3002 2.Tensor\u7684\u53d8\u5f62 torch\u6539\u53d8\u5f20\u91cf\u5f62\u72b6\uff0c\u4e0d\u6539\u53d8\u5f20\u91cf\u5b58\u50a8\u7684data\u6307\u9488\u6307\u5411\u7684\u5185\u5bb9\uff0c\u53ea\u6539\u53d8\u5f20\u91cf\u7684\u53d6\u6570\u65b9\u5f0f\u3002libtorch\u7684\u53d8\u5f62\u65b9\u5f0f\u548cpytorch\u4e00\u81f4\uff0c\u6709view\uff0ctranspose\uff0creshape\uff0cpermute\u7b49\u5e38\u7528\u53d8\u5f62\u3002 auto b = torch::full({10},3); b.view({1, 2,-1}); std::cout << b; b = b.view({1, 2,-1}); std::cout << b; auto c = b.transpose(0,1); std::cout << c; auto d = b.reshape({1,1,-1}); std::cout << d; auto e = b.permute({1,0,2}); std::cout << e; .view \u4e0d\u662finplace\u64cd\u4f5c\uff0c\u9700\u8981\u52a0=\u3002\u53d8\u5f62\u64cd\u4f5c\u6ca1\u592a\u591a\u8981\u8bf4\u7684\uff0c\u548cpytorch\u4e00\u6837\u3002\u8fd8\u6709squeeze\u548cunsqueeze\u64cd\u4f5c\uff0c\u4e5f\u4e0epytorch\u76f8\u540c\u3002 3.Tensor\u7684\u5207\u7247 \u901a\u8fc7\u7d22\u5f15\u622a\u53d6\u5f20\u91cf\uff0c\u4ee3\u7801\u5982\u4e0b auto b = torch::rand({10,3,28,28}); std::cout << b[0].sizes(); //\u7b2c0\u5f20\u7167\u7247 std::cout << b[0][0].sizes(); //\u7b2c0\u5f20\u7167\u7247\u7684\u7b2c0\u4e2a\u901a\u9053 std::cout << b[0][0][0].sizes(); //\u7b2c0\u5f20\u7167\u7247\u7684\u7b2c0\u4e2a\u901a\u9053\u7684\u7b2c0\u884c\u50cf\u7d20 dim\u4e3a1 std::cout << b[0][0][0][0].sizes(); //\u7b2c0\u5f20\u7167\u7247\u7684\u7b2c0\u4e2a\u901a\u9053\u7684\u7b2c0\u884c\u7684\u7b2c0\u4e2a\u50cf\u7d20 dim\u4e3a0 \u9664\u4e86\u7d22\u5f15\uff0c\u8fd8\u6709\u5176\u4ed6\u64cd\u4f5c\u662f\u5e38\u7528\u7684\uff0c\u5982narrow\uff0cselect\uff0cindex\uff0cindex_select\u3002 std::cout << b.index_select(0,torch::tensor({0, 3, 3})).sizes(); //\u9009\u62e9\u7b2c0\u7ef4\u76840\uff0c3\uff0c3\u7ec4\u6210\u65b0\u5f20\u91cf[3,3,28,28] std::cout << b.index_select(1,torch::tensor({0,2})).sizes(); //\u9009\u62e9\u7b2c1\u7ef4\u7684\u7b2c0\u548c\u7b2c2\u7684\u7ec4\u6210\u65b0\u5f20\u91cf[10, 2, 28, 28] std::cout << b.index_select(2,torch::arange(0,8)).sizes(); //\u9009\u62e9\u5341\u5f20\u56fe\u7247\u6bcf\u4e2a\u901a\u9053\u7684\u524d8\u5217\u7684\u6240\u6709\u50cf\u7d20[10, 3, 8, 28] std::cout << b.narrow(1,0,2).sizes(); //\u9009\u62e9\u7b2c1\u7ef4\uff0c\u4ece0\u5f00\u59cb\uff0c\u622a\u53d6\u957f\u5ea6\u4e3a2\u7684\u90e8\u5206\u5f20\u91cf[10, 2, 28, 28] std::cout << b.select(3,2).sizes(); //\u9009\u62e9\u7b2c3\u7ef4\u5ea6\u7684\u7b2c\u4e8c\u4e2a\u5f20\u91cf\uff0c\u5373\u6240\u6709\u56fe\u7247\u7684\u7b2c2\u884c\u7ec4\u6210\u7684\u5f20\u91cf[10, 3, 28] index\u9700\u8981\u5355\u72ec\u8bf4\u660e\u7528\u9014\u3002\u5728pytorch\u4e2d\uff0c\u901a\u8fc7\u63a9\u7801Mask\u5bf9\u5f20\u91cf\u8fdb\u884c\u7b5b\u9009\u662f\u5bb9\u6613\u7684\u76f4\u63a5 Tensor[Mask] \u5373\u53ef\u3002\u4f46\u662fc++\u4e2d\u65e0\u6cd5\u76f4\u63a5\u8fd9\u6837\u4f7f\u7528\uff0c\u9700\u8981index\u51fd\u6570\u5b9e\u73b0\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a auto c = torch::randn({3,4}); auto mask = torch::zeros({3,4}); mask[0][0] = 1; std::cout<<c; std::cout<<c.index({mask.to(torch::kBool)}); \u6709\u7f51\u53cb\u63d0\u95ee\uff0c\u8fd9\u6837index\u51fa\u6765\u7684\u5f20\u91cf\u662f\u6df1\u62f7\u8d1d\u7684\u7ed3\u679c\uff0c\u4e5f\u5c31\u662f\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u90a3\u4e48\u5982\u4f55\u5bf9\u539f\u59cb\u5f20\u91cf\u7684mask\u6307\u5411\u7684\u503c\u505a\u4fee\u6539\u5462\u3002\u67e5\u770btorch\u7684api\u53d1\u73b0\u8fd8\u6709index_put_\u51fd\u6570\u7528\u4e8e\u76f4\u63a5\u653e\u7f6e\u6307\u5b9a\u7684\u5f20\u91cf\u6216\u8005\u5e38\u6570\u3002\u7ec4\u5408index_put_\u548cindex\u51fd\u6570\u53ef\u4ee5\u5b9e\u73b0\u8be5\u9700\u6c42\u3002 auto c = torch::randn({ 3,4 }); auto mask = torch::zeros({ 3,4 }); mask[0][0] = 1; mask[0][2] = 1; std::cout << c; std::cout << c.index({ mask.to(torch::kBool) }); std::cout << c.index_put_({ mask.to(torch::kBool) }, c.index({ mask.to(torch::kBool) })+1.5); std::cout << c; \u6b64\u5916python\u4e2d\u8fd8\u6709\u4e00\u79cd\u5e38\u89c1\u53d6\u6570\u65b9\u5f0f tensor[:,0::4] \u8fd9\u79cd\u5728\u7b2c1\u7ef4\uff0c\u8d77\u59cb\u4f4d\u7f6e\u4e3a0\uff0c\u95f4\u96944\u53d6\u6570\u7684\u65b9\u5f0f\uff0c\u5728c++\u4e2d\u76f4\u63a5\u7528 slice \u51fd\u6570\u5b9e\u73b0\u3002 4.Tensor\u95f4\u7684\u64cd\u4f5c \u62fc\u63a5\u548c\u5806\u53e0 auto b = torch::ones({3,4}); auto c = torch::zeros({3,4}); auto cat = torch::cat({b,c},1); //1\u8868\u793a\u7b2c1\u7ef4\uff0c\u8f93\u51fa\u5f20\u91cf[3,8] auto stack = torch::stack({b,c},1); //1\u8868\u793a\u7b2c1\u7ef4\uff0c\u8f93\u51fa[3,2,4] std::cout << b << c << cat << stack; \u5230\u8fd9\u8bfb\u8005\u4f1a\u53d1\u73b0\uff0c\u4ecepytorch\u5230libtorch\uff0c\u638c\u63e1\u4e86 [] \u5230 {} \u7684\u53d8\u5316\u5c31\u7b80\u5355\u5f88\u591a\uff0c\u5927\u90e8\u5206\u64cd\u4f5c\u53ef\u4ee5\u76f4\u63a5\u8fc1\u79fb\u3002 \u56db\u5219\u8fd0\u7b97\u64cd\u4f5c\u540c\u7406\uff0c\u50cf\u5bf9\u5e94\u5143\u7d20\u4e58\u9664\u76f4\u63a5\u7528 * \u548c / \u5373\u53ef\uff0c\u4e5f\u53ef\u4ee5\u7528 .mul \u548c .div \u3002\u77e9\u9635\u4e58\u6cd5\u7528 .mm \uff0c\u52a0\u5165\u6279\u6b21\u5c31\u662f .bmm \u3002 auto b = torch::rand({3,4}); auto c = torch::rand({3,4}); std::cout << b << c << b*c << b/c << b.mm(c.t()); \u5176\u4ed6\u4e00\u4e9b\u64cd\u4f5c\u50cf clamp \uff0c min \uff0c max \u8fd9\u79cd\u90fd\u548cpytorch\u7c7b\u4f3c\uff0c\u4eff\u7167\u4e0a\u8ff0\u65b9\u6cd5\u53ef\u4ee5\u81ea\u884c\u63a2\u7d22\u3002 \u611f\u8c22\u5927\u4f6c\uff1a libtorch\u6559\u7a0b\uff08\u4e8c\uff09","title":"\u7b2c\u4e8c\u7ae0 \u5f20\u91cf\u7684\u5e38\u89c4\u64cd\u4f5c"},{"location":"chapter2/#_1","text":"ibtorch(pytorch c++)\u7684\u5927\u591a\u6570api\u548cpytorch\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\uff0clibtorch\u4e2d\u5f20\u91cf\u7684\u521d\u59cb\u5316\u4e5f\u548cpytorch\u4e2d\u7684\u7c7b\u4f3c\u3002\u672c\u6587\u4ecb\u7ecd\u56db\u79cd\u6df1\u5ea6\u56fe\u50cf\u7f16\u7a0b\u9700\u8981\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002","title":"\u7b2c\u4e8c\u7ae0 \u5f20\u91cf\u7684\u5e38\u89c4\u64cd\u4f5c"},{"location":"chapter2/#1tensor","text":"\u7b2c\u4e00\u79cd\uff0c\u56fa\u5b9a\u5c3a\u5bf8\u548c\u503c\u7684\u521d\u59cb\u5316\u3002 //\u5e38\u89c1\u56fa\u5b9a\u503c\u7684\u521d\u59cb\u5316\u65b9\u5f0f auto b = torch::zeros({3,4}); b = torch::ones({3,4}); b= torch::eye(4); b = torch::full({3,4},10); b = torch::tensor({33,22,11}); pytorch\u4e2d\u7528 [] \u8868\u793a\u5c3a\u5bf8\uff0c\u800ccpp\u4e2d\u7528 {} \u8868\u793a\u3002zeros\u4ea7\u751f\u503c\u5168\u4e3a0\u7684\u5f20\u91cf\u3002ones\u4ea7\u751f\u503c\u5168\u4e3a1\u7684\u5f20\u91cf\u3002eye\u4ea7\u751f\u5355\u4f4d\u77e9\u9635\u5f20\u91cf\u3002full\u4ea7\u751f\u6307\u5b9a\u503c\u548c\u5c3a\u5bf8\u7684\u5f20\u91cf\u3002 torch::tensor({}) \u4e5f\u53ef\u4ee5\u4ea7\u751f\u5f20\u91cf\uff0c\u6548\u679c\u548cpytorch\u7684 torch.Tensor([]) \u6216\u8005 torch.tensor([]) \u4e00\u6837\u3002 \u7b2c\u4e8c\u79cd\uff0c\u56fa\u5b9a\u5c3a\u5bf8\uff0c\u968f\u673a\u503c\u7684\u521d\u59cb\u5316\u65b9\u6cd5 //\u968f\u673a\u521d\u59cb\u5316 auto r = torch::rand({3,4}); r = torch::randn({3, 4}); r = torch::randint(0, 4,{3,3}); rand\u4ea7\u751f0-1\u4e4b\u95f4\u7684\u968f\u673a\u503c\uff0crandn\u53d6\u6b63\u6001\u5206\u5e03N(0,1)\u7684\u968f\u673a\u503c\uff0crandint\u53d6 [min,max) \u7684\u968f\u673a\u6574\u578b\u6570\u503c\u3002 \u7b2c\u4e09\u79cd\uff0c\u4ecec++\u7684\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u800c\u6765 int aa[10] = {3,4,6}; std::vector<float> aaaa = {3,4,6}; auto aaaaa = torch::from_blob(aa,{3},torch::kFloat); auto aaa = torch::from_blob(aaaa.data(),{3},torch::kFloat); pytorch\u53ef\u4ee5\u63a5\u53d7\u4ece\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u5982numpy\u548clist\u7684\u6570\u636e\u8f6c\u5316\u6210\u5f20\u91cf\u3002libtorch\u540c\u6837\u53ef\u4ee5\u63a5\u53d7\u5176\u4ed6\u6570\u636e\u6307\u9488\uff0c\u901a\u8fc7 from_blob \u51fd\u6570\u5373\u53ef\u8f6c\u6362\u3002\u8fd9\u4e2a\u65b9\u5f0f\u5728\u90e8\u7f72\u4e2d\u7ecf\u5e38\u7528\u5230\uff0c\u5982\u679c\u56fe\u50cf\u662fopencv\u52a0\u8f7d\u7684\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7 from_blob \u5c06\u56fe\u50cf\u6307\u9488\u8f6c\u6210\u5f20\u91cf\u3002 \u7b2c\u56db\u79cd\uff0c\u6839\u636e\u5df2\u6709\u5f20\u91cf\u521d\u59cb\u5316 auto b = torch::zeros({3,4}); auto d = torch::Tensor(b); d = torch::zeros_like(b); d = torch::ones_like(b); d = torch::rand_like(b,torch::kFloat); d = b.clone(); \u8fd9\u91cc\uff0c auto d = torch::Tensor(b) \u7b49\u4ef7\u4e8e auto d = b \uff0c\u4e24\u8005\u521d\u59cb\u5316\u7684\u5f20\u91cfd\u5747\u53d7\u539f\u5f20\u91cfb\u7684\u5f71\u54cd\uff0cb\u4e2d\u7684\u503c\u53d1\u751f\u6539\u53d8\uff0cd\u4e5f\u5c06\u53d1\u751f\u6539\u53d8\uff0c\u4f46\u662fb\u5982\u679c\u53ea\u662f\u5f20\u91cf\u53d8\u5f62\uff0cd\u5374\u4e0d\u4f1a\u8ddf\u7740\u53d8\u5f62\uff0c\u4ecd\u65e7\u4fdd\u6301\u521d\u59cb\u5316\u65f6\u7684\u5f62\u72b6\uff0c\u8fd9\u79cd\u8868\u73b0\u79f0\u4e3a\u6d45\u62f7\u8d1d\u3002zeros_like\u548cones_like\u987e\u540d\u601d\u4e49\u5c06\u4ea7\u751f\u548c\u539f\u5f20\u91cfb\u76f8\u540c\u5f62\u72b6\u76840\u5f20\u91cf\u548c1\u5f20\u91cf\uff0crandlike\u540c\u7406\u3002\u6700\u540e\u4e00\u4e2aclone\u51fd\u6570\u5219\u662f\u5b8c\u5168\u62f7\u8d1d\u6210\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u539f\u5f20\u91cfb\u7684\u53d8\u5316\u4e0d\u4f1a\u5f71\u54cdd\uff0c\u8fd9\u88ab\u79f0\u4f5c \u6df1\u62f7\u8d1d \u3002","title":"1.Tensor\u7684\u521d\u59cb\u5316"},{"location":"chapter2/#2tensor","text":"torch\u6539\u53d8\u5f20\u91cf\u5f62\u72b6\uff0c\u4e0d\u6539\u53d8\u5f20\u91cf\u5b58\u50a8\u7684data\u6307\u9488\u6307\u5411\u7684\u5185\u5bb9\uff0c\u53ea\u6539\u53d8\u5f20\u91cf\u7684\u53d6\u6570\u65b9\u5f0f\u3002libtorch\u7684\u53d8\u5f62\u65b9\u5f0f\u548cpytorch\u4e00\u81f4\uff0c\u6709view\uff0ctranspose\uff0creshape\uff0cpermute\u7b49\u5e38\u7528\u53d8\u5f62\u3002 auto b = torch::full({10},3); b.view({1, 2,-1}); std::cout << b; b = b.view({1, 2,-1}); std::cout << b; auto c = b.transpose(0,1); std::cout << c; auto d = b.reshape({1,1,-1}); std::cout << d; auto e = b.permute({1,0,2}); std::cout << e; .view \u4e0d\u662finplace\u64cd\u4f5c\uff0c\u9700\u8981\u52a0=\u3002\u53d8\u5f62\u64cd\u4f5c\u6ca1\u592a\u591a\u8981\u8bf4\u7684\uff0c\u548cpytorch\u4e00\u6837\u3002\u8fd8\u6709squeeze\u548cunsqueeze\u64cd\u4f5c\uff0c\u4e5f\u4e0epytorch\u76f8\u540c\u3002","title":"2.Tensor\u7684\u53d8\u5f62"},{"location":"chapter2/#3tensor","text":"\u901a\u8fc7\u7d22\u5f15\u622a\u53d6\u5f20\u91cf\uff0c\u4ee3\u7801\u5982\u4e0b auto b = torch::rand({10,3,28,28}); std::cout << b[0].sizes(); //\u7b2c0\u5f20\u7167\u7247 std::cout << b[0][0].sizes(); //\u7b2c0\u5f20\u7167\u7247\u7684\u7b2c0\u4e2a\u901a\u9053 std::cout << b[0][0][0].sizes(); //\u7b2c0\u5f20\u7167\u7247\u7684\u7b2c0\u4e2a\u901a\u9053\u7684\u7b2c0\u884c\u50cf\u7d20 dim\u4e3a1 std::cout << b[0][0][0][0].sizes(); //\u7b2c0\u5f20\u7167\u7247\u7684\u7b2c0\u4e2a\u901a\u9053\u7684\u7b2c0\u884c\u7684\u7b2c0\u4e2a\u50cf\u7d20 dim\u4e3a0 \u9664\u4e86\u7d22\u5f15\uff0c\u8fd8\u6709\u5176\u4ed6\u64cd\u4f5c\u662f\u5e38\u7528\u7684\uff0c\u5982narrow\uff0cselect\uff0cindex\uff0cindex_select\u3002 std::cout << b.index_select(0,torch::tensor({0, 3, 3})).sizes(); //\u9009\u62e9\u7b2c0\u7ef4\u76840\uff0c3\uff0c3\u7ec4\u6210\u65b0\u5f20\u91cf[3,3,28,28] std::cout << b.index_select(1,torch::tensor({0,2})).sizes(); //\u9009\u62e9\u7b2c1\u7ef4\u7684\u7b2c0\u548c\u7b2c2\u7684\u7ec4\u6210\u65b0\u5f20\u91cf[10, 2, 28, 28] std::cout << b.index_select(2,torch::arange(0,8)).sizes(); //\u9009\u62e9\u5341\u5f20\u56fe\u7247\u6bcf\u4e2a\u901a\u9053\u7684\u524d8\u5217\u7684\u6240\u6709\u50cf\u7d20[10, 3, 8, 28] std::cout << b.narrow(1,0,2).sizes(); //\u9009\u62e9\u7b2c1\u7ef4\uff0c\u4ece0\u5f00\u59cb\uff0c\u622a\u53d6\u957f\u5ea6\u4e3a2\u7684\u90e8\u5206\u5f20\u91cf[10, 2, 28, 28] std::cout << b.select(3,2).sizes(); //\u9009\u62e9\u7b2c3\u7ef4\u5ea6\u7684\u7b2c\u4e8c\u4e2a\u5f20\u91cf\uff0c\u5373\u6240\u6709\u56fe\u7247\u7684\u7b2c2\u884c\u7ec4\u6210\u7684\u5f20\u91cf[10, 3, 28] index\u9700\u8981\u5355\u72ec\u8bf4\u660e\u7528\u9014\u3002\u5728pytorch\u4e2d\uff0c\u901a\u8fc7\u63a9\u7801Mask\u5bf9\u5f20\u91cf\u8fdb\u884c\u7b5b\u9009\u662f\u5bb9\u6613\u7684\u76f4\u63a5 Tensor[Mask] \u5373\u53ef\u3002\u4f46\u662fc++\u4e2d\u65e0\u6cd5\u76f4\u63a5\u8fd9\u6837\u4f7f\u7528\uff0c\u9700\u8981index\u51fd\u6570\u5b9e\u73b0\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a auto c = torch::randn({3,4}); auto mask = torch::zeros({3,4}); mask[0][0] = 1; std::cout<<c; std::cout<<c.index({mask.to(torch::kBool)}); \u6709\u7f51\u53cb\u63d0\u95ee\uff0c\u8fd9\u6837index\u51fa\u6765\u7684\u5f20\u91cf\u662f\u6df1\u62f7\u8d1d\u7684\u7ed3\u679c\uff0c\u4e5f\u5c31\u662f\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u90a3\u4e48\u5982\u4f55\u5bf9\u539f\u59cb\u5f20\u91cf\u7684mask\u6307\u5411\u7684\u503c\u505a\u4fee\u6539\u5462\u3002\u67e5\u770btorch\u7684api\u53d1\u73b0\u8fd8\u6709index_put_\u51fd\u6570\u7528\u4e8e\u76f4\u63a5\u653e\u7f6e\u6307\u5b9a\u7684\u5f20\u91cf\u6216\u8005\u5e38\u6570\u3002\u7ec4\u5408index_put_\u548cindex\u51fd\u6570\u53ef\u4ee5\u5b9e\u73b0\u8be5\u9700\u6c42\u3002 auto c = torch::randn({ 3,4 }); auto mask = torch::zeros({ 3,4 }); mask[0][0] = 1; mask[0][2] = 1; std::cout << c; std::cout << c.index({ mask.to(torch::kBool) }); std::cout << c.index_put_({ mask.to(torch::kBool) }, c.index({ mask.to(torch::kBool) })+1.5); std::cout << c; \u6b64\u5916python\u4e2d\u8fd8\u6709\u4e00\u79cd\u5e38\u89c1\u53d6\u6570\u65b9\u5f0f tensor[:,0::4] \u8fd9\u79cd\u5728\u7b2c1\u7ef4\uff0c\u8d77\u59cb\u4f4d\u7f6e\u4e3a0\uff0c\u95f4\u96944\u53d6\u6570\u7684\u65b9\u5f0f\uff0c\u5728c++\u4e2d\u76f4\u63a5\u7528 slice \u51fd\u6570\u5b9e\u73b0\u3002","title":"3.Tensor\u7684\u5207\u7247"},{"location":"chapter2/#4tensor","text":"\u62fc\u63a5\u548c\u5806\u53e0 auto b = torch::ones({3,4}); auto c = torch::zeros({3,4}); auto cat = torch::cat({b,c},1); //1\u8868\u793a\u7b2c1\u7ef4\uff0c\u8f93\u51fa\u5f20\u91cf[3,8] auto stack = torch::stack({b,c},1); //1\u8868\u793a\u7b2c1\u7ef4\uff0c\u8f93\u51fa[3,2,4] std::cout << b << c << cat << stack; \u5230\u8fd9\u8bfb\u8005\u4f1a\u53d1\u73b0\uff0c\u4ecepytorch\u5230libtorch\uff0c\u638c\u63e1\u4e86 [] \u5230 {} \u7684\u53d8\u5316\u5c31\u7b80\u5355\u5f88\u591a\uff0c\u5927\u90e8\u5206\u64cd\u4f5c\u53ef\u4ee5\u76f4\u63a5\u8fc1\u79fb\u3002 \u56db\u5219\u8fd0\u7b97\u64cd\u4f5c\u540c\u7406\uff0c\u50cf\u5bf9\u5e94\u5143\u7d20\u4e58\u9664\u76f4\u63a5\u7528 * \u548c / \u5373\u53ef\uff0c\u4e5f\u53ef\u4ee5\u7528 .mul \u548c .div \u3002\u77e9\u9635\u4e58\u6cd5\u7528 .mm \uff0c\u52a0\u5165\u6279\u6b21\u5c31\u662f .bmm \u3002 auto b = torch::rand({3,4}); auto c = torch::rand({3,4}); std::cout << b << c << b*c << b/c << b.mm(c.t()); \u5176\u4ed6\u4e00\u4e9b\u64cd\u4f5c\u50cf clamp \uff0c min \uff0c max \u8fd9\u79cd\u90fd\u548cpytorch\u7c7b\u4f3c\uff0c\u4eff\u7167\u4e0a\u8ff0\u65b9\u6cd5\u53ef\u4ee5\u81ea\u884c\u63a2\u7d22\u3002 \u611f\u8c22\u5927\u4f6c\uff1a libtorch\u6559\u7a0b\uff08\u4e8c\uff09","title":"4.Tensor\u95f4\u7684\u64cd\u4f5c"},{"location":"chapter3/","text":"\u7b2c\u4e09\u7ae0 \u6a21\u578b\u642d\u5efa 1.\u57fa\u672c\u6a21\u5757\u7684\u642d\u5efa \u6a21\u5757\u5316\u7f16\u7a0b\u7684\u601d\u60f3\u975e\u5e38\u91cd\u8981\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7f16\u7a0b\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u91cd\u590d\u7684\u6572\u4ee3\u7801\u8fc7\u7a0b\uff0c\u540c\u65f6\u4ee3\u7801\u53ef\u8bfb\u6027\u4e5f\u4f1a\u589e\u52a0\u3002\u672c\u7ae0\u5c06\u8bb2\u8ff0\u5982\u4f55\u4f7f\u7528libtorch\u642d\u5efa\u4e00\u4e9bMLP\u548cCNN\u7684\u57fa\u672c\u6a21\u5757\u3002 1.MLP\u57fa\u672c\u5355\u5143 \u9996\u5148\u662f\u7ebf\u6027\u5c42\u7684\u58f0\u660e\u548c\u5b9a\u4e49\uff0c\u5305\u62ec\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad\u51fd\u6570\u3002\u4ee3\u7801\u5982\u4e0b\uff1a // LinearBnReluImpl \u7c7b class LinearBnReluImpl : public torch::nn::Module{ public: LinearBnReluImpl(int intput_features, int output_features); torch::Tensor forward(torch::Tensor x); private: //layers torch::nn::Linear ln{nullptr}; //ln\u662f\u6307\u5411\u7c7b\uff0c\u7ed3\u6784\u6216\u8054\u5408\u7684\u6307\u9488 torch::nn::BatchNorm1d bn{nullptr}; }; TORCH_MODULE(LinearBnRelu); // \u7c7b\u7684\u6784\u9020\u51fd\u6570 LinearBnReluImpl::LinearBnReluImpl(int in_features, int out_features){ ln = register_module(\"ln\", torch::nn::Linear(torch::nn::LinearOptions(in_features, out_features))); bn = register_module(\"bn\", torch::nn::BatchNorm1d(out_features)); } // \u7c7b\u7684forword\u6210\u5458\u51fd\u6570 torch::Tensor LinearBnReluImpl::forward(torch::Tensor x){ x = torch::relu(ln->forward(x)); x = bn(x); return x; } \u5728MLP\u7684\u6784\u9020\u7ebf\u6027\u5c42\u6a21\u5757\u7c7b\u65f6\uff0c\u6211\u4eec\u7ee7\u627f\u4e86 torch::nn::Module \u7c7b\uff0c\u5c06\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad\u6a21\u5757\u4f5c\u4e3apublic\uff0c\u53ef\u4ee5\u7ed9\u5bf9\u8c61\u4f7f\u7528\uff0c\u800c\u91cc\u9762\u7684\u7ebf\u6027\u5c42 torch::nn::Linear \u548c\u5f52\u4e00\u5316\u5c42 torch::nn::BatchNorm1d \u88ab\u9690\u85cf\u4f5c\u4e3a\u79c1\u6709\u53d8\u91cf\u3002 \u5b9a\u4e49\u6784\u9020\u51fd\u6570\u65f6\uff0c\u9700\u8981\u5c06\u539f\u672c\u7684\u6307\u9488\u5bf9\u8c61ln\u548cbn\u8fdb\u884c\u8d4b\u503c\uff0c\u540c\u65f6\u5c06\u4e24\u8005\u7684\u540d\u79f0\u4e5f\u786e\u5b9a\u3002\u524d\u5411\u4f20\u64ad\u51fd\u6570\u5c31\u548cpytorch\u4e2d\u7684forward\u7c7b\u4f3c\u3002 2.CNN\u57fa\u672c\u5355\u5143 CNN\u7684\u57fa\u672c\u5355\u5143\u6784\u5efa\u548cMLP\u7684\u6784\u5efa\u7c7b\u4f3c\uff0c\u4f46\u662f\u53c8\u7a0d\u6709\u4e0d\u540c\uff0c\u9996\u5148\u9700\u8981\u5b9a\u4e49\u7684\u65f6\u5377\u79ef\u8d85\u53c2\u6570\u786e\u5b9a\u51fd\u6570\u3002 inline torch::nn::Conv2dOptions conv_options(int64_t in_planes, int64_t out_planes, int64_t kerner_size, int64_t stride = 1, int64_t padding = 0, bool with_bias = false) { torch::nn::Conv2dOptions conv_options = torch::nn::Conv2dOptions(in_planes, out_planes, kerner_size); conv_options.stride(stride); conv_options.padding(padding); conv_options.bias(with_bias); return conv_options; } \u8be5\u51fd\u6570\u8fd4\u56de torch::nn::Conv2dOptions \u5bf9\u8c61\uff0c\u5bf9\u8c61\u7684\u8d85\u53c2\u6570\u7531\u51fd\u6570\u63a5\u53e3\u6307\u5b9a\uff0c\u8fd9\u6837\u53ef\u4ee5\u65b9\u4fbf\u4f7f\u7528\u3002\u540c\u65f6\u6307\u5b9ainline(\u5185\u8054\u51fd\u6570\uff09\uff0c\u63d0\u9ad8Release\u6a21\u5f0f\u4e0b\u4ee3\u7801\u6267\u884c\u6548\u7387\u3002 C++ \u5185\u8054\u51fd\u6570\u662f\u901a\u5e38\u4e0e\u7c7b\u4e00\u8d77\u4f7f\u7528\u3002\u5982\u679c\u4e00\u4e2a\u51fd\u6570\u662f\u5185\u8054\u7684\uff0c\u90a3\u4e48\u5728\u7f16\u8bd1\u65f6\uff0c\u7f16\u8bd1\u5668\u4f1a\u628a\u8be5\u51fd\u6570\u7684\u4ee3\u7801\u526f\u672c\u653e\u7f6e\u5728\u6bcf\u4e2a\u8c03\u7528\u8be5\u51fd\u6570\u7684\u5730\u65b9\u3002 \u5bf9\u5185\u8054\u51fd\u6570\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\uff0c\u90fd\u9700\u8981\u91cd\u65b0\u7f16\u8bd1\u51fd\u6570\u7684\u6240\u6709\u5ba2\u6237\u7aef\uff0c\u56e0\u4e3a\u7f16\u8bd1\u5668\u9700\u8981\u91cd\u65b0\u66f4\u6362\u4e00\u6b21\u6240\u6709\u7684\u4ee3\u7801\uff0c\u5426\u5219\u5c06\u4f1a\u7ee7\u7eed\u4f7f\u7528\u65e7\u7684\u51fd\u6570\u3002 \u5982\u679c\u60f3\u628a\u4e00\u4e2a\u51fd\u6570\u5b9a\u4e49\u4e3a\u5185\u8054\u51fd\u6570\uff0c\u5219\u9700\u8981\u5728\u51fd\u6570\u540d\u524d\u9762\u653e\u7f6e\u5173\u952e\u5b57 inline\uff0c\u5728\u8c03\u7528\u51fd\u6570\u4e4b\u524d\u9700\u8981\u5bf9\u51fd\u6570\u8fdb\u884c\u5b9a\u4e49\u3002\u5982\u679c\u5df2\u5b9a\u4e49\u7684\u51fd\u6570\u591a\u4e8e\u4e00\u884c\uff0c\u7f16\u8bd1\u5668\u4f1a\u5ffd\u7565 inline \u9650\u5b9a\u7b26\u3002 \u968f\u540e\u5219\u662f\u548cMLP\u7684\u7ebf\u6027\u6a21\u5757\u7c7b\u4f3c\uff0cCNN\u7684\u57fa\u672c\u6a21\u5757\u7531\u5377\u79ef\u5c42\uff0c\u6fc0\u6d3b\u51fd\u6570\u548c\u5f52\u4e00\u5316\u5c42\u7ec4\u6210\u3002\u4ee3\u7801\u5982\u4e0b\uff1a class ConvReluBnImpl : public torch::nn::Module { public: ConvReluBnImpl(int input_channel=3, int output_channel=64, int kernel_size = 3, int stride = 1); // \u6784\u9020\u51fd\u6570 torch::Tensor forward(torch::Tensor x); //forward\u6210\u5458\u51fd\u6570 private: // Declare layers torch::nn::Conv2d conv{ nullptr }; torch::nn::BatchNorm2d bn{ nullptr }; }; TORCH_MODULE(ConvReluBn); // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 ConvReluBnImpl::ConvReluBnImpl(int input_channel, int output_channel, int kernel_size, int stride) { conv = register_module(\"conv\", torch::nn::Conv2d(conv_options(input_channel,output_channel,kernel_size,stride,kernel_size/2))); bn = register_module(\"bn\", torch::nn::BatchNorm2d(output_channel)); } // forward\u51fd\u6570\u7684\u5b9e\u73b0 torch::Tensor ConvReluBnImpl::forward(torch::Tensor x) { x = torch::relu(conv->forward(x)); x = bn(x); return x; } \u6bcf\u4e00\u5c42\u7684\u5b9e\u73b0\u5747\u662f\u901a\u8fc7\u524d\u9762\u5b9a\u4e49\u7684\u57fa\u672c\u6a21\u5757LinearBnRelu\u3002 2.\u7b80\u5355\u7684MLP \u5728MLP\u7684\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u4ee5\u642d\u5efa\u4e00\u4e2a\u56db\u5c42\u611f\u77e5\u673a\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528cpp\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8be5\u611f\u77e5\u673a\u63a5\u53d7in_features\u4e2a\u7279\u5f81\uff0c\u8f93\u51faout_features\u4e2a\u7f16\u7801\u540e\u7684\u7279\u5f81\u3002\u4e2d\u95f4\u7279\u5f81\u6570\u5b9a\u4e49\u4e3a32\uff0c64\u548c128\u3002 class MLP: public torch::nn::Module{ // \u7ee7\u627f public: MLP(int in_features, int out_features); torch::Tensor forward(torch::Tensor x); private: int mid_features[3] = {32,64,128}; LinearBnRelu ln1{nullptr}; LinearBnRelu ln2{nullptr}; LinearBnRelu ln3{nullptr}; torch::nn::Linear out_ln{nullptr}; }; // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 MLP::MLP(int in_features, int out_features){ ln1 = LinearBnRelu(in_features, mid_features[0]); ln2 = LinearBnRelu(mid_features[0], mid_features[1]); ln3 = LinearBnRelu(mid_features[1], mid_features[2]); out_ln = torch::nn::Linear(mid_features[2], out_features); ln1 = register_module(\"ln1\", ln1); ln2 = register_module(\"ln2\", ln2); ln3 = register_module(\"ln3\", ln3); out_ln = register_module(\"out_ln\",out_ln); } // forward\u51fd\u6570\u7684\u5b9e\u73b0 torch::Tensor MLP::forward(torch::Tensor x){ x = ln1->forward(x); //\u6307\u9488\u6210\u5458\u7684\u8bbf\u95ee\u8c03\u7528 x = ln2->forward(x); x = ln3->forward(x); x = out_ln->forward(x); return x; } \u6bcf\u4e00\u5c42\u7684\u5b9e\u73b0\u5747\u662f\u901a\u8fc7\u524d\u9762\u5b9a\u4e49\u7684\u57fa\u672c\u6a21\u5757LinearBnRelu\u3002 3.\u7b80\u5355CNN \u524d\u9762\u4ecb\u7ecd\u4e86\u6784\u5efaCNN\u7684\u57fa\u672c\u6a21\u5757ConvReluBn\uff0c\u63a5\u4e0b\u6765\u5c1d\u8bd5\u7528c++\u642d\u5efaCNN\u6a21\u578b\u3002\u8be5CNN\u7531\u4e09\u4e2astage\u7ec4\u6210\uff0c\u6bcf\u4e2astage\u53c8\u7531\u4e00\u4e2a\u5377\u79ef\u5c42\u4e00\u4e2a\u4e0b\u91c7\u6837\u5c42\u7ec4\u6210\u3002\u8fd9\u6837\u76f8\u5f53\u4e8e\u5bf9\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u4e868\u500d\u4e0b\u91c7\u6837\u3002\u4e2d\u95f4\u5c42\u7684\u901a\u9053\u6570\u53d8\u5316\u4e0e\u524d\u9762MLP\u7279\u5f81\u6570\u53d8\u5316\u76f8\u540c\uff0c\u5747\u4e3a\u8f93\u5165->32->64->128->\u8f93\u51fa\u3002 class plainCNN : public torch::nn::Module{ public: plainCNN(int in_channels, int out_channels); torch::Tensor forward(torch::Tensor x); private: int mid_channels[3] = {32,64,128}; ConvReluBn conv1{nullptr}; ConvReluBn down1{nullptr}; ConvReluBn conv2{nullptr}; ConvReluBn down2{nullptr}; ConvReluBn conv3{nullptr}; ConvReluBn down3{nullptr}; torch::nn::Conv2d out_conv{nullptr}; }; // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 plainCNN::plainCNN(int in_channels, int out_channels){ conv1 = ConvReluBn(in_channels,mid_channels[0],3); down1 = ConvReluBn(mid_channels[0],mid_channels[0],3,2); conv2 = ConvReluBn(mid_channels[0],mid_channels[1],3); down2 = ConvReluBn(mid_channels[1],mid_channels[1],3,2); conv3 = ConvReluBn(mid_channels[1],mid_channels[2],3); down3 = ConvReluBn(mid_channels[2],mid_channels[2],3,2); out_conv = torch::nn::Conv2d(conv_options(mid_channels[2],out_channels,3)); conv1 = register_module(\"conv1\",conv1); down1 = register_module(\"down1\",down1); conv2 = register_module(\"conv2\",conv2); down2 = register_module(\"down2\",down2); conv3 = register_module(\"conv3\",conv3); down3 = register_module(\"down3\",down3); out_conv = register_module(\"out_conv\",out_conv); } // forward \u65b9\u6cd5\u7684\u4f7f\u7528 torch::Tensor plainCNN::forward(torch::Tensor x){ x = conv1->forward(x); x = down1->forward(x); x = conv2->forward(x); x = down2->forward(x); x = conv3->forward(x); x = down3->forward(x); x = out_conv->forward(x); return x; } \u5047\u5b9a\u8f93\u5165\u4e00\u4e2a\u4e09\u901a\u9053\u56fe\u7247\uff0c\u8f93\u51fa\u901a\u9053\u6570\u5b9a\u4e49\u4e3an\uff0c\u8f93\u5165\u8868\u793a\u4e00\u4e2a [1,3,224,224] \u7684\u5f20\u91cf\uff0c\u5c06\u5f97\u5230\u4e00\u4e2a [1,n,28,28] \u7684\u8f93\u51fa\u5f20\u91cf\u3002 4.\u7b80\u5355LSTM \u6700\u540e\u5219\u662f\u4e00\u4e2a\u7b80\u5355\u7684LSTM\u7684\u4f8b\u5b50\uff0c\u7528\u4ee5\u5904\u7406\u65f6\u5e8f\u578b\u7279\u5f81\u3002\u5728\u76f4\u63a5\u4f7f\u7528 torch::nn::LSTM \u7c7b\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u5b9a\u4e00\u4e2a\u8fd4\u56de torch::nn::LSTMOptions \u5bf9\u8c61\u7684\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u63a5\u53d7\u5173\u4e8eLSTM\u7684\u8d85\u53c2\u6570\uff0c\u8fd4\u56de\u8fd9\u4e9b\u8d85\u53c2\u6570\u5b9a\u4e49\u7684\u7ed3\u679c\u3002 inline torch::nn::LSTMOptions lstmOption(int in_features, int hidden_layer_size, int num_layers, bool batch_first = false, bool bidirectional = false){ torch::nn::LSTMOptions lstmOption = torch::nn::LSTMOptions(in_features, hidden_layer_size); lstmOption.num_layers(num_layers).batch_first(batch_first).bidirectional(bidirectional); return lstmOption; } //batch_first: true for io(batch, seq, feature) else io(seq, batch, feature) class LSTM: public torch::nn::Module{ public: LSTM(int in_features, int hidden_layer_size, int out_size, int num_layers, bool batch_first); torch::Tensor forward(torch::Tensor x); private: torch::nn::LSTM lstm{nullptr}; torch::nn::Linear ln{nullptr}; std::tuple<torch::Tensor, torch::Tensor> hidden_cell; }; \u58f0\u660e\u597dLSTM\u4ee5\u540e\uff0c\u6211\u4eec\u5c06\u5185\u90e8\u7684\u521d\u59cb\u5316\u51fd\u6570\u548c\u524d\u5411\u4f20\u64ad\u51fd\u6570\u5b9e\u73b0\u5982\u4e0b\uff1a LSTM::LSTM(int in_features, int hidden_layer_size, int out_size, int num_layers, bool batch_first){ lstm = torch::nn::LSTM(lstmOption(in_features, hidden_layer_size, num_layers, batch_first)); ln = torch::nn::Linear(hidden_layer_size, out_size); lstm = register_module(\"lstm\",lstm); ln = register_module(\"ln\",ln); } torch::Tensor LSTM::forward(torch::Tensor x){ auto lstm_out = lstm->forward(x); auto predictions = ln->forward(std::get<0>(lstm_out)); return predictions.select(1,-1); } \u611f\u8c22\u5927\u4f6c\u5f00\u6e90\uff1a libtorch\u6559\u7a0b\uff08\u4e09\uff09","title":"\u7b2c\u4e09\u7ae0 \u6a21\u578b\u642d\u5efa"},{"location":"chapter3/#_1","text":"","title":"\u7b2c\u4e09\u7ae0 \u6a21\u578b\u642d\u5efa"},{"location":"chapter3/#1","text":"\u6a21\u5757\u5316\u7f16\u7a0b\u7684\u601d\u60f3\u975e\u5e38\u91cd\u8981\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7f16\u7a0b\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u91cd\u590d\u7684\u6572\u4ee3\u7801\u8fc7\u7a0b\uff0c\u540c\u65f6\u4ee3\u7801\u53ef\u8bfb\u6027\u4e5f\u4f1a\u589e\u52a0\u3002\u672c\u7ae0\u5c06\u8bb2\u8ff0\u5982\u4f55\u4f7f\u7528libtorch\u642d\u5efa\u4e00\u4e9bMLP\u548cCNN\u7684\u57fa\u672c\u6a21\u5757\u3002","title":"1.\u57fa\u672c\u6a21\u5757\u7684\u642d\u5efa"},{"location":"chapter3/#1mlp","text":"\u9996\u5148\u662f\u7ebf\u6027\u5c42\u7684\u58f0\u660e\u548c\u5b9a\u4e49\uff0c\u5305\u62ec\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad\u51fd\u6570\u3002\u4ee3\u7801\u5982\u4e0b\uff1a // LinearBnReluImpl \u7c7b class LinearBnReluImpl : public torch::nn::Module{ public: LinearBnReluImpl(int intput_features, int output_features); torch::Tensor forward(torch::Tensor x); private: //layers torch::nn::Linear ln{nullptr}; //ln\u662f\u6307\u5411\u7c7b\uff0c\u7ed3\u6784\u6216\u8054\u5408\u7684\u6307\u9488 torch::nn::BatchNorm1d bn{nullptr}; }; TORCH_MODULE(LinearBnRelu); // \u7c7b\u7684\u6784\u9020\u51fd\u6570 LinearBnReluImpl::LinearBnReluImpl(int in_features, int out_features){ ln = register_module(\"ln\", torch::nn::Linear(torch::nn::LinearOptions(in_features, out_features))); bn = register_module(\"bn\", torch::nn::BatchNorm1d(out_features)); } // \u7c7b\u7684forword\u6210\u5458\u51fd\u6570 torch::Tensor LinearBnReluImpl::forward(torch::Tensor x){ x = torch::relu(ln->forward(x)); x = bn(x); return x; } \u5728MLP\u7684\u6784\u9020\u7ebf\u6027\u5c42\u6a21\u5757\u7c7b\u65f6\uff0c\u6211\u4eec\u7ee7\u627f\u4e86 torch::nn::Module \u7c7b\uff0c\u5c06\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad\u6a21\u5757\u4f5c\u4e3apublic\uff0c\u53ef\u4ee5\u7ed9\u5bf9\u8c61\u4f7f\u7528\uff0c\u800c\u91cc\u9762\u7684\u7ebf\u6027\u5c42 torch::nn::Linear \u548c\u5f52\u4e00\u5316\u5c42 torch::nn::BatchNorm1d \u88ab\u9690\u85cf\u4f5c\u4e3a\u79c1\u6709\u53d8\u91cf\u3002 \u5b9a\u4e49\u6784\u9020\u51fd\u6570\u65f6\uff0c\u9700\u8981\u5c06\u539f\u672c\u7684\u6307\u9488\u5bf9\u8c61ln\u548cbn\u8fdb\u884c\u8d4b\u503c\uff0c\u540c\u65f6\u5c06\u4e24\u8005\u7684\u540d\u79f0\u4e5f\u786e\u5b9a\u3002\u524d\u5411\u4f20\u64ad\u51fd\u6570\u5c31\u548cpytorch\u4e2d\u7684forward\u7c7b\u4f3c\u3002","title":"1.MLP\u57fa\u672c\u5355\u5143"},{"location":"chapter3/#2cnn","text":"CNN\u7684\u57fa\u672c\u5355\u5143\u6784\u5efa\u548cMLP\u7684\u6784\u5efa\u7c7b\u4f3c\uff0c\u4f46\u662f\u53c8\u7a0d\u6709\u4e0d\u540c\uff0c\u9996\u5148\u9700\u8981\u5b9a\u4e49\u7684\u65f6\u5377\u79ef\u8d85\u53c2\u6570\u786e\u5b9a\u51fd\u6570\u3002 inline torch::nn::Conv2dOptions conv_options(int64_t in_planes, int64_t out_planes, int64_t kerner_size, int64_t stride = 1, int64_t padding = 0, bool with_bias = false) { torch::nn::Conv2dOptions conv_options = torch::nn::Conv2dOptions(in_planes, out_planes, kerner_size); conv_options.stride(stride); conv_options.padding(padding); conv_options.bias(with_bias); return conv_options; } \u8be5\u51fd\u6570\u8fd4\u56de torch::nn::Conv2dOptions \u5bf9\u8c61\uff0c\u5bf9\u8c61\u7684\u8d85\u53c2\u6570\u7531\u51fd\u6570\u63a5\u53e3\u6307\u5b9a\uff0c\u8fd9\u6837\u53ef\u4ee5\u65b9\u4fbf\u4f7f\u7528\u3002\u540c\u65f6\u6307\u5b9ainline(\u5185\u8054\u51fd\u6570\uff09\uff0c\u63d0\u9ad8Release\u6a21\u5f0f\u4e0b\u4ee3\u7801\u6267\u884c\u6548\u7387\u3002 C++ \u5185\u8054\u51fd\u6570\u662f\u901a\u5e38\u4e0e\u7c7b\u4e00\u8d77\u4f7f\u7528\u3002\u5982\u679c\u4e00\u4e2a\u51fd\u6570\u662f\u5185\u8054\u7684\uff0c\u90a3\u4e48\u5728\u7f16\u8bd1\u65f6\uff0c\u7f16\u8bd1\u5668\u4f1a\u628a\u8be5\u51fd\u6570\u7684\u4ee3\u7801\u526f\u672c\u653e\u7f6e\u5728\u6bcf\u4e2a\u8c03\u7528\u8be5\u51fd\u6570\u7684\u5730\u65b9\u3002 \u5bf9\u5185\u8054\u51fd\u6570\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\uff0c\u90fd\u9700\u8981\u91cd\u65b0\u7f16\u8bd1\u51fd\u6570\u7684\u6240\u6709\u5ba2\u6237\u7aef\uff0c\u56e0\u4e3a\u7f16\u8bd1\u5668\u9700\u8981\u91cd\u65b0\u66f4\u6362\u4e00\u6b21\u6240\u6709\u7684\u4ee3\u7801\uff0c\u5426\u5219\u5c06\u4f1a\u7ee7\u7eed\u4f7f\u7528\u65e7\u7684\u51fd\u6570\u3002 \u5982\u679c\u60f3\u628a\u4e00\u4e2a\u51fd\u6570\u5b9a\u4e49\u4e3a\u5185\u8054\u51fd\u6570\uff0c\u5219\u9700\u8981\u5728\u51fd\u6570\u540d\u524d\u9762\u653e\u7f6e\u5173\u952e\u5b57 inline\uff0c\u5728\u8c03\u7528\u51fd\u6570\u4e4b\u524d\u9700\u8981\u5bf9\u51fd\u6570\u8fdb\u884c\u5b9a\u4e49\u3002\u5982\u679c\u5df2\u5b9a\u4e49\u7684\u51fd\u6570\u591a\u4e8e\u4e00\u884c\uff0c\u7f16\u8bd1\u5668\u4f1a\u5ffd\u7565 inline \u9650\u5b9a\u7b26\u3002 \u968f\u540e\u5219\u662f\u548cMLP\u7684\u7ebf\u6027\u6a21\u5757\u7c7b\u4f3c\uff0cCNN\u7684\u57fa\u672c\u6a21\u5757\u7531\u5377\u79ef\u5c42\uff0c\u6fc0\u6d3b\u51fd\u6570\u548c\u5f52\u4e00\u5316\u5c42\u7ec4\u6210\u3002\u4ee3\u7801\u5982\u4e0b\uff1a class ConvReluBnImpl : public torch::nn::Module { public: ConvReluBnImpl(int input_channel=3, int output_channel=64, int kernel_size = 3, int stride = 1); // \u6784\u9020\u51fd\u6570 torch::Tensor forward(torch::Tensor x); //forward\u6210\u5458\u51fd\u6570 private: // Declare layers torch::nn::Conv2d conv{ nullptr }; torch::nn::BatchNorm2d bn{ nullptr }; }; TORCH_MODULE(ConvReluBn); // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 ConvReluBnImpl::ConvReluBnImpl(int input_channel, int output_channel, int kernel_size, int stride) { conv = register_module(\"conv\", torch::nn::Conv2d(conv_options(input_channel,output_channel,kernel_size,stride,kernel_size/2))); bn = register_module(\"bn\", torch::nn::BatchNorm2d(output_channel)); } // forward\u51fd\u6570\u7684\u5b9e\u73b0 torch::Tensor ConvReluBnImpl::forward(torch::Tensor x) { x = torch::relu(conv->forward(x)); x = bn(x); return x; } \u6bcf\u4e00\u5c42\u7684\u5b9e\u73b0\u5747\u662f\u901a\u8fc7\u524d\u9762\u5b9a\u4e49\u7684\u57fa\u672c\u6a21\u5757LinearBnRelu\u3002","title":"2.CNN\u57fa\u672c\u5355\u5143"},{"location":"chapter3/#2mlp","text":"\u5728MLP\u7684\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u4ee5\u642d\u5efa\u4e00\u4e2a\u56db\u5c42\u611f\u77e5\u673a\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528cpp\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8be5\u611f\u77e5\u673a\u63a5\u53d7in_features\u4e2a\u7279\u5f81\uff0c\u8f93\u51faout_features\u4e2a\u7f16\u7801\u540e\u7684\u7279\u5f81\u3002\u4e2d\u95f4\u7279\u5f81\u6570\u5b9a\u4e49\u4e3a32\uff0c64\u548c128\u3002 class MLP: public torch::nn::Module{ // \u7ee7\u627f public: MLP(int in_features, int out_features); torch::Tensor forward(torch::Tensor x); private: int mid_features[3] = {32,64,128}; LinearBnRelu ln1{nullptr}; LinearBnRelu ln2{nullptr}; LinearBnRelu ln3{nullptr}; torch::nn::Linear out_ln{nullptr}; }; // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 MLP::MLP(int in_features, int out_features){ ln1 = LinearBnRelu(in_features, mid_features[0]); ln2 = LinearBnRelu(mid_features[0], mid_features[1]); ln3 = LinearBnRelu(mid_features[1], mid_features[2]); out_ln = torch::nn::Linear(mid_features[2], out_features); ln1 = register_module(\"ln1\", ln1); ln2 = register_module(\"ln2\", ln2); ln3 = register_module(\"ln3\", ln3); out_ln = register_module(\"out_ln\",out_ln); } // forward\u51fd\u6570\u7684\u5b9e\u73b0 torch::Tensor MLP::forward(torch::Tensor x){ x = ln1->forward(x); //\u6307\u9488\u6210\u5458\u7684\u8bbf\u95ee\u8c03\u7528 x = ln2->forward(x); x = ln3->forward(x); x = out_ln->forward(x); return x; } \u6bcf\u4e00\u5c42\u7684\u5b9e\u73b0\u5747\u662f\u901a\u8fc7\u524d\u9762\u5b9a\u4e49\u7684\u57fa\u672c\u6a21\u5757LinearBnRelu\u3002","title":"2.\u7b80\u5355\u7684MLP"},{"location":"chapter3/#3cnn","text":"\u524d\u9762\u4ecb\u7ecd\u4e86\u6784\u5efaCNN\u7684\u57fa\u672c\u6a21\u5757ConvReluBn\uff0c\u63a5\u4e0b\u6765\u5c1d\u8bd5\u7528c++\u642d\u5efaCNN\u6a21\u578b\u3002\u8be5CNN\u7531\u4e09\u4e2astage\u7ec4\u6210\uff0c\u6bcf\u4e2astage\u53c8\u7531\u4e00\u4e2a\u5377\u79ef\u5c42\u4e00\u4e2a\u4e0b\u91c7\u6837\u5c42\u7ec4\u6210\u3002\u8fd9\u6837\u76f8\u5f53\u4e8e\u5bf9\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u4e868\u500d\u4e0b\u91c7\u6837\u3002\u4e2d\u95f4\u5c42\u7684\u901a\u9053\u6570\u53d8\u5316\u4e0e\u524d\u9762MLP\u7279\u5f81\u6570\u53d8\u5316\u76f8\u540c\uff0c\u5747\u4e3a\u8f93\u5165->32->64->128->\u8f93\u51fa\u3002 class plainCNN : public torch::nn::Module{ public: plainCNN(int in_channels, int out_channels); torch::Tensor forward(torch::Tensor x); private: int mid_channels[3] = {32,64,128}; ConvReluBn conv1{nullptr}; ConvReluBn down1{nullptr}; ConvReluBn conv2{nullptr}; ConvReluBn down2{nullptr}; ConvReluBn conv3{nullptr}; ConvReluBn down3{nullptr}; torch::nn::Conv2d out_conv{nullptr}; }; // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 plainCNN::plainCNN(int in_channels, int out_channels){ conv1 = ConvReluBn(in_channels,mid_channels[0],3); down1 = ConvReluBn(mid_channels[0],mid_channels[0],3,2); conv2 = ConvReluBn(mid_channels[0],mid_channels[1],3); down2 = ConvReluBn(mid_channels[1],mid_channels[1],3,2); conv3 = ConvReluBn(mid_channels[1],mid_channels[2],3); down3 = ConvReluBn(mid_channels[2],mid_channels[2],3,2); out_conv = torch::nn::Conv2d(conv_options(mid_channels[2],out_channels,3)); conv1 = register_module(\"conv1\",conv1); down1 = register_module(\"down1\",down1); conv2 = register_module(\"conv2\",conv2); down2 = register_module(\"down2\",down2); conv3 = register_module(\"conv3\",conv3); down3 = register_module(\"down3\",down3); out_conv = register_module(\"out_conv\",out_conv); } // forward \u65b9\u6cd5\u7684\u4f7f\u7528 torch::Tensor plainCNN::forward(torch::Tensor x){ x = conv1->forward(x); x = down1->forward(x); x = conv2->forward(x); x = down2->forward(x); x = conv3->forward(x); x = down3->forward(x); x = out_conv->forward(x); return x; } \u5047\u5b9a\u8f93\u5165\u4e00\u4e2a\u4e09\u901a\u9053\u56fe\u7247\uff0c\u8f93\u51fa\u901a\u9053\u6570\u5b9a\u4e49\u4e3an\uff0c\u8f93\u5165\u8868\u793a\u4e00\u4e2a [1,3,224,224] \u7684\u5f20\u91cf\uff0c\u5c06\u5f97\u5230\u4e00\u4e2a [1,n,28,28] \u7684\u8f93\u51fa\u5f20\u91cf\u3002","title":"3.\u7b80\u5355CNN"},{"location":"chapter3/#4lstm","text":"\u6700\u540e\u5219\u662f\u4e00\u4e2a\u7b80\u5355\u7684LSTM\u7684\u4f8b\u5b50\uff0c\u7528\u4ee5\u5904\u7406\u65f6\u5e8f\u578b\u7279\u5f81\u3002\u5728\u76f4\u63a5\u4f7f\u7528 torch::nn::LSTM \u7c7b\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u5b9a\u4e00\u4e2a\u8fd4\u56de torch::nn::LSTMOptions \u5bf9\u8c61\u7684\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u63a5\u53d7\u5173\u4e8eLSTM\u7684\u8d85\u53c2\u6570\uff0c\u8fd4\u56de\u8fd9\u4e9b\u8d85\u53c2\u6570\u5b9a\u4e49\u7684\u7ed3\u679c\u3002 inline torch::nn::LSTMOptions lstmOption(int in_features, int hidden_layer_size, int num_layers, bool batch_first = false, bool bidirectional = false){ torch::nn::LSTMOptions lstmOption = torch::nn::LSTMOptions(in_features, hidden_layer_size); lstmOption.num_layers(num_layers).batch_first(batch_first).bidirectional(bidirectional); return lstmOption; } //batch_first: true for io(batch, seq, feature) else io(seq, batch, feature) class LSTM: public torch::nn::Module{ public: LSTM(int in_features, int hidden_layer_size, int out_size, int num_layers, bool batch_first); torch::Tensor forward(torch::Tensor x); private: torch::nn::LSTM lstm{nullptr}; torch::nn::Linear ln{nullptr}; std::tuple<torch::Tensor, torch::Tensor> hidden_cell; }; \u58f0\u660e\u597dLSTM\u4ee5\u540e\uff0c\u6211\u4eec\u5c06\u5185\u90e8\u7684\u521d\u59cb\u5316\u51fd\u6570\u548c\u524d\u5411\u4f20\u64ad\u51fd\u6570\u5b9e\u73b0\u5982\u4e0b\uff1a LSTM::LSTM(int in_features, int hidden_layer_size, int out_size, int num_layers, bool batch_first){ lstm = torch::nn::LSTM(lstmOption(in_features, hidden_layer_size, num_layers, batch_first)); ln = torch::nn::Linear(hidden_layer_size, out_size); lstm = register_module(\"lstm\",lstm); ln = register_module(\"ln\",ln); } torch::Tensor LSTM::forward(torch::Tensor x){ auto lstm_out = lstm->forward(x); auto predictions = ln->forward(std::get<0>(lstm_out)); return predictions.select(1,-1); } \u611f\u8c22\u5927\u4f6c\u5f00\u6e90\uff1a libtorch\u6559\u7a0b\uff08\u4e09\uff09","title":"4.\u7b80\u5355LSTM"},{"location":"chapter4/","text":"\u7b2c\u56db\u7ae0 \u6570\u636e\u52a0\u8f7d\u6a21\u5757 \u672c\u7ae0\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528libtorch\u81ea\u5e26\u7684\u6570\u636e\u52a0\u8f7d\u6a21\u5757\uff0c\u4f7f\u7528\u8be5\u6a21\u5757\u662f\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u7684\u91cd\u8981\u6761\u4ef6\u3002\u9664\u975e\u8fd9\u4e2a\u6570\u636e\u52a0\u8f7d\u6a21\u5757\u529f\u80fd\u4e0d\u591f\uff0c\u4e0d\u7136\u7ee7\u627flibtorch\u7684\u6570\u636e\u52a0\u8f7d\u7c7b\u8fd8\u662f\u5f88\u6709\u5fc5\u8981\u7684\uff0c\u7b80\u5355\u9ad8\u6548\u3002 1.\u4f7f\u7528\u524d\u7f6e\u6761\u4ef6 libtorch\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u57fa\u7c7b\u4f9b\u7528\u6237\u81ea\u5b9a\u4e49\u6d3e\u751f\u7c7b\uff0c torch::data::Dataset \u5c31\u662f\u5176\u4e2d\u4e00\u4e2a\u5e38\u7528\u57fa\u7c7b\u3002\u4f7f\u7528\u8be5\u7c7b\u9700\u8981\u660e\u767d\u57fa\u7c7b\u548c\u6d3e\u751f\u7c7b\uff0c\u4ee5\u53ca\u6240\u8c13\u7684\u7ee7\u627f\u548c\u591a\u6001\u3002\u6709c++\u7f16\u7a0b\u7ecf\u9a8c\u8005\u5e94\u8be5\u90fd\u4e0d\u4f1a\u964c\u751f\uff0c\u4e3a\u65b9\u4fbf\u4e0d\u540c\u9636\u6bb5\u8bfb\u8005\u5c31\u7b80\u5355\u89e3\u91ca\u4e00\u4e0b\u5427\u3002\u7c7b\u5c31\u662f\u7236\u4eb2\uff0c\u53ef\u4ee5\u751f\u51fa\u4e0d\u540c\u7684\u513f\u5b50\uff0c\u751f\u513f\u5b50\u53eb\u6d3e\u751f\u6216\u8005\u7ee7\u627f(\u770b\u4f7f\u7528\u8bed\u5883)\uff0c\u751f\u4e0d\u540c\u7684\u513f\u5b50\u5c31\u5b9e\u73b0\u4e86\u591a\u6001\u3002\u7236\u4eb2\u5c31\u662f\u57fa\u7c7b\uff0c\u513f\u5b50\u5c31\u662f\u6d3e\u751f\u7c7b\u3002\u73b0\u5b9e\u4e2d\uff0c\u7236\u4eb2\u4f1a\u628a\u81ea\u8eab\u7684\u4e00\u90e8\u5206\u8d22\u4ea7\u7559\u4e0b\u6765\u517b\u8001\uff0c\u513f\u5b50\u4eec\u90fd\u4e0d\u80fd\u78b0\uff0c\u8fd9\u5c31\u662fprivate\u4e86\uff0c\u90e8\u5206\u8d22\u4ea7\u513f\u5b50\u80fd\u7528\uff0c\u4f46\u662f\u513f\u5b50\u7684\u5bf9\u8c61\u4e0d\u80fd\u7528\uff0c\u8fd9\u53ebprotected\uff0c\u8fd8\u6709\u4e9b\u8d22\u4ea7\u8c01\u90fd\u80fd\u7528\u5c31\u662fpublic\u3002\u548c\u73b0\u5b9e\u4e2d\u7684\u7236\u5b50\u7c7b\u4f3c\uff0c\u4ee3\u7801\u4e2d\uff0c\u6d3e\u751f\u7c7b\u53ef\u4ee5\u4f7f\u7528\u7236\u7c7b\u7684\u90e8\u5206\u5c5e\u6027\u6216\u8005\u51fd\u6570\uff0c\u5168\u770b\u7236\u7c7b\u600e\u6837\u5b9a\u4e49\u3002 \u7136\u540e\u7406\u89e3\u4e00\u4e0b\u865a\u51fd\u6570\uff0c\u5c31\u662f\u7236\u4eb2\u6307\u5b9a\u4e86\u90e8\u5206\u8d22\u4ea7\u662fpublic\u7684\uff0c\u4f46\u662f\u662f\u7528\u6765\u4e70\u623f\u7684\uff0c\u4e0d\u540c\u7684\u513f\u5b50\u53ef\u4ee5\u4e70\u4e0d\u540c\u7684\u623f\u5b50\uff0c\u53ef\u4ee5\u5168\u6b3e\u53ef\u4ee5\u8d37\u6b3e\uff0c\u8fd9\u5c31\u662f\u8d22\u4ea7\u5728\u7236\u4eb2\u90a3\u5c31\u662fvirtual\u7684\u3002\u5b50\u7c7b\u8981\u7ee7\u627f\u8fd9\u4e2avirtual\u8d22\u4ea7\u53ef\u4ee5\u81ea\u5df1\u91cd\u65b0\u89c4\u5212\u4f7f\u7528\u65b9\u5f0f\u3002 \u4e8b\u5b9e\u4e0a\uff0c\u5982\u679c\u6709\u8fc7pytorch\u7684\u7f16\u7a0b\u7ecf\u9a8c\u8005\u5f88\u5feb\u4f1a\u53d1\u73b0\uff0clibtorch\u7684Dataset\u7c7b\u7684\u4f7f\u7528\u548cpython\u4e0b\u4f7f\u7528\u975e\u5e38\u76f8\u50cf\u3002pytorch\u81ea\u5b9a\u4e49dataload\uff0c\u9700\u8981\u5b9a\u4e49\u597dDataset\u7684\u6d3e\u751f\u7c7b\uff0c\u5305\u62ec\u521d\u59cb\u5316\u51fd\u6570 init \uff0c\u83b7\u53d6\u51fd\u6570 getitem \u4ee5\u53ca\u6570\u636e\u96c6\u5927\u5c0f\u51fd\u6570 len \u3002\u7c7b\u4f3c\u7684\uff0clibtorch\u4e2d\u540c\u6837\u9700\u8981\u5904\u7406\u597d\u521d\u59cb\u5316\u51fd\u6570\uff0c get() \u51fd\u6570\u548c size() \u51fd\u6570\u3002 2.\u56fe\u7247\u6587\u4ef6\u904d\u5386 \u4e0b\u9762\u4ee5\u5206\u7c7b\u4efb\u52a1\u4e3a\u4f8b\uff0c\u4ecb\u7ecdlibtorch\u7684Dataset\u7c7b\u7684\u4f7f\u7528\u3002\u4f7f\u7528pytorch\u5b98\u7f51\u63d0\u4f9b\u7684 \u6606\u866b\u5206\u7c7b\u6570\u636e\u96c6 \uff0c\u4e0b\u8f7d\u5230\u672c\u5730\u89e3\u538b\u3002\u5c06\u8be5\u6570\u636e\u96c6\u6839\u76ee\u5f55\u4f5c\u4e3a\u7d22\u5f15\uff0c\u5b9e\u73b0Dataloader\u5bf9\u56fe\u7247\u7684\u52a0\u8f7d\u3002 \u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u52a0\u8f7d\u56fe\u7247\u7684\u51fd\u6570\uff0c\u4f7f\u7528\u7f51\u4e0a\u51fa\u73b0\u8f83\u591a\u7684c++\u904d\u5386\u6587\u4ef6\u5939\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u7a0d\u4f5c\u4fee\u6539\u5982\u4e0b\uff1a //\u904d\u5386\u8be5\u76ee\u5f55\u4e0b\u7684.jpg\u56fe\u7247 //\u51fd\u6570\u58f0\u660e // https://github.com/AllentDan/LibtorchTutorials/tree/main/lesson4-DatasetUtilization void load_data_from_folder(std::string image_dir, std::string type, std::vector<std::string> &list_images, std::vector<int> &list_labels, int label); void load_data_from_folder(std::string path, std::string type, std::vector<std::string> &list_images, std::vector<int> &list_labels, int label) { // \u58f0\u660e\u53d8\u91cf long long hFile = 0; //\u53e5\u67c4 struct _finddata_t fileInfo; std::string pathName; if ((hFile = _findfirst(pathName.assign(path).append(\"\\\\*.*\").c_str(), &fileInfo)) == -1) { return; } do { const char* s = fileInfo.name; const char* t = type.data(); if (fileInfo.attrib&_A_SUBDIR) //\u662f\u5b50\u6587\u4ef6\u5939 { //\u904d\u5386\u5b50\u6587\u4ef6\u5939\u4e2d\u7684\u6587\u4ef6(\u5939) if (strcmp(s, \".\") == 0 || strcmp(s, \"..\") == 0) //\u5b50\u6587\u4ef6\u5939\u76ee\u5f55\u662f.\u6216\u8005.. continue; std::string sub_path = path + \"\\\\\" + fileInfo.name; label++; load_data_from_folder(sub_path, type, list_images, list_labels, label); } else //\u5224\u65ad\u662f\u4e0d\u662f\u540e\u7f00\u4e3atype\u6587\u4ef6 { if (strstr(s, t)) { std::string image_path = path + \"\\\\\" + fileInfo.name; list_images.push_back(image_path); list_labels.push_back(label); } } } while (_findnext(hFile, &fileInfo) == 0); return; } \u4fee\u6539\u540e\u7684\u51fd\u6570\u63a5\u53d7\u6570\u636e\u96c6\u6587\u4ef6\u5939\u8def\u5f84image_dir\u548c\u56fe\u7247\u7c7b\u578bimage_type\uff0c\u5c06\u904d\u5386\u5230\u7684\u56fe\u7247\u8def\u5f84\u548c\u5176\u7c7b\u522b\u5206\u522b\u5b58\u50a8\u5230list_images\u548clist_labels\uff0c\u6700\u540elable\u53d8\u91cf\u7528\u4e8e\u8868\u793a\u7c7b\u522b\u8ba1\u6570\u3002\u4f20\u5165lable=-1\uff0c\u8fd4\u56de\u7684lable\u503c\u52a0\u4e00\u540e\u7b49\u4e8e\u56fe\u7247\u7c7b\u522b\u3002 3.\u81ea\u5b9a\u4e49Dataset \u5b9a\u4e49dataSetClc\uff0c\u8be5\u7c7b\u7ee7\u627f\u81ea torch::data::Dataset \u3002\u5b9a\u4e49\u79c1\u6709\u53d8\u91cfimage_paths\u548clabels\u5206\u522b\u5b58\u50a8\u56fe\u7247\u8def\u5f84\u548c\u7c7b\u522b\uff0c\u662f\u4e24\u4e2avector\u53d8\u91cf\u3002dataSetClc\u7684\u521d\u59cb\u5316\u51fd\u6570\u5c31\u662f\u52a0\u8f7d\u56fe\u7247\u548c\u7c7b\u522b\u3002\u901a\u8fc7get()\u51fd\u6570\u8fd4\u56de\u7531\u56fe\u50cf\u548c\u7c7b\u522b\u6784\u6210\u7684\u5f20\u91cf\u5217\u8868\u3002\u53ef\u4ee5\u5728get()\u51fd\u6570\u4e2d\u505a\u4efb\u610f\u9488\u5bf9\u56fe\u50cf\u7684\u64cd\u4f5c\uff0c\u5982\u6570\u636e\u589e\u5f3a\u7b49\u3002\u6548\u679c\u7b49\u4ef7\u4e8epytorch\u4e2d\u7684getitem\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u3002 class dataSetClc:public torch::data::Dataset<dataSetClc>{ public: int class_index = 0; dataSetClc(std::string image_dir, std::string type){ load_data_from_folder(image_dir, std::string(type), image_paths, labels, class_index-1); } // Override get() function to return tensor at location index // \u91cd\u5199get()\u548csize()\u65b9\u6cd5 torch::data::Example<> get(size_t index) override{ std::string image_path = image_paths.at(index); //vector\u7684\u5207\u7247 cv::Mat image = cv::imread(image_path); cv::resize(image, image, cv::Size(224, 224)); //\u5c3a\u5bf8\u7edf\u4e00\uff0c\u7528\u4e8e\u5f20\u91cfstack\uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528stack int label = labels.at(index); torch::Tensor img_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }); // Channels x Height x Width torch::Tensor label_tensor = torch::full({ 1 }, label); return {img_tensor.clone(), label_tensor.clone()}; } // Override size() function, return the length of data torch::optional<size_t> size() const override { return image_paths.size(); }; private: std::vector<std::string> image_paths; std::vector<int> labels; }; 4.\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Dataset \u4e0b\u9762\u4f7f\u7528\u5b9a\u4e49\u597d\u7684\u6570\u636e\u52a0\u8f7d\u7c7b\uff0c\u4ee5\u6606\u866b\u5206\u7c7b\u4e2d\u7684\u8bad\u7ec3\u96c6\u4f5c\u4e3a\u6d4b\u8bd5\uff0c\u4ee3\u7801\u5982\u4e0b\u3002\u53ef\u4ee5\u6253\u5370\u52a0\u8f7d\u7684\u56fe\u7247\u5f20\u91cf\u548c\u7c7b\u522b\u3002 int batch_size = 2; std::string image_dir = \"your path to\\\\hymenoptera_data\\\\train\"; auto mdataset = myDataset(image_dir,\".jpg\").map(torch::data::transforms::Stack<>()); auto mdataloader = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(std::move(mdataset), batch_size); for(auto &batch: *mdataloader){ auto data = batch.data; auto target = batch.target; std::cout<<data.sizes()<<target; }","title":"\u7b2c\u56db\u7ae0 \u6570\u636e\u52a0\u8f7d\u6a21\u5757"},{"location":"chapter4/#_1","text":"\u672c\u7ae0\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528libtorch\u81ea\u5e26\u7684\u6570\u636e\u52a0\u8f7d\u6a21\u5757\uff0c\u4f7f\u7528\u8be5\u6a21\u5757\u662f\u5b9e\u73b0\u6a21\u578b\u8bad\u7ec3\u7684\u91cd\u8981\u6761\u4ef6\u3002\u9664\u975e\u8fd9\u4e2a\u6570\u636e\u52a0\u8f7d\u6a21\u5757\u529f\u80fd\u4e0d\u591f\uff0c\u4e0d\u7136\u7ee7\u627flibtorch\u7684\u6570\u636e\u52a0\u8f7d\u7c7b\u8fd8\u662f\u5f88\u6709\u5fc5\u8981\u7684\uff0c\u7b80\u5355\u9ad8\u6548\u3002","title":"\u7b2c\u56db\u7ae0 \u6570\u636e\u52a0\u8f7d\u6a21\u5757"},{"location":"chapter4/#1","text":"libtorch\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u57fa\u7c7b\u4f9b\u7528\u6237\u81ea\u5b9a\u4e49\u6d3e\u751f\u7c7b\uff0c torch::data::Dataset \u5c31\u662f\u5176\u4e2d\u4e00\u4e2a\u5e38\u7528\u57fa\u7c7b\u3002\u4f7f\u7528\u8be5\u7c7b\u9700\u8981\u660e\u767d\u57fa\u7c7b\u548c\u6d3e\u751f\u7c7b\uff0c\u4ee5\u53ca\u6240\u8c13\u7684\u7ee7\u627f\u548c\u591a\u6001\u3002\u6709c++\u7f16\u7a0b\u7ecf\u9a8c\u8005\u5e94\u8be5\u90fd\u4e0d\u4f1a\u964c\u751f\uff0c\u4e3a\u65b9\u4fbf\u4e0d\u540c\u9636\u6bb5\u8bfb\u8005\u5c31\u7b80\u5355\u89e3\u91ca\u4e00\u4e0b\u5427\u3002\u7c7b\u5c31\u662f\u7236\u4eb2\uff0c\u53ef\u4ee5\u751f\u51fa\u4e0d\u540c\u7684\u513f\u5b50\uff0c\u751f\u513f\u5b50\u53eb\u6d3e\u751f\u6216\u8005\u7ee7\u627f(\u770b\u4f7f\u7528\u8bed\u5883)\uff0c\u751f\u4e0d\u540c\u7684\u513f\u5b50\u5c31\u5b9e\u73b0\u4e86\u591a\u6001\u3002\u7236\u4eb2\u5c31\u662f\u57fa\u7c7b\uff0c\u513f\u5b50\u5c31\u662f\u6d3e\u751f\u7c7b\u3002\u73b0\u5b9e\u4e2d\uff0c\u7236\u4eb2\u4f1a\u628a\u81ea\u8eab\u7684\u4e00\u90e8\u5206\u8d22\u4ea7\u7559\u4e0b\u6765\u517b\u8001\uff0c\u513f\u5b50\u4eec\u90fd\u4e0d\u80fd\u78b0\uff0c\u8fd9\u5c31\u662fprivate\u4e86\uff0c\u90e8\u5206\u8d22\u4ea7\u513f\u5b50\u80fd\u7528\uff0c\u4f46\u662f\u513f\u5b50\u7684\u5bf9\u8c61\u4e0d\u80fd\u7528\uff0c\u8fd9\u53ebprotected\uff0c\u8fd8\u6709\u4e9b\u8d22\u4ea7\u8c01\u90fd\u80fd\u7528\u5c31\u662fpublic\u3002\u548c\u73b0\u5b9e\u4e2d\u7684\u7236\u5b50\u7c7b\u4f3c\uff0c\u4ee3\u7801\u4e2d\uff0c\u6d3e\u751f\u7c7b\u53ef\u4ee5\u4f7f\u7528\u7236\u7c7b\u7684\u90e8\u5206\u5c5e\u6027\u6216\u8005\u51fd\u6570\uff0c\u5168\u770b\u7236\u7c7b\u600e\u6837\u5b9a\u4e49\u3002 \u7136\u540e\u7406\u89e3\u4e00\u4e0b\u865a\u51fd\u6570\uff0c\u5c31\u662f\u7236\u4eb2\u6307\u5b9a\u4e86\u90e8\u5206\u8d22\u4ea7\u662fpublic\u7684\uff0c\u4f46\u662f\u662f\u7528\u6765\u4e70\u623f\u7684\uff0c\u4e0d\u540c\u7684\u513f\u5b50\u53ef\u4ee5\u4e70\u4e0d\u540c\u7684\u623f\u5b50\uff0c\u53ef\u4ee5\u5168\u6b3e\u53ef\u4ee5\u8d37\u6b3e\uff0c\u8fd9\u5c31\u662f\u8d22\u4ea7\u5728\u7236\u4eb2\u90a3\u5c31\u662fvirtual\u7684\u3002\u5b50\u7c7b\u8981\u7ee7\u627f\u8fd9\u4e2avirtual\u8d22\u4ea7\u53ef\u4ee5\u81ea\u5df1\u91cd\u65b0\u89c4\u5212\u4f7f\u7528\u65b9\u5f0f\u3002 \u4e8b\u5b9e\u4e0a\uff0c\u5982\u679c\u6709\u8fc7pytorch\u7684\u7f16\u7a0b\u7ecf\u9a8c\u8005\u5f88\u5feb\u4f1a\u53d1\u73b0\uff0clibtorch\u7684Dataset\u7c7b\u7684\u4f7f\u7528\u548cpython\u4e0b\u4f7f\u7528\u975e\u5e38\u76f8\u50cf\u3002pytorch\u81ea\u5b9a\u4e49dataload\uff0c\u9700\u8981\u5b9a\u4e49\u597dDataset\u7684\u6d3e\u751f\u7c7b\uff0c\u5305\u62ec\u521d\u59cb\u5316\u51fd\u6570 init \uff0c\u83b7\u53d6\u51fd\u6570 getitem \u4ee5\u53ca\u6570\u636e\u96c6\u5927\u5c0f\u51fd\u6570 len \u3002\u7c7b\u4f3c\u7684\uff0clibtorch\u4e2d\u540c\u6837\u9700\u8981\u5904\u7406\u597d\u521d\u59cb\u5316\u51fd\u6570\uff0c get() \u51fd\u6570\u548c size() \u51fd\u6570\u3002","title":"1.\u4f7f\u7528\u524d\u7f6e\u6761\u4ef6"},{"location":"chapter4/#2","text":"\u4e0b\u9762\u4ee5\u5206\u7c7b\u4efb\u52a1\u4e3a\u4f8b\uff0c\u4ecb\u7ecdlibtorch\u7684Dataset\u7c7b\u7684\u4f7f\u7528\u3002\u4f7f\u7528pytorch\u5b98\u7f51\u63d0\u4f9b\u7684 \u6606\u866b\u5206\u7c7b\u6570\u636e\u96c6 \uff0c\u4e0b\u8f7d\u5230\u672c\u5730\u89e3\u538b\u3002\u5c06\u8be5\u6570\u636e\u96c6\u6839\u76ee\u5f55\u4f5c\u4e3a\u7d22\u5f15\uff0c\u5b9e\u73b0Dataloader\u5bf9\u56fe\u7247\u7684\u52a0\u8f7d\u3002 \u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u52a0\u8f7d\u56fe\u7247\u7684\u51fd\u6570\uff0c\u4f7f\u7528\u7f51\u4e0a\u51fa\u73b0\u8f83\u591a\u7684c++\u904d\u5386\u6587\u4ef6\u5939\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u7a0d\u4f5c\u4fee\u6539\u5982\u4e0b\uff1a //\u904d\u5386\u8be5\u76ee\u5f55\u4e0b\u7684.jpg\u56fe\u7247 //\u51fd\u6570\u58f0\u660e // https://github.com/AllentDan/LibtorchTutorials/tree/main/lesson4-DatasetUtilization void load_data_from_folder(std::string image_dir, std::string type, std::vector<std::string> &list_images, std::vector<int> &list_labels, int label); void load_data_from_folder(std::string path, std::string type, std::vector<std::string> &list_images, std::vector<int> &list_labels, int label) { // \u58f0\u660e\u53d8\u91cf long long hFile = 0; //\u53e5\u67c4 struct _finddata_t fileInfo; std::string pathName; if ((hFile = _findfirst(pathName.assign(path).append(\"\\\\*.*\").c_str(), &fileInfo)) == -1) { return; } do { const char* s = fileInfo.name; const char* t = type.data(); if (fileInfo.attrib&_A_SUBDIR) //\u662f\u5b50\u6587\u4ef6\u5939 { //\u904d\u5386\u5b50\u6587\u4ef6\u5939\u4e2d\u7684\u6587\u4ef6(\u5939) if (strcmp(s, \".\") == 0 || strcmp(s, \"..\") == 0) //\u5b50\u6587\u4ef6\u5939\u76ee\u5f55\u662f.\u6216\u8005.. continue; std::string sub_path = path + \"\\\\\" + fileInfo.name; label++; load_data_from_folder(sub_path, type, list_images, list_labels, label); } else //\u5224\u65ad\u662f\u4e0d\u662f\u540e\u7f00\u4e3atype\u6587\u4ef6 { if (strstr(s, t)) { std::string image_path = path + \"\\\\\" + fileInfo.name; list_images.push_back(image_path); list_labels.push_back(label); } } } while (_findnext(hFile, &fileInfo) == 0); return; } \u4fee\u6539\u540e\u7684\u51fd\u6570\u63a5\u53d7\u6570\u636e\u96c6\u6587\u4ef6\u5939\u8def\u5f84image_dir\u548c\u56fe\u7247\u7c7b\u578bimage_type\uff0c\u5c06\u904d\u5386\u5230\u7684\u56fe\u7247\u8def\u5f84\u548c\u5176\u7c7b\u522b\u5206\u522b\u5b58\u50a8\u5230list_images\u548clist_labels\uff0c\u6700\u540elable\u53d8\u91cf\u7528\u4e8e\u8868\u793a\u7c7b\u522b\u8ba1\u6570\u3002\u4f20\u5165lable=-1\uff0c\u8fd4\u56de\u7684lable\u503c\u52a0\u4e00\u540e\u7b49\u4e8e\u56fe\u7247\u7c7b\u522b\u3002","title":"2.\u56fe\u7247\u6587\u4ef6\u904d\u5386"},{"location":"chapter4/#3dataset","text":"\u5b9a\u4e49dataSetClc\uff0c\u8be5\u7c7b\u7ee7\u627f\u81ea torch::data::Dataset \u3002\u5b9a\u4e49\u79c1\u6709\u53d8\u91cfimage_paths\u548clabels\u5206\u522b\u5b58\u50a8\u56fe\u7247\u8def\u5f84\u548c\u7c7b\u522b\uff0c\u662f\u4e24\u4e2avector\u53d8\u91cf\u3002dataSetClc\u7684\u521d\u59cb\u5316\u51fd\u6570\u5c31\u662f\u52a0\u8f7d\u56fe\u7247\u548c\u7c7b\u522b\u3002\u901a\u8fc7get()\u51fd\u6570\u8fd4\u56de\u7531\u56fe\u50cf\u548c\u7c7b\u522b\u6784\u6210\u7684\u5f20\u91cf\u5217\u8868\u3002\u53ef\u4ee5\u5728get()\u51fd\u6570\u4e2d\u505a\u4efb\u610f\u9488\u5bf9\u56fe\u50cf\u7684\u64cd\u4f5c\uff0c\u5982\u6570\u636e\u589e\u5f3a\u7b49\u3002\u6548\u679c\u7b49\u4ef7\u4e8epytorch\u4e2d\u7684getitem\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u3002 class dataSetClc:public torch::data::Dataset<dataSetClc>{ public: int class_index = 0; dataSetClc(std::string image_dir, std::string type){ load_data_from_folder(image_dir, std::string(type), image_paths, labels, class_index-1); } // Override get() function to return tensor at location index // \u91cd\u5199get()\u548csize()\u65b9\u6cd5 torch::data::Example<> get(size_t index) override{ std::string image_path = image_paths.at(index); //vector\u7684\u5207\u7247 cv::Mat image = cv::imread(image_path); cv::resize(image, image, cv::Size(224, 224)); //\u5c3a\u5bf8\u7edf\u4e00\uff0c\u7528\u4e8e\u5f20\u91cfstack\uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528stack int label = labels.at(index); torch::Tensor img_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }); // Channels x Height x Width torch::Tensor label_tensor = torch::full({ 1 }, label); return {img_tensor.clone(), label_tensor.clone()}; } // Override size() function, return the length of data torch::optional<size_t> size() const override { return image_paths.size(); }; private: std::vector<std::string> image_paths; std::vector<int> labels; };","title":"3.\u81ea\u5b9a\u4e49Dataset"},{"location":"chapter4/#4dataset","text":"\u4e0b\u9762\u4f7f\u7528\u5b9a\u4e49\u597d\u7684\u6570\u636e\u52a0\u8f7d\u7c7b\uff0c\u4ee5\u6606\u866b\u5206\u7c7b\u4e2d\u7684\u8bad\u7ec3\u96c6\u4f5c\u4e3a\u6d4b\u8bd5\uff0c\u4ee3\u7801\u5982\u4e0b\u3002\u53ef\u4ee5\u6253\u5370\u52a0\u8f7d\u7684\u56fe\u7247\u5f20\u91cf\u548c\u7c7b\u522b\u3002 int batch_size = 2; std::string image_dir = \"your path to\\\\hymenoptera_data\\\\train\"; auto mdataset = myDataset(image_dir,\".jpg\").map(torch::data::transforms::Stack<>()); auto mdataloader = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(std::move(mdataset), batch_size); for(auto &batch: *mdataloader){ auto data = batch.data; auto target = batch.target; std::cout<<data.sizes()<<target; }","title":"4.\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Dataset"},{"location":"chapter5/","text":"\u7b2c\u4e94\u7ae0 \u5206\u7c7b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u524d\u9762\u7684\u7ae0\u8282\u4e2d\u6211\u4eec\u4ecb\u7ecd\u4e86libtorch\u7684\u73af\u5883\u642d\u5efa\uff0clibtorch\u5f20\u91cf\u5e38\u7528\u64cd\u4f5c\uff0c\u7b80\u5355\u7684MLP\uff0cCNN\u548cLSTM\u6a21\u578b\u642d\u5efa\uff0c\u4ee5\u53ca\u6570\u636e\u52a0\u8f7d\u7c7b\u7684\u4f7f\u7528\u3002\u672c\u7ae0\u5c06\u4ee5\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e3a\u4f8b\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528c++\u8bad\u7ec3\u4e00\u4e2a\u56fe\u7247\u5206\u7c7b\u5668\u3002 1.\u6a21\u578b \u672c\u6587\u4ee5VGG\u4e3a\u4f8b\uff0c\u5bf9\u6bd4pytorch\u4e0b\u7684\u6a21\u578b\u642d\u5efa\u548c\u8bad\u7ec3\uff0c\u9610\u8ff0Libtorch\u7684\u6a21\u578b\u642d\u5efa\uff0c\u6a21\u578b\u52a0\u8f7d\u9884\u8bad\u7ec3\uff08from ImageNet\uff09\u6743\u91cd\u3002VGG\u6a21\u578b\u662f2014\u5e74\u7684ImageNet\u5206\u7c7b\u51a0\u519b\uff0c\u7531\u4e8e\u540e\u7eed\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u6dfb\u52a0\u4e86\u4e00\u4e9b\u6210\u5206\uff0c\u5982BatchNorm\uff0c\u5f62\u6210\u4e00\u4e9b\u65b0\u53d8\u79cd\u3002\u672c\u6587\u4ee5vgg16bn\u4e3a\u4f8b\u4f5c\u4ecb\u7ecd\uff0cvgg16bn\u5c31\u662fvgg16\u52a0\u4e0a\u4e86\u540e\u7eed\u63d0\u51fa\u7684BatchNorm\u5c42\u3002 1.\u5206\u6790\u6a21\u578b \u9996\u5148\u4ecb\u7ecdpytorch\u7684\u6a21\u578b\u6e90\u7801\uff0cpytorch\u7684torchvision.models.VGG\u4e2d\u6709\u63d0\u4f9b\u5b98\u65b9\u7684VGG\u6a21\u578b\u4ee3\u7801\u3002\u76f4\u63a5\u590d\u5236\u4e0a\u6765\u5206\u6790\uff1a class VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=True): super(VGG, self).__init__() self.features = features self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0) def make_layers(cfg, batch_norm=False): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v return nn.Sequential(*layers) cfgs = { 'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'], } def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs): if pretrained: kwargs['init_weights'] = False model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return model \u548c\u73b0\u5728\u7684\u590d\u6742\u6a21\u578b\u76f8\u6bd4\uff0cVGG\u6a21\u578b\u7ed3\u6784\u8f83\u4e3a\u7b80\u5355\uff0c\u5c31\u662f\u7b80\u5355\u7684\u591a\u6b21\u5377\u79ef+\u4e0b\u91c7\u6837\u5806\u53e0\uff0c\u540e\u63a5\u4e00\u4e2a\u4e09\u5c42\u7684MLP\u3002\u4ee3\u7801\u4e2dVGG\u6a21\u578b\u7c7b\u6709\u4e09\u4e2a\u6210\u5458\u51fd\u6570\uff0c\u4e00\u4e2a\u521d\u59cb\u5316\u51fd\u6570init\uff0c\u4e00\u4e2a\u524d\u5411\u4f20\u64ad\u51fd\u6570forward\uff0c\u6700\u540e\u4e00\u4e2a\u6743\u91cd\u521d\u59cb\u5316\u51fd\u6570\u3002\u7c7b\u5916\u90e8\u6709\u4e00\u4e2a\u51fd\u6570make_layers\u51fd\u6570\u7528\u4e8e\u751f\u6210CNN\u4e3b\u5e72\uff0c\u8fd4\u56de\u4e00\u4e2ann.Sequential\u5bf9\u8c61\u3002 \u6253\u5f00python\u7f16\u8f91\u5668\uff08or IDE\uff0c\u9ed8\u8ba4\u6709pytorch\u7f16\u7a0b\u7ecf\u9a8c\uff09\u3002\u8f93\u5165\u4e0b\u9762\u4ee3\u7801\uff1a from torchvision.models import vgg16,vgg16_bn model = vgg16_bn(pretrained=True) for k,v in model.named_parameters(): print(k) \u53d1\u73b0\u6253\u5370\u51fa\u6a21\u578b\u6bcf\u4e00\u5c42\uff08\u6709\u6743\u91cd\u7684\u5c42\uff0c\u4e0d\u5305\u62ec\u7c7b\u4f3c\u6fc0\u6d3b\u51fd\u6570\u5c42\uff09\u7684\u540d\u79f0\u3002 features.0.weight features.0.bias features.1.weight features.1.bias features.3.weight features.3.bias features.4.weight features.4.bias features.7.weight features.7.bias features.8.weight features.8.bias features.10.weight features.10.bias features.11.weight features.11.bias features.14.weight features.14.bias features.15.weight features.15.bias features.17.weight features.17.bias features.18.weight features.18.bias features.20.weight features.20.bias features.21.weight features.21.bias features.24.weight features.24.bias features.25.weight features.25.bias features.27.weight features.27.bias features.28.weight features.28.bias features.30.weight features.30.bias features.31.weight features.31.bias features.34.weight features.34.bias features.35.weight features.35.bias features.37.weight features.37.bias features.38.weight features.38.bias features.40.weight features.40.bias features.41.weight features.41.bias classifier.0.weight classifier.0.bias classifier.3.weight classifier.3.bias classifier.6.weight classifier.6.bias \u8fd8\u884c\uff0c\u4e0d\u662f\u5f88\u957f\u3002\u8fd9\u6b65\u64cd\u4f5c\u5bf9\u540e\u7eed\u6a21\u578b\u642d\u5efa\u548c\u52a0\u8f7d\u6743\u91cd\u5f88\u91cd\u8981\uff0c\u56e0\u4e3atorch\u7684\u6a21\u578b\u52a0\u8f7d\u5fc5\u987b\u8981\u6709\u4e00\u4e00\u5bf9\u5e94\u7684\u6743\u91cd\u5c42\u7684\u540d\u79f0\u3002\u5982\u679c\u4ee3\u7801\u4e2d\u7684\u6a21\u578b\u548c\u52a0\u8f7d\u8def\u5f84\u5bf9\u5e94\u7684\u6743\u91cd\u63d0\u4f9b\u7684\u6743\u91cd\u5c42\u540d\u79f0\u4e0d\u4e00\u81f4\uff0c\u5c31\u4f1a\u4ea7\u751f\u9519\u8bef\u3002 \u5206\u6790\u6a21\u578b\u6253\u5370\u7684\u540d\u79f0\uff0c\u5176\u5b9e\u5c31\u4f1a\u53d1\u73b0\u53ea\u6709features\uff0cclassifier\uff0cweight\u548cbias\u548c\u6570\u5b57\u3002\u8054\u7cfb\u524d\u9762\u7684\u5b98\u65b9\u4ee3\u7801\u7684\u521d\u59cb\u5316\u51fd\u6570init\uff0c\u51fd\u6570\u5185\u90e8\u6709self.classifer\u548cself.features\uff0c\u5c31\u5f88\u5bb9\u6613\u5f97\u51fapytorch\u6a21\u578b\u7684\u5185\u90e8\u5c42\u540d\u79f0\u547d\u540d\u89c4\u5f8b\u4e86\u3002weight\u548cbias\u5bf9\u5e94conv\u5c42\u91cc\u7684self.conv\u548cself.bias\u3002\u70b9\u548c\u6570\u5b57\u8868\u793ann.Sequential\u91cc\u7684\u5e8f\u53f7\u3002 2.\u642d\u5efa\u6a21\u578b \u4e0b\u9762\u5728c++\u4e2d\u642d\u5efa\u4e00\u4e2a\u548cpytorch\u4e0b\u5b8c\u5168\u4e00\u81f4\u7684vgg16bn\u3002\u5982\u679c\u4e0d\u4e00\u81f4\u7684\u8bdd\u5176\u5b9e\u4e0d\u5f71\u54cd\u6b63\u5e38\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u9884\u6d4b\uff0c\u4f46\u662f\u5f71\u54cd\u521d\u59cb\u5316\u72b6\u6001\uff0c\u6a21\u578b\u52a0\u8f7d\u4eceImageNet\u6570\u636e\u96c6\u8bad\u7ec3\u597d\u7684\u6743\u91cd\u4ee5\u540e\uff0c\u8bad\u7ec3\u6536\u655b\u7684\u901f\u5ea6\u548c\u6536\u655b\u540e\u7684\u7cbe\u5ea6\u90fd\u4f1a\u597d\u5f88\u591a\u3002 \u9996\u5148\u662f.h\u6587\u4ef6\u4e2d\u8981\u505a\u7684\uff0c\u4e00\u4e2aconv_options\u786e\u5b9a\u5377\u79ef\u8d85\u53c2\u6570\uff0c\u56e0\u4e3a\u5e38\u7528\u6240\u4ee5inline\u4e00\u4e0b\u3002maxpool_options\u51fd\u6570\u786e\u5b9aMaxPool2d\u7684\u8d85\u53c2\u6570\u3002\u5982\u4f55\u5b9a\u4e49\u4e00\u4e2a\u548cpytorch\u4e00\u81f4\u7684make_features\u51fd\u6570\uff0c\u518d\u5728VGG\u7c7b\u4e2d\u58f0\u660e\u548cpytorch\u4e00\u81f4\u7684\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad\u51fd\u6570\u3002\u6700\u540e\u5219\u662f\u4e00\u4e2avgg16bn\u51fd\u6570\uff0c\u8fd4\u56devgg16bn\u6a21\u578b\u3002 //\u548c\u524d\u9762\u7ae0\u8282\u4e00\u81f4\uff0c\u5b9a\u4e49\u4e00\u4e2a\u786e\u5b9aconv\u8d85\u53c2\u6570\u7684\u51fd\u6570 inline torch::nn::Conv2dOptions conv_options(int64_t in_planes, int64_t out_planes, int64_t kerner_size, int64_t stride = 1, int64_t padding = 0, bool with_bias = false) { torch::nn::Conv2dOptions conv_options = torch::nn::Conv2dOptions(in_planes, out_planes, kerner_size); conv_options.stride(stride); conv_options.padding(padding); conv_options.bias(with_bias); return conv_options; } //\u4eff\u7167\u4e0a\u9762\u7684conv_options\uff0c\u5b9a\u4e49\u4e00\u4e2a\u786e\u5b9aMaxPool2d\u7684\u8d85\u53c2\u6570\u7684\u51fd\u6570 inline torch::nn::MaxPool2dOptions maxpool_options(int kernel_size, int stride){ torch::nn::MaxPool2dOptions maxpool_options(kernel_size); maxpool_options.stride(stride); return maxpool_options; } //\u5bf9\u5e94pytorch\u4e2d\u7684make_features\u51fd\u6570\uff0c\u8fd4\u56deCNN\u4e3b\u4f53\uff0c\u8be5\u4e3b\u4f53\u662f\u4e00\u4e2atorch::nn::Sequential\u5bf9\u8c61 torch::nn::Sequential make_features(std::vector<int> &cfg, bool batch_norm); //VGG\u7c7b\u7684\u58f0\u660e\uff0c\u5305\u62ec\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad class VGGImpl: public torch::nn::Module { private: torch::nn::Sequential features_{nullptr}; torch::nn::AdaptiveAvgPool2d avgpool{nullptr}; torch::nn::Sequential classifier; public: VGGImpl(std::vector<int> &cfg, int num_classes = 1000, bool batch_norm = false); torch::Tensor forward(torch::Tensor x); }; TORCH_MODULE(VGG); //vgg16bn\u51fd\u6570\u7684\u58f0\u660e VGG vgg16bn(int num_classes); \u7136\u540e\u5728.cpp\u6587\u4ef6\u4e2d\u5b9a\u4e49\u597d.h\u6587\u4ef6\u4e2d\u7684\u58f0\u660e\u3002.cpp\u6587\u4ef6\u7684\u5185\u5bb9\u5982\u4e0b\uff1a // make_features \u65b9\u6cd5 torch::nn::Sequential make_features(std::vector<int> &cfg, bool batch_norm){ torch::nn::Sequential features; // \u58f0\u660e nn.sequential \u5bf9\u8c61 int in_channels = 3; //\u58f0\u660e int\u5bf9\u8c61 for(auto v : cfg){ //vector\u7684\u904d\u5386(C++\u6807\u51c6\u6a21\u677f\u5e93) if(v==-1){ features->push_back(torch::nn::MaxPool2d(maxpool_options(2,2))); } else{ auto conv2d = torch::nn::Conv2d(conv_options(in_channels,v,3,1,1)); features->push_back(conv2d); if(batch_norm){ features->push_back(torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(v))); } features->push_back(torch::nn::ReLU(torch::nn::ReLUOptions(true))); in_channels = v; } } return features; } VGGImpl::VGGImpl(std::vector<int> &cfg, int num_classes, bool batch_norm){ features_ = make_features(cfg,batch_norm); avgpool = torch::nn::AdaptiveAvgPool2d(torch::nn::AdaptiveAvgPool2dOptions(7)); classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(512 * 7 * 7, 4096))); classifier->push_back(torch::nn::ReLU(torch::nn::ReLUOptions(true))); classifier->push_back(torch::nn::Dropout()); classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(4096, 4096))); classifier->push_back(torch::nn::ReLU(torch::nn::ReLUOptions(true))); classifier->push_back(torch::nn::Dropout()); classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(4096, num_classes))); features_ = register_module(\"features\",features_); classifier = register_module(\"classifier\",classifier); } torch::Tensor VGGImpl::forward(torch::Tensor x){ x = features_->forward(x); x = avgpool(x); x = torch::flatten(x,1); x = classifier->forward(x); return torch::log_softmax(x, 1); } VGG vgg16bn(int num_classes){ std::vector<int> cfg_dd = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; VGG vgg = VGG(cfg_dd,num_classes,true); return vgg; } \u7531\u4e8ec++\u4e2d\u5143\u7ec4\u5982\u679c\u592a\u957f\u7684\u8bdd\u58f0\u660e\u4e5f\u4f1a\u5f88\u957f\uff0c\u800c\u5217\u8868\u6216\u8005vector\u53ea\u63a5\u53d7\u540c\u7c7b\u578b\u7684\u6570\u636e\uff0c\u5c31\u5c06\u539f\u6765pytorch\u4e2d\u7684cfg\u91cc\u7684\u2019M\u2019\u6539\u6210-1\u3002\u5728\u8bfb\u53d6cfg\u65f6\u5224\u65ad\u7531\u539f\u6765\u7684\u2019M\u2019\u53d8\u6210\u5224\u65ad\u662f\u5426-1\u5373\u53ef\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7ed9\u6a21\u578b\u4e0d\u540c\u5c42\u547d\u540d\u65f6\uff0c\u4ee3\u7801\u91cc\u53ea\u51fa\u73b0\u4e86register_module\u5bf9features\u548cclassifier\u547d\u540d\uff0c\u8fd9\u548cpytorch\u4fdd\u6301\u4e00\u81f4\u3002 3.\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6743\u91cd \u4e0b\u9762\u67e5\u770b\u6211\u4eecc++\u5b9a\u4e49\u7684\u6a21\u578b\u662f\u5426\u548cpytorch\u5b8c\u5168\u4e00\u81f4\u3002\u5728\u4e3b\u51fd\u6570\u4e2d\u5b9e\u4f8b\u5316\u4e00\u4e2aVGG\u7684\u5bf9\u8c61\uff0c\u7136\u540e\u6253\u5370\u5404\u4e2a\u5c42\u7684\u540d\u79f0\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a std::vector<int> cfg_16bn = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; auto vgg16bn = VGG(cfg_16bn,1000,true); auto dict16bn = vgg16bn->named_parameters(); //vector for (auto n = dict16bn.begin(); n != dict16bn.end(); n++) { std::cout<<(*n).key()<<std::endl; } \u53ef\u4ee5\u53d1\u73b0\uff0c\u5404\u4e2a\u5c42\u540d\u79f0\u548cpytorch\u4e2d\u7684\u6a21\u578b\u5185\u90e8\u5c42\u7684\u540d\u79f0\u5b8c\u5168\u4e00\u81f4\u3002\u8fd9\u6837\u6211\u4eec\u5c06pytorch\u7684\u6a21\u578b\u6743\u91cd\u4fdd\u5b58\u4e0b\u6765\uff0c\u7136\u540e\u52a0\u8f7d\u5230c++\u4e2d\u3002 import torch from torchvision.models import vgg16,vgg16_bn model=model.to(torch.device(\"cpu\")) model.eval() var=torch.ones((1,3,224,224)) traced_script_module = torch.jit.trace(model, var) # torchscript model traced_script_module.save(\"vgg16bn.pt\") \u8fd9\u6837\uff0c\u6a21\u578b\u7684\u5377\u79ef\u5c42\uff0c\u5f52\u4e00\u5316\u5c42\uff0c\u7ebf\u6027\u5c42\u7684\u6743\u91cd\u5c31\u4fdd\u5b58\u5230.pt\u6587\u4ef6\u4e2d\u4e86\u3002\u4e0b\u9762\u5c1d\u8bd5\u52a0\u8f7d\u5230c++\u4e2d\u3002c++\u4e2d\u7684\u52a0\u8f7d\u4ee3\u7801\u8f83\u4e3a\u7b80\u5355\uff0c\u76f4\u63a5\u5728\u5b9a\u4e49\u597d\u7684vgg16bn\u6a21\u578b\u540e\u9762\u52a0\u8f7d\u8bd5\u8bd5\uff1a std::vector<int> cfg_16bn = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; auto vgg16bn = VGG(cfg_16bn,1000,true); torch::load(vgg16bn,\"your path to vgg16bn.pt\"); \u6b63\u5e38\u8fd0\u884c\u8fc7\u4e86\u4e00\u822c\u5c31\u4ee3\u8868\u6a21\u578b\u5df2\u7ecf\u6210\u529f\u52a0\u8f7d\u4e86\u3002 4.\u6570\u636e\u52a0\u8f7d \u548c\u7b2c\u56db\u7ae0\u4e00\u6837\uff0c\u672c\u7ae0\u8fd8\u662f\u4f7f\u7528pytorch\u5b98\u7f51\u63d0\u4f9b\u7684\u6606\u866b\u5206\u7c7b\u6570\u636e\u96c6\u3002\u4e0b\u8f7d\u89e3\u538b\u540e\u6709train\u548cval\u6587\u4ef6\u5939\uff0c\u91cc\u9762\u5206\u522b\u6709\u4e24\u7c7b\u6606\u866b\u56fe\u7247\u3002\u6570\u636e\u52a0\u8f7d\u6a21\u5757\u4ee3\u7801\u548c\u4e0a\u4e00\u7ae0\u4e00\u81f4\uff0c\u5c31\u4e0d\u91cd\u590d\u4e86\u3002 5.\u5c01\u88c5 1.\u58f0\u660e \u89e3\u51b3\u4e86\u57fa\u672c\u7684\u6a21\u578b\u5b9a\u4e49\u548c\u52a0\u8f7d\uff0c\u6570\u636e\u52a0\u8f7d\u7b49\u95ee\u9898\uff0c\u4e0b\u9762\u5c31\u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2aClassifier\u7c7b\u4e86\u3002\u8fd9\u4e2a\u7c7b\u7684\u529f\u80fd\u4e3b\u8981\u6709\uff1a \u521d\u59cb\u5316\uff1a\u5728\u521d\u59cb\u5316\u4e2d\u5b8c\u6210\u6a21\u578b\u6302\u8f7d\uff0c\u662fcpu\u8fd8\u662f\u67d0\u4e2agpu\uff1b\u5b9a\u4e49\u597d\u5206\u7c7b\u5668\u5e76\u52a0\u8f7d\u9884\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u5b9e\u73b0\u66f4\u597d\u66f4\u5feb\u8bad\u7ec3\u3002 \u8bad\u7ec3\uff1a\u53ef\u4ee5\u6307\u5b9a\u5206\u7c7b\u5668\u8bad\u7ec3\u7684\u5468\u671f\u6570\uff0c\u8bad\u7ec3\u7684batch_size\uff0c\u5b66\u4e60\u7387\u4ee5\u53ca\u6a21\u578b\u4fdd\u5b58\u7684\u8def\u5f84\u3002 \u9884\u6d4b\uff1a\u4f20\u5165\u56fe\u7247\u5c31\u53ef\u4ee5\u8fd4\u56de\u5206\u7c7b\u5668\u9884\u6d4b\u7684\u7c7b\u522b\u3002 \u52a0\u8f7d\u6743\u91cd\u3002 \u7c7b\u7684\u58f0\u660e\u5f88\u7b80\u5355\uff1a class Classifier { private: torch::Device device = torch::Device(torch::kCPU); //kCUDA: device = torch::Device(torch::kCUDA,0) VGG vgg = VGG{nullptr}; public: Classifier(int gpu_id = 0); void Initialize(int num_classes, std::string pretrained_path); void Train(int epochs, int batch_size, float learning_rate, std::string train_val_dir, std::string image_type, std::string save_path); int Predict(cv::Mat &image); void LoadWeight(std::string weight); }; 2.\u5b9a\u4e49 \u7c7b\u7684\u6210\u5458\u51fd\u6570\u5b9a\u4e49\u8f83\u4e3a\u590d\u6742\uff1a void Classifier::LoadWeight(std::string weight){ torch::load(vgg,weight); vgg->eval(); return; } LoadWeight\u6ca1\u592a\u591a\u8981\u8bb2\u7684\uff0c\u5f88\u7b80\u5355\u7684\u52a0\u8f7d\u6a21\u578b\u5e76\u7f6e\u4e3aeval()\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\u521d\u59cb\u5316\u548c\u8bad\u7ec3\u51fd\u6570\uff0c\u521d\u59cb\u5316\u51fd\u6570\u7531\u4e8e\u6a21\u578b\u6700\u540e\u4e00\u5c42\u7684num_class\u4e0d\u5b9a\uff0c\u6240\u4ee5\u4e0d\u80fd\u76f4\u63a5\u52a0\u8f7d\u4e4b\u524d\u4fdd\u5b58\u7684\u6743\u91cd\u3002\u800c\u8bad\u7ec3\u51fd\u6570\u8981\u5206\u522b\u7528train\u548cval\uff0c\u5e76\u4e14\u8981\u6ce8\u610f\u635f\u5931\u8bbe\u7f6e\u7b49\u3002 3.\u521d\u59cb\u5316(\u6784\u9020\u51fd\u6570) \u9996\u5148\u662f\u521d\u59cb\u5316\u51fd\u6570\uff0c\u521d\u59cb\u5316\u51fd\u6570\u9996\u5148\u5148\u5b9a\u4e49\u4e00\u4e2anum_class\u5bf9\u5e94\u7684\u5206\u7c7b\u5668vgg16bn\uff0c\u7136\u540e\u5b9a\u4e49\u4e00\u4e2anum_class=1000\u7684vgg16bn\u3002\u52a0\u8f7d\u65f6\u52a0\u8f7d\u540e\u8005\uff0c\u7136\u540e\u5c06\u6743\u91cd\u62f7\u8d1d\u81f3\u524d\u8005\u4e2d\u3002\u62f7\u8d1d\u8fc7\u7a0b\u975e\u5e38\u7cbe\u534e\uff0c\u9700\u8981\u8bfb\u8005\u7ec6\u7ec6\u63e3\u6469\u3002\u9664\u4e86\u62f7\u8d1d\u53c2\u6570\uff0c\u521d\u59cb\u5316\u8fd8\u4f1a\u5b9a\u4e49\u597d\u52a0\u8f7d\u5230gpu_id\u5bf9\u5e94\u7684GPU\u4e0a\uff0c\u6216\u8005\u8bbe\u7f6egpu_id\u5c0f\u4e8e0\u52a0\u8f7d\u5230cpu\u4e0a\u3002 Classifier::Classifier(int gpu_id) { if (gpu_id >= 0) { device = torch::Device(torch::kCUDA, gpu_id); } else { device = torch::Device(torch::kCPU); } } void Classifier::Initialize(int _num_classes, std::string _pretrained_path){ std::vector<int> cfg_d = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; auto net_pretrained = VGG(cfg_d,1000,true); vgg = VGG(cfg_d,_num_classes,true); torch::load(net_pretrained, _pretrained_path); torch::OrderedDict<std::string, at::Tensor> pretrained_dict = net_pretrained->named_parameters(); torch::OrderedDict<std::string, at::Tensor> model_dict = vgg->named_parameters(); for (auto n = pretrained_dict.begin(); n != pretrained_dict.end(); n++) { if (strstr((*n).key().data(), \"classifier\")) { continue; } model_dict[(*n).key()] = (*n).value(); } torch::autograd::GradMode::set_enabled(false); // \u4f7f\u53c2\u6570\u53ef\u4ee5\u62f7\u8d1d auto new_params = model_dict; auto params = vgg->named_parameters(true ); auto buffers = vgg->named_buffers(true); for (auto& val : new_params) { auto name = val.key(); auto* t = params.find(name); if (t != nullptr) { t->copy_(val.value()); } else { t = buffers.find(name); if (t != nullptr) { t->copy_(val.value()); } } } torch::autograd::GradMode::set_enabled(true); try { vgg->to(device); } catch (const std::exception&e) { std::cout << e.what() << std::endl; } return; } 4.\u8bad\u7ec3 \u7136\u540e\u662f\u8bad\u7ec3\u51fd\u6570\uff0c\u8bad\u7ec3\u51fd\u6570\u5206\u522b\u4f7f\u7528train_loader\u548cval_loader\uff0c\u524d\u8005\u52a0\u8f7dtrain\u6587\u4ef6\u5939\u4e0b\u7684\u56fe\u7247\u8bad\u7ec3\uff0c\u540e\u8005\u7528\u4e8e\u8bc4\u4f30\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5b9a\u4e49\u597d\u4f18\u5316\u5668\uff0c\u635f\u5931\u51fd\u6570\u7b49\u3002 void Classifier::Train(int num_epochs, int batch_size, float learning_rate, std::string train_val_dir, std::string image_type, std::string save_path){ std::string path_train = train_val_dir+ \"\\\\train\"; std::string path_val = train_val_dir + \"\\\\val\"; auto custom_dataset_train = dataSetClc(path_train, image_type).map(torch::data::transforms::Stack<>()); auto custom_dataset_val = dataSetClc(path_val, image_type).map(torch::data::transforms::Stack<>()); auto data_loader_train = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(std::move(custom_dataset_train), batch_size); auto data_loader_val = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(std::move(custom_dataset_val), batch_size); float loss_train = 0; float loss_val = 0; float acc_train = 0.0; float acc_val = 0.0; float best_acc = 0.0; for (size_t epoch = 1; epoch <= num_epochs; ++epoch) { size_t batch_index_train = 0; size_t batch_index_val = 0; if (epoch == int(num_epochs / 2)) { learning_rate /= 10; } torch::optim::Adam optimizer(vgg->parameters(), learning_rate); // \u5b66\u4e60\u7387 if (epoch < int(num_epochs / 8)) { for (auto mm : vgg->named_parameters()) { if (strstr(mm.key().data(), \"classifier\")) { mm.value().set_requires_grad(true); } else { mm.value().set_requires_grad(false); } } } else { for (auto mm : vgg->named_parameters()) { mm.value().set_requires_grad(true); } } // \u904d\u5386data_loader\uff0c\u4ea7\u751f\u6279\u6b21 for (auto& batch : *data_loader_train) { auto data = batch.data; auto target = batch.target.squeeze(); data = data.to(torch::kF32).to(device).div(255.0); target = target.to(torch::kInt64).to(device); optimizer.zero_grad(); // Execute the model torch::Tensor prediction = vgg->forward(data); auto acc = prediction.argmax(1).eq(target).sum(); acc_train += acc.template item<float>() / batch_size; // \u8ba1\u7b97\u635f\u5931\u5927\u5c0f torch::Tensor loss = torch::nll_loss(prediction, target); // \u8ba1\u7b97\u68af\u5ea6 loss.backward(); // \u66f4\u65b0\u6743\u91cd optimizer.step(); loss_train += loss.item<float>(); batch_index_train++; std::cout << \"Epoch: \" << epoch << \" |Train Loss: \" << loss_train / batch_index_train << \" |Train Acc:\" << acc_train / batch_index_train << \"\\r\"; } std::cout << std::endl; //\u9a8c\u8bc1 vgg->eval(); for (auto& batch : *data_loader_val) { auto data = batch.data; auto target = batch.target.squeeze(); data = data.to(torch::kF32).to(device).div(255.0); target = target.to(torch::kInt64).to(device); torch::Tensor prediction = vgg->forward(data); // \u8ba1\u7b97\u635f\u5931,NLL\u548cLog_softmax\u914d\u5408\u5f62\u6210\u4ea4\u53c9\u71b5\u635f\u5931 torch::Tensor loss = torch::nll_loss(prediction, target); auto acc = prediction.argmax(1).eq(target).sum(); acc_val += acc.template item<float>() / batch_size; loss_val += loss.item<float>(); batch_index_val++; std::cout << \"Epoch: \" << epoch << \" |Val Loss: \" << loss_val / batch_index_val << \" |Valid Acc:\" << acc_val / batch_index_val << \"\\r\"; } std::cout << std::endl; if (acc_val > best_acc) { torch::save(vgg, save_path); best_acc = acc_val; } loss_train = 0; loss_val = 0; acc_train = 0; acc_val = 0; batch_index_train = 0; batch_index_val = 0; } } 5.\u9884\u6d4b \u6700\u540e\u662f\u9884\u6d4b\uff0c\u8fd4\u56de\u7c7b\u522bans\uff0c\u4e2d\u95f4\u8ba1\u7b97\u7f6e\u4fe1\u5ea6prob\u3002 int Classifier::Predict(cv::Mat& image){ cv::resize(image, image, cv::Size(448, 448)); torch::Tensor img_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }); img_tensor = img_tensor.to(device).unsqueeze(0).to(torch::kF32).div(255.0); auto prediction = vgg->forward(img_tensor); prediction = torch::softmax(prediction,1); auto class_id = prediction.argmax(1); int ans = int(class_id.item().toInt()); float prob = prediction[0][ans].item().toFloat(); return ans; } \u672b\u5c3e\u8d34\u4e00\u4e2a\u8bad\u7ec3\u65f6\u7684\u56fe\uff1a \u8bad\u7ec3\u65f6\u7684\u53c2\u6570\u8bbe\u7f6e\u5982\u4e0b\uff1a std::string vgg_path = \"your path to vgg16_bn.pt\"; std::string train_val_dir = \"your path to hymenoptera_data\"; Classifier classifier(0); classifier.Initialize(2,vgg_path); classifier.Train(300,4,0.0003,train_val_dir,\".jpg\",\"classifer.pt\"); \u5176\u5b9e\uff0c\u5468\u671f\u6570\u8bbe\u7f6e300\u65f6\uff0c\u524d\u9762\u5f88\u591a\u4e2a\u5468\u671f\u90fd\u5728\u505a\u56fa\u5b9aCNN\u7684\u8fc1\u79fb\u5b66\u4e60(or finetune)\u3002\u53ef\u4ee5\u8bbe\u7f6e\u5c0f\u4e00\u4e9b\u67e5\u770b\u76f4\u63a5\u8bad\u7ec3\u5168\u90e8\u6a21\u578b\u4f1a\u600e\u6837\uff0c\u4ee5\u53ca\u601d\u8003\u4e3a\u4f55\u4f1a\u8fd9\u6837\u3002 \u81f3\u6b64\uff0clibtorch\u521d\u7ea7\u6559\u7a0b\u5df2\u7ecf\u5b8c\u6210\uff0c\u5751\u5f88\u591a\uff0c\u4f5c\u8005\u5df2\u7ecf\u4e3a\u4f60\u8e29\u597d\uff0c\u66f4\u9ad8\u7ea7\u7684\u90e8\u5206\u5728\u51c6\u5907\u4e2d\u3002 \u611f\u8c22\u5927\u4f6c\u5f00\u6e90\uff1a https://github.com/AllentDan/LibtorchTutorials/tree/main/lesson5-TrainingVGG","title":"\u7b2c\u4e94\u7ae0 \u5206\u7c7b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b"},{"location":"chapter5/#_1","text":"\u524d\u9762\u7684\u7ae0\u8282\u4e2d\u6211\u4eec\u4ecb\u7ecd\u4e86libtorch\u7684\u73af\u5883\u642d\u5efa\uff0clibtorch\u5f20\u91cf\u5e38\u7528\u64cd\u4f5c\uff0c\u7b80\u5355\u7684MLP\uff0cCNN\u548cLSTM\u6a21\u578b\u642d\u5efa\uff0c\u4ee5\u53ca\u6570\u636e\u52a0\u8f7d\u7c7b\u7684\u4f7f\u7528\u3002\u672c\u7ae0\u5c06\u4ee5\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e3a\u4f8b\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528c++\u8bad\u7ec3\u4e00\u4e2a\u56fe\u7247\u5206\u7c7b\u5668\u3002","title":"\u7b2c\u4e94\u7ae0 \u5206\u7c7b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b"},{"location":"chapter5/#1","text":"\u672c\u6587\u4ee5VGG\u4e3a\u4f8b\uff0c\u5bf9\u6bd4pytorch\u4e0b\u7684\u6a21\u578b\u642d\u5efa\u548c\u8bad\u7ec3\uff0c\u9610\u8ff0Libtorch\u7684\u6a21\u578b\u642d\u5efa\uff0c\u6a21\u578b\u52a0\u8f7d\u9884\u8bad\u7ec3\uff08from ImageNet\uff09\u6743\u91cd\u3002VGG\u6a21\u578b\u662f2014\u5e74\u7684ImageNet\u5206\u7c7b\u51a0\u519b\uff0c\u7531\u4e8e\u540e\u7eed\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u6dfb\u52a0\u4e86\u4e00\u4e9b\u6210\u5206\uff0c\u5982BatchNorm\uff0c\u5f62\u6210\u4e00\u4e9b\u65b0\u53d8\u79cd\u3002\u672c\u6587\u4ee5vgg16bn\u4e3a\u4f8b\u4f5c\u4ecb\u7ecd\uff0cvgg16bn\u5c31\u662fvgg16\u52a0\u4e0a\u4e86\u540e\u7eed\u63d0\u51fa\u7684BatchNorm\u5c42\u3002","title":"1.\u6a21\u578b"},{"location":"chapter5/#1_1","text":"\u9996\u5148\u4ecb\u7ecdpytorch\u7684\u6a21\u578b\u6e90\u7801\uff0cpytorch\u7684torchvision.models.VGG\u4e2d\u6709\u63d0\u4f9b\u5b98\u65b9\u7684VGG\u6a21\u578b\u4ee3\u7801\u3002\u76f4\u63a5\u590d\u5236\u4e0a\u6765\u5206\u6790\uff1a class VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=True): super(VGG, self).__init__() self.features = features self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0) def make_layers(cfg, batch_norm=False): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v return nn.Sequential(*layers) cfgs = { 'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'], } def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs): if pretrained: kwargs['init_weights'] = False model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return model \u548c\u73b0\u5728\u7684\u590d\u6742\u6a21\u578b\u76f8\u6bd4\uff0cVGG\u6a21\u578b\u7ed3\u6784\u8f83\u4e3a\u7b80\u5355\uff0c\u5c31\u662f\u7b80\u5355\u7684\u591a\u6b21\u5377\u79ef+\u4e0b\u91c7\u6837\u5806\u53e0\uff0c\u540e\u63a5\u4e00\u4e2a\u4e09\u5c42\u7684MLP\u3002\u4ee3\u7801\u4e2dVGG\u6a21\u578b\u7c7b\u6709\u4e09\u4e2a\u6210\u5458\u51fd\u6570\uff0c\u4e00\u4e2a\u521d\u59cb\u5316\u51fd\u6570init\uff0c\u4e00\u4e2a\u524d\u5411\u4f20\u64ad\u51fd\u6570forward\uff0c\u6700\u540e\u4e00\u4e2a\u6743\u91cd\u521d\u59cb\u5316\u51fd\u6570\u3002\u7c7b\u5916\u90e8\u6709\u4e00\u4e2a\u51fd\u6570make_layers\u51fd\u6570\u7528\u4e8e\u751f\u6210CNN\u4e3b\u5e72\uff0c\u8fd4\u56de\u4e00\u4e2ann.Sequential\u5bf9\u8c61\u3002 \u6253\u5f00python\u7f16\u8f91\u5668\uff08or IDE\uff0c\u9ed8\u8ba4\u6709pytorch\u7f16\u7a0b\u7ecf\u9a8c\uff09\u3002\u8f93\u5165\u4e0b\u9762\u4ee3\u7801\uff1a from torchvision.models import vgg16,vgg16_bn model = vgg16_bn(pretrained=True) for k,v in model.named_parameters(): print(k) \u53d1\u73b0\u6253\u5370\u51fa\u6a21\u578b\u6bcf\u4e00\u5c42\uff08\u6709\u6743\u91cd\u7684\u5c42\uff0c\u4e0d\u5305\u62ec\u7c7b\u4f3c\u6fc0\u6d3b\u51fd\u6570\u5c42\uff09\u7684\u540d\u79f0\u3002 features.0.weight features.0.bias features.1.weight features.1.bias features.3.weight features.3.bias features.4.weight features.4.bias features.7.weight features.7.bias features.8.weight features.8.bias features.10.weight features.10.bias features.11.weight features.11.bias features.14.weight features.14.bias features.15.weight features.15.bias features.17.weight features.17.bias features.18.weight features.18.bias features.20.weight features.20.bias features.21.weight features.21.bias features.24.weight features.24.bias features.25.weight features.25.bias features.27.weight features.27.bias features.28.weight features.28.bias features.30.weight features.30.bias features.31.weight features.31.bias features.34.weight features.34.bias features.35.weight features.35.bias features.37.weight features.37.bias features.38.weight features.38.bias features.40.weight features.40.bias features.41.weight features.41.bias classifier.0.weight classifier.0.bias classifier.3.weight classifier.3.bias classifier.6.weight classifier.6.bias \u8fd8\u884c\uff0c\u4e0d\u662f\u5f88\u957f\u3002\u8fd9\u6b65\u64cd\u4f5c\u5bf9\u540e\u7eed\u6a21\u578b\u642d\u5efa\u548c\u52a0\u8f7d\u6743\u91cd\u5f88\u91cd\u8981\uff0c\u56e0\u4e3atorch\u7684\u6a21\u578b\u52a0\u8f7d\u5fc5\u987b\u8981\u6709\u4e00\u4e00\u5bf9\u5e94\u7684\u6743\u91cd\u5c42\u7684\u540d\u79f0\u3002\u5982\u679c\u4ee3\u7801\u4e2d\u7684\u6a21\u578b\u548c\u52a0\u8f7d\u8def\u5f84\u5bf9\u5e94\u7684\u6743\u91cd\u63d0\u4f9b\u7684\u6743\u91cd\u5c42\u540d\u79f0\u4e0d\u4e00\u81f4\uff0c\u5c31\u4f1a\u4ea7\u751f\u9519\u8bef\u3002 \u5206\u6790\u6a21\u578b\u6253\u5370\u7684\u540d\u79f0\uff0c\u5176\u5b9e\u5c31\u4f1a\u53d1\u73b0\u53ea\u6709features\uff0cclassifier\uff0cweight\u548cbias\u548c\u6570\u5b57\u3002\u8054\u7cfb\u524d\u9762\u7684\u5b98\u65b9\u4ee3\u7801\u7684\u521d\u59cb\u5316\u51fd\u6570init\uff0c\u51fd\u6570\u5185\u90e8\u6709self.classifer\u548cself.features\uff0c\u5c31\u5f88\u5bb9\u6613\u5f97\u51fapytorch\u6a21\u578b\u7684\u5185\u90e8\u5c42\u540d\u79f0\u547d\u540d\u89c4\u5f8b\u4e86\u3002weight\u548cbias\u5bf9\u5e94conv\u5c42\u91cc\u7684self.conv\u548cself.bias\u3002\u70b9\u548c\u6570\u5b57\u8868\u793ann.Sequential\u91cc\u7684\u5e8f\u53f7\u3002","title":"1.\u5206\u6790\u6a21\u578b"},{"location":"chapter5/#2","text":"\u4e0b\u9762\u5728c++\u4e2d\u642d\u5efa\u4e00\u4e2a\u548cpytorch\u4e0b\u5b8c\u5168\u4e00\u81f4\u7684vgg16bn\u3002\u5982\u679c\u4e0d\u4e00\u81f4\u7684\u8bdd\u5176\u5b9e\u4e0d\u5f71\u54cd\u6b63\u5e38\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u9884\u6d4b\uff0c\u4f46\u662f\u5f71\u54cd\u521d\u59cb\u5316\u72b6\u6001\uff0c\u6a21\u578b\u52a0\u8f7d\u4eceImageNet\u6570\u636e\u96c6\u8bad\u7ec3\u597d\u7684\u6743\u91cd\u4ee5\u540e\uff0c\u8bad\u7ec3\u6536\u655b\u7684\u901f\u5ea6\u548c\u6536\u655b\u540e\u7684\u7cbe\u5ea6\u90fd\u4f1a\u597d\u5f88\u591a\u3002 \u9996\u5148\u662f.h\u6587\u4ef6\u4e2d\u8981\u505a\u7684\uff0c\u4e00\u4e2aconv_options\u786e\u5b9a\u5377\u79ef\u8d85\u53c2\u6570\uff0c\u56e0\u4e3a\u5e38\u7528\u6240\u4ee5inline\u4e00\u4e0b\u3002maxpool_options\u51fd\u6570\u786e\u5b9aMaxPool2d\u7684\u8d85\u53c2\u6570\u3002\u5982\u4f55\u5b9a\u4e49\u4e00\u4e2a\u548cpytorch\u4e00\u81f4\u7684make_features\u51fd\u6570\uff0c\u518d\u5728VGG\u7c7b\u4e2d\u58f0\u660e\u548cpytorch\u4e00\u81f4\u7684\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad\u51fd\u6570\u3002\u6700\u540e\u5219\u662f\u4e00\u4e2avgg16bn\u51fd\u6570\uff0c\u8fd4\u56devgg16bn\u6a21\u578b\u3002 //\u548c\u524d\u9762\u7ae0\u8282\u4e00\u81f4\uff0c\u5b9a\u4e49\u4e00\u4e2a\u786e\u5b9aconv\u8d85\u53c2\u6570\u7684\u51fd\u6570 inline torch::nn::Conv2dOptions conv_options(int64_t in_planes, int64_t out_planes, int64_t kerner_size, int64_t stride = 1, int64_t padding = 0, bool with_bias = false) { torch::nn::Conv2dOptions conv_options = torch::nn::Conv2dOptions(in_planes, out_planes, kerner_size); conv_options.stride(stride); conv_options.padding(padding); conv_options.bias(with_bias); return conv_options; } //\u4eff\u7167\u4e0a\u9762\u7684conv_options\uff0c\u5b9a\u4e49\u4e00\u4e2a\u786e\u5b9aMaxPool2d\u7684\u8d85\u53c2\u6570\u7684\u51fd\u6570 inline torch::nn::MaxPool2dOptions maxpool_options(int kernel_size, int stride){ torch::nn::MaxPool2dOptions maxpool_options(kernel_size); maxpool_options.stride(stride); return maxpool_options; } //\u5bf9\u5e94pytorch\u4e2d\u7684make_features\u51fd\u6570\uff0c\u8fd4\u56deCNN\u4e3b\u4f53\uff0c\u8be5\u4e3b\u4f53\u662f\u4e00\u4e2atorch::nn::Sequential\u5bf9\u8c61 torch::nn::Sequential make_features(std::vector<int> &cfg, bool batch_norm); //VGG\u7c7b\u7684\u58f0\u660e\uff0c\u5305\u62ec\u521d\u59cb\u5316\u548c\u524d\u5411\u4f20\u64ad class VGGImpl: public torch::nn::Module { private: torch::nn::Sequential features_{nullptr}; torch::nn::AdaptiveAvgPool2d avgpool{nullptr}; torch::nn::Sequential classifier; public: VGGImpl(std::vector<int> &cfg, int num_classes = 1000, bool batch_norm = false); torch::Tensor forward(torch::Tensor x); }; TORCH_MODULE(VGG); //vgg16bn\u51fd\u6570\u7684\u58f0\u660e VGG vgg16bn(int num_classes); \u7136\u540e\u5728.cpp\u6587\u4ef6\u4e2d\u5b9a\u4e49\u597d.h\u6587\u4ef6\u4e2d\u7684\u58f0\u660e\u3002.cpp\u6587\u4ef6\u7684\u5185\u5bb9\u5982\u4e0b\uff1a // make_features \u65b9\u6cd5 torch::nn::Sequential make_features(std::vector<int> &cfg, bool batch_norm){ torch::nn::Sequential features; // \u58f0\u660e nn.sequential \u5bf9\u8c61 int in_channels = 3; //\u58f0\u660e int\u5bf9\u8c61 for(auto v : cfg){ //vector\u7684\u904d\u5386(C++\u6807\u51c6\u6a21\u677f\u5e93) if(v==-1){ features->push_back(torch::nn::MaxPool2d(maxpool_options(2,2))); } else{ auto conv2d = torch::nn::Conv2d(conv_options(in_channels,v,3,1,1)); features->push_back(conv2d); if(batch_norm){ features->push_back(torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(v))); } features->push_back(torch::nn::ReLU(torch::nn::ReLUOptions(true))); in_channels = v; } } return features; } VGGImpl::VGGImpl(std::vector<int> &cfg, int num_classes, bool batch_norm){ features_ = make_features(cfg,batch_norm); avgpool = torch::nn::AdaptiveAvgPool2d(torch::nn::AdaptiveAvgPool2dOptions(7)); classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(512 * 7 * 7, 4096))); classifier->push_back(torch::nn::ReLU(torch::nn::ReLUOptions(true))); classifier->push_back(torch::nn::Dropout()); classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(4096, 4096))); classifier->push_back(torch::nn::ReLU(torch::nn::ReLUOptions(true))); classifier->push_back(torch::nn::Dropout()); classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(4096, num_classes))); features_ = register_module(\"features\",features_); classifier = register_module(\"classifier\",classifier); } torch::Tensor VGGImpl::forward(torch::Tensor x){ x = features_->forward(x); x = avgpool(x); x = torch::flatten(x,1); x = classifier->forward(x); return torch::log_softmax(x, 1); } VGG vgg16bn(int num_classes){ std::vector<int> cfg_dd = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; VGG vgg = VGG(cfg_dd,num_classes,true); return vgg; } \u7531\u4e8ec++\u4e2d\u5143\u7ec4\u5982\u679c\u592a\u957f\u7684\u8bdd\u58f0\u660e\u4e5f\u4f1a\u5f88\u957f\uff0c\u800c\u5217\u8868\u6216\u8005vector\u53ea\u63a5\u53d7\u540c\u7c7b\u578b\u7684\u6570\u636e\uff0c\u5c31\u5c06\u539f\u6765pytorch\u4e2d\u7684cfg\u91cc\u7684\u2019M\u2019\u6539\u6210-1\u3002\u5728\u8bfb\u53d6cfg\u65f6\u5224\u65ad\u7531\u539f\u6765\u7684\u2019M\u2019\u53d8\u6210\u5224\u65ad\u662f\u5426-1\u5373\u53ef\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7ed9\u6a21\u578b\u4e0d\u540c\u5c42\u547d\u540d\u65f6\uff0c\u4ee3\u7801\u91cc\u53ea\u51fa\u73b0\u4e86register_module\u5bf9features\u548cclassifier\u547d\u540d\uff0c\u8fd9\u548cpytorch\u4fdd\u6301\u4e00\u81f4\u3002","title":"2.\u642d\u5efa\u6a21\u578b"},{"location":"chapter5/#3","text":"\u4e0b\u9762\u67e5\u770b\u6211\u4eecc++\u5b9a\u4e49\u7684\u6a21\u578b\u662f\u5426\u548cpytorch\u5b8c\u5168\u4e00\u81f4\u3002\u5728\u4e3b\u51fd\u6570\u4e2d\u5b9e\u4f8b\u5316\u4e00\u4e2aVGG\u7684\u5bf9\u8c61\uff0c\u7136\u540e\u6253\u5370\u5404\u4e2a\u5c42\u7684\u540d\u79f0\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a std::vector<int> cfg_16bn = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; auto vgg16bn = VGG(cfg_16bn,1000,true); auto dict16bn = vgg16bn->named_parameters(); //vector for (auto n = dict16bn.begin(); n != dict16bn.end(); n++) { std::cout<<(*n).key()<<std::endl; } \u53ef\u4ee5\u53d1\u73b0\uff0c\u5404\u4e2a\u5c42\u540d\u79f0\u548cpytorch\u4e2d\u7684\u6a21\u578b\u5185\u90e8\u5c42\u7684\u540d\u79f0\u5b8c\u5168\u4e00\u81f4\u3002\u8fd9\u6837\u6211\u4eec\u5c06pytorch\u7684\u6a21\u578b\u6743\u91cd\u4fdd\u5b58\u4e0b\u6765\uff0c\u7136\u540e\u52a0\u8f7d\u5230c++\u4e2d\u3002 import torch from torchvision.models import vgg16,vgg16_bn model=model.to(torch.device(\"cpu\")) model.eval() var=torch.ones((1,3,224,224)) traced_script_module = torch.jit.trace(model, var) # torchscript model traced_script_module.save(\"vgg16bn.pt\") \u8fd9\u6837\uff0c\u6a21\u578b\u7684\u5377\u79ef\u5c42\uff0c\u5f52\u4e00\u5316\u5c42\uff0c\u7ebf\u6027\u5c42\u7684\u6743\u91cd\u5c31\u4fdd\u5b58\u5230.pt\u6587\u4ef6\u4e2d\u4e86\u3002\u4e0b\u9762\u5c1d\u8bd5\u52a0\u8f7d\u5230c++\u4e2d\u3002c++\u4e2d\u7684\u52a0\u8f7d\u4ee3\u7801\u8f83\u4e3a\u7b80\u5355\uff0c\u76f4\u63a5\u5728\u5b9a\u4e49\u597d\u7684vgg16bn\u6a21\u578b\u540e\u9762\u52a0\u8f7d\u8bd5\u8bd5\uff1a std::vector<int> cfg_16bn = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; auto vgg16bn = VGG(cfg_16bn,1000,true); torch::load(vgg16bn,\"your path to vgg16bn.pt\"); \u6b63\u5e38\u8fd0\u884c\u8fc7\u4e86\u4e00\u822c\u5c31\u4ee3\u8868\u6a21\u578b\u5df2\u7ecf\u6210\u529f\u52a0\u8f7d\u4e86\u3002","title":"3.\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6743\u91cd"},{"location":"chapter5/#4","text":"\u548c\u7b2c\u56db\u7ae0\u4e00\u6837\uff0c\u672c\u7ae0\u8fd8\u662f\u4f7f\u7528pytorch\u5b98\u7f51\u63d0\u4f9b\u7684\u6606\u866b\u5206\u7c7b\u6570\u636e\u96c6\u3002\u4e0b\u8f7d\u89e3\u538b\u540e\u6709train\u548cval\u6587\u4ef6\u5939\uff0c\u91cc\u9762\u5206\u522b\u6709\u4e24\u7c7b\u6606\u866b\u56fe\u7247\u3002\u6570\u636e\u52a0\u8f7d\u6a21\u5757\u4ee3\u7801\u548c\u4e0a\u4e00\u7ae0\u4e00\u81f4\uff0c\u5c31\u4e0d\u91cd\u590d\u4e86\u3002","title":"4.\u6570\u636e\u52a0\u8f7d"},{"location":"chapter5/#5","text":"1.\u58f0\u660e \u89e3\u51b3\u4e86\u57fa\u672c\u7684\u6a21\u578b\u5b9a\u4e49\u548c\u52a0\u8f7d\uff0c\u6570\u636e\u52a0\u8f7d\u7b49\u95ee\u9898\uff0c\u4e0b\u9762\u5c31\u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2aClassifier\u7c7b\u4e86\u3002\u8fd9\u4e2a\u7c7b\u7684\u529f\u80fd\u4e3b\u8981\u6709\uff1a \u521d\u59cb\u5316\uff1a\u5728\u521d\u59cb\u5316\u4e2d\u5b8c\u6210\u6a21\u578b\u6302\u8f7d\uff0c\u662fcpu\u8fd8\u662f\u67d0\u4e2agpu\uff1b\u5b9a\u4e49\u597d\u5206\u7c7b\u5668\u5e76\u52a0\u8f7d\u9884\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u5b9e\u73b0\u66f4\u597d\u66f4\u5feb\u8bad\u7ec3\u3002 \u8bad\u7ec3\uff1a\u53ef\u4ee5\u6307\u5b9a\u5206\u7c7b\u5668\u8bad\u7ec3\u7684\u5468\u671f\u6570\uff0c\u8bad\u7ec3\u7684batch_size\uff0c\u5b66\u4e60\u7387\u4ee5\u53ca\u6a21\u578b\u4fdd\u5b58\u7684\u8def\u5f84\u3002 \u9884\u6d4b\uff1a\u4f20\u5165\u56fe\u7247\u5c31\u53ef\u4ee5\u8fd4\u56de\u5206\u7c7b\u5668\u9884\u6d4b\u7684\u7c7b\u522b\u3002 \u52a0\u8f7d\u6743\u91cd\u3002 \u7c7b\u7684\u58f0\u660e\u5f88\u7b80\u5355\uff1a class Classifier { private: torch::Device device = torch::Device(torch::kCPU); //kCUDA: device = torch::Device(torch::kCUDA,0) VGG vgg = VGG{nullptr}; public: Classifier(int gpu_id = 0); void Initialize(int num_classes, std::string pretrained_path); void Train(int epochs, int batch_size, float learning_rate, std::string train_val_dir, std::string image_type, std::string save_path); int Predict(cv::Mat &image); void LoadWeight(std::string weight); }; 2.\u5b9a\u4e49 \u7c7b\u7684\u6210\u5458\u51fd\u6570\u5b9a\u4e49\u8f83\u4e3a\u590d\u6742\uff1a void Classifier::LoadWeight(std::string weight){ torch::load(vgg,weight); vgg->eval(); return; } LoadWeight\u6ca1\u592a\u591a\u8981\u8bb2\u7684\uff0c\u5f88\u7b80\u5355\u7684\u52a0\u8f7d\u6a21\u578b\u5e76\u7f6e\u4e3aeval()\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\u521d\u59cb\u5316\u548c\u8bad\u7ec3\u51fd\u6570\uff0c\u521d\u59cb\u5316\u51fd\u6570\u7531\u4e8e\u6a21\u578b\u6700\u540e\u4e00\u5c42\u7684num_class\u4e0d\u5b9a\uff0c\u6240\u4ee5\u4e0d\u80fd\u76f4\u63a5\u52a0\u8f7d\u4e4b\u524d\u4fdd\u5b58\u7684\u6743\u91cd\u3002\u800c\u8bad\u7ec3\u51fd\u6570\u8981\u5206\u522b\u7528train\u548cval\uff0c\u5e76\u4e14\u8981\u6ce8\u610f\u635f\u5931\u8bbe\u7f6e\u7b49\u3002 3.\u521d\u59cb\u5316(\u6784\u9020\u51fd\u6570) \u9996\u5148\u662f\u521d\u59cb\u5316\u51fd\u6570\uff0c\u521d\u59cb\u5316\u51fd\u6570\u9996\u5148\u5148\u5b9a\u4e49\u4e00\u4e2anum_class\u5bf9\u5e94\u7684\u5206\u7c7b\u5668vgg16bn\uff0c\u7136\u540e\u5b9a\u4e49\u4e00\u4e2anum_class=1000\u7684vgg16bn\u3002\u52a0\u8f7d\u65f6\u52a0\u8f7d\u540e\u8005\uff0c\u7136\u540e\u5c06\u6743\u91cd\u62f7\u8d1d\u81f3\u524d\u8005\u4e2d\u3002\u62f7\u8d1d\u8fc7\u7a0b\u975e\u5e38\u7cbe\u534e\uff0c\u9700\u8981\u8bfb\u8005\u7ec6\u7ec6\u63e3\u6469\u3002\u9664\u4e86\u62f7\u8d1d\u53c2\u6570\uff0c\u521d\u59cb\u5316\u8fd8\u4f1a\u5b9a\u4e49\u597d\u52a0\u8f7d\u5230gpu_id\u5bf9\u5e94\u7684GPU\u4e0a\uff0c\u6216\u8005\u8bbe\u7f6egpu_id\u5c0f\u4e8e0\u52a0\u8f7d\u5230cpu\u4e0a\u3002 Classifier::Classifier(int gpu_id) { if (gpu_id >= 0) { device = torch::Device(torch::kCUDA, gpu_id); } else { device = torch::Device(torch::kCPU); } } void Classifier::Initialize(int _num_classes, std::string _pretrained_path){ std::vector<int> cfg_d = {64, 64, -1, 128, 128, -1, 256, 256, 256, -1, 512, 512, 512, -1, 512, 512, 512, -1}; auto net_pretrained = VGG(cfg_d,1000,true); vgg = VGG(cfg_d,_num_classes,true); torch::load(net_pretrained, _pretrained_path); torch::OrderedDict<std::string, at::Tensor> pretrained_dict = net_pretrained->named_parameters(); torch::OrderedDict<std::string, at::Tensor> model_dict = vgg->named_parameters(); for (auto n = pretrained_dict.begin(); n != pretrained_dict.end(); n++) { if (strstr((*n).key().data(), \"classifier\")) { continue; } model_dict[(*n).key()] = (*n).value(); } torch::autograd::GradMode::set_enabled(false); // \u4f7f\u53c2\u6570\u53ef\u4ee5\u62f7\u8d1d auto new_params = model_dict; auto params = vgg->named_parameters(true ); auto buffers = vgg->named_buffers(true); for (auto& val : new_params) { auto name = val.key(); auto* t = params.find(name); if (t != nullptr) { t->copy_(val.value()); } else { t = buffers.find(name); if (t != nullptr) { t->copy_(val.value()); } } } torch::autograd::GradMode::set_enabled(true); try { vgg->to(device); } catch (const std::exception&e) { std::cout << e.what() << std::endl; } return; } 4.\u8bad\u7ec3 \u7136\u540e\u662f\u8bad\u7ec3\u51fd\u6570\uff0c\u8bad\u7ec3\u51fd\u6570\u5206\u522b\u4f7f\u7528train_loader\u548cval_loader\uff0c\u524d\u8005\u52a0\u8f7dtrain\u6587\u4ef6\u5939\u4e0b\u7684\u56fe\u7247\u8bad\u7ec3\uff0c\u540e\u8005\u7528\u4e8e\u8bc4\u4f30\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5b9a\u4e49\u597d\u4f18\u5316\u5668\uff0c\u635f\u5931\u51fd\u6570\u7b49\u3002 void Classifier::Train(int num_epochs, int batch_size, float learning_rate, std::string train_val_dir, std::string image_type, std::string save_path){ std::string path_train = train_val_dir+ \"\\\\train\"; std::string path_val = train_val_dir + \"\\\\val\"; auto custom_dataset_train = dataSetClc(path_train, image_type).map(torch::data::transforms::Stack<>()); auto custom_dataset_val = dataSetClc(path_val, image_type).map(torch::data::transforms::Stack<>()); auto data_loader_train = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(std::move(custom_dataset_train), batch_size); auto data_loader_val = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(std::move(custom_dataset_val), batch_size); float loss_train = 0; float loss_val = 0; float acc_train = 0.0; float acc_val = 0.0; float best_acc = 0.0; for (size_t epoch = 1; epoch <= num_epochs; ++epoch) { size_t batch_index_train = 0; size_t batch_index_val = 0; if (epoch == int(num_epochs / 2)) { learning_rate /= 10; } torch::optim::Adam optimizer(vgg->parameters(), learning_rate); // \u5b66\u4e60\u7387 if (epoch < int(num_epochs / 8)) { for (auto mm : vgg->named_parameters()) { if (strstr(mm.key().data(), \"classifier\")) { mm.value().set_requires_grad(true); } else { mm.value().set_requires_grad(false); } } } else { for (auto mm : vgg->named_parameters()) { mm.value().set_requires_grad(true); } } // \u904d\u5386data_loader\uff0c\u4ea7\u751f\u6279\u6b21 for (auto& batch : *data_loader_train) { auto data = batch.data; auto target = batch.target.squeeze(); data = data.to(torch::kF32).to(device).div(255.0); target = target.to(torch::kInt64).to(device); optimizer.zero_grad(); // Execute the model torch::Tensor prediction = vgg->forward(data); auto acc = prediction.argmax(1).eq(target).sum(); acc_train += acc.template item<float>() / batch_size; // \u8ba1\u7b97\u635f\u5931\u5927\u5c0f torch::Tensor loss = torch::nll_loss(prediction, target); // \u8ba1\u7b97\u68af\u5ea6 loss.backward(); // \u66f4\u65b0\u6743\u91cd optimizer.step(); loss_train += loss.item<float>(); batch_index_train++; std::cout << \"Epoch: \" << epoch << \" |Train Loss: \" << loss_train / batch_index_train << \" |Train Acc:\" << acc_train / batch_index_train << \"\\r\"; } std::cout << std::endl; //\u9a8c\u8bc1 vgg->eval(); for (auto& batch : *data_loader_val) { auto data = batch.data; auto target = batch.target.squeeze(); data = data.to(torch::kF32).to(device).div(255.0); target = target.to(torch::kInt64).to(device); torch::Tensor prediction = vgg->forward(data); // \u8ba1\u7b97\u635f\u5931,NLL\u548cLog_softmax\u914d\u5408\u5f62\u6210\u4ea4\u53c9\u71b5\u635f\u5931 torch::Tensor loss = torch::nll_loss(prediction, target); auto acc = prediction.argmax(1).eq(target).sum(); acc_val += acc.template item<float>() / batch_size; loss_val += loss.item<float>(); batch_index_val++; std::cout << \"Epoch: \" << epoch << \" |Val Loss: \" << loss_val / batch_index_val << \" |Valid Acc:\" << acc_val / batch_index_val << \"\\r\"; } std::cout << std::endl; if (acc_val > best_acc) { torch::save(vgg, save_path); best_acc = acc_val; } loss_train = 0; loss_val = 0; acc_train = 0; acc_val = 0; batch_index_train = 0; batch_index_val = 0; } } 5.\u9884\u6d4b \u6700\u540e\u662f\u9884\u6d4b\uff0c\u8fd4\u56de\u7c7b\u522bans\uff0c\u4e2d\u95f4\u8ba1\u7b97\u7f6e\u4fe1\u5ea6prob\u3002 int Classifier::Predict(cv::Mat& image){ cv::resize(image, image, cv::Size(448, 448)); torch::Tensor img_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }); img_tensor = img_tensor.to(device).unsqueeze(0).to(torch::kF32).div(255.0); auto prediction = vgg->forward(img_tensor); prediction = torch::softmax(prediction,1); auto class_id = prediction.argmax(1); int ans = int(class_id.item().toInt()); float prob = prediction[0][ans].item().toFloat(); return ans; } \u672b\u5c3e\u8d34\u4e00\u4e2a\u8bad\u7ec3\u65f6\u7684\u56fe\uff1a \u8bad\u7ec3\u65f6\u7684\u53c2\u6570\u8bbe\u7f6e\u5982\u4e0b\uff1a std::string vgg_path = \"your path to vgg16_bn.pt\"; std::string train_val_dir = \"your path to hymenoptera_data\"; Classifier classifier(0); classifier.Initialize(2,vgg_path); classifier.Train(300,4,0.0003,train_val_dir,\".jpg\",\"classifer.pt\"); \u5176\u5b9e\uff0c\u5468\u671f\u6570\u8bbe\u7f6e300\u65f6\uff0c\u524d\u9762\u5f88\u591a\u4e2a\u5468\u671f\u90fd\u5728\u505a\u56fa\u5b9aCNN\u7684\u8fc1\u79fb\u5b66\u4e60(or finetune)\u3002\u53ef\u4ee5\u8bbe\u7f6e\u5c0f\u4e00\u4e9b\u67e5\u770b\u76f4\u63a5\u8bad\u7ec3\u5168\u90e8\u6a21\u578b\u4f1a\u600e\u6837\uff0c\u4ee5\u53ca\u601d\u8003\u4e3a\u4f55\u4f1a\u8fd9\u6837\u3002 \u81f3\u6b64\uff0clibtorch\u521d\u7ea7\u6559\u7a0b\u5df2\u7ecf\u5b8c\u6210\uff0c\u5751\u5f88\u591a\uff0c\u4f5c\u8005\u5df2\u7ecf\u4e3a\u4f60\u8e29\u597d\uff0c\u66f4\u9ad8\u7ea7\u7684\u90e8\u5206\u5728\u51c6\u5907\u4e2d\u3002 \u611f\u8c22\u5927\u4f6c\u5f00\u6e90\uff1a https://github.com/AllentDan/LibtorchTutorials/tree/main/lesson5-TrainingVGG","title":"5.\u5c01\u88c5"},{"location":"chapter6/","text":"\u7b2c\u516d\u7ae0 \u5206\u5272\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u672c\u7ae0\u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u5982\u4f55\u7528C++\u5b9e\u73b0\u4e00\u4e2a\u8bed\u4e49\u5206\u5272\u5668\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u8bad\u7ec3\u548c\u9884\u6d4b\u7684\u529f\u80fd\u3002\u672c\u6587\u7684\u5206\u5272\u6a21\u578b\u67b6\u6784\u4f7f\u7528\u7b80\u5355\u7684U-Net\u7ed3\u6784\uff0c\u4ee3\u7801\u7ed3\u6784\u53c2\u8003\u4e86 qubvel segmentation \u4e2d\u7684U-Net\u90e8\u5206\uff0c\u8be5\u9879\u76ee\u7b80\u79f0SMP\uff0c\u662f\u57fa\u4e8epytorch\u5b9e\u73b0\u7684\u5f00\u6e90\u8bed\u4e49\u5206\u5272\u9879\u76ee\u3002\u672c\u6587\u5206\u4eab\u7684c++\u6a21\u578b\u51e0\u4e4e\u5b8c\u7f8e\u590d\u73b0\u4e86python\u7684\u7248\u672c\u3002 1.\u6a21\u578b\u7b80\u4ecb \u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0bU-Net\u6a21\u578b\u3002U-Net\u6a21\u578b\u7684\u63d0\u51fa\u662f\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u76f8\u6bd4\u4e8e\u5f53\u65f6\u7684\u5176\u4ed6\u6a21\u578b\u7ed3\u6784\uff0cU-Net\u7684\u5206\u5272\u80fd\u529b\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002\u4e00\u4e2a\u7ecf\u5178\u7684U-Net\u7ed3\u6784\u56fe\u5982\u4e0b\uff1a U-Net\u6a21\u578b\u91c7\u7528\u5178\u578b\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5de6\u8fb9\u7684\u7f16\u7801\u90e8\u5206\u7c7b\u4f3cVGG\u6a21\u578b\uff0c\u662f\u53cc\u5377\u79ef+\u4e0b\u91c7\u6837\u7684\u591a\u6b21\u5806\u53e0\u3002U-Net\u6a21\u578b\u53f3\u8fb9\u7684\u89e3\u7801\u90e8\u5206\u540c\u6837\u662f\u53cc\u5377\u79ef\uff0c\u4f46\u662f\u4e3a\u4e86\u5f97\u5230\u63a5\u8fd1\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u7684\u8f93\u51fa\u56fe\u50cf\uff0c\u9488\u5bf9\u7f16\u7801\u7684\u4e0b\u91c7\u6837\u5b9e\u65bd\u4e86\u5bf9\u5e94\u7684\u4e0a\u91c7\u6837\u3002\u6700\u91cd\u8981\u7684\u662f\uff0cU-Net\u4e4b\u6240\u4ee5\u6548\u679c\u7a81\u51fa\uff0c\u91cd\u8981\u539f\u56e0\u5728\u4e8e\u5176\u5728\u89e3\u7801\u90e8\u5206\u5229\u7528\u4e86\u7f16\u7801\u73af\u8282\u7684\u7279\u5f81\u56fe\uff0c\u62fc\u63a5\u7f16\u7801\u548c\u89e3\u7801\u7684\u7279\u5f81\u56fe\uff0c\u518d\u5bf9\u62fc\u63a5\u540e\u7279\u5f81\u56fe\u5377\u79ef\u4e0a\u91c7\u6837\uff0c\u91cd\u590d\u591a\u6b21\u5f97\u5230\u89e3\u7801\u8f93\u51fa\u3002 2.\u7f16\u7801\u5668\u2014ResNet \u672c\u6587\u4ecb\u7ecd\u7684\u7f16\u7801\u5668\u4f7f\u7528ResNet\u7f51\u7edc\uff0c\u540c\u65f6\u53ef\u4ee5\u50cf\u7b2c\u4e94\u7ae0\u4e00\u6837\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u5373\u9aa8\u5e72\u7f51\u7edc\u4e3aImageNet\u9884\u8bad\u7ec3\u7684ResNet\u3002\u8bdd\u4e0d\u591a\u8bf4\uff0c\u76f4\u63a5\u4e0ac++\u7684ResNet\u4ee3\u7801\u3002 1.Block\u642d\u5efa \u5efa\u8bae\u770b\u672c\u6587\u4ee3\u7801\u65f6\u6253\u5f00pytorch\u7684torchvision\u4e2d\u7684resnet.py\uff0c\u5bf9\u6bd4\u9605\u8bfb\u3002 \u9996\u5148\u662f\u57fa\u7840\u6a21\u5757\uff0cpytorch\u9488\u5bf9resnet18\uff0cresne34\u548cresnet50\uff0cresnet101\uff0cresnet152\u8fdb\u884c\u5206\u7c7b\uff0cresnet18\u4e0eresnet34\u5747\u4f7f\u7528BasicBlock\uff0c\u800c\u66f4\u6df1\u7684\u7f51\u7edc\u4f7f\u7528BottleNeck\u3002\u6211\u4e0d\u60f3\u4f7f\u7528\u6a21\u677f\u7c7b\u7f16\u7a0b\uff0c\u5c31\u76f4\u63a5\u5c06\u4e24\u4e2a\u6a21\u5757\u5408\u4e3a\u4e00\u4f53\u3002\u58f0\u660e\u5982\u4e0b\uff1a class BlockImpl : public torch::nn::Module { public: BlockImpl(int64_t inplanes, int64_t planes, int64_t stride_ = 1, torch::nn::Sequential downsample_ = nullptr, int groups = 1, int base_width = 64, bool is_basic = true); torch::Tensor forward(torch::Tensor x); torch::nn::Sequential downsample{ nullptr }; private: bool is_basic = true; int64_t stride = 1; torch::nn::Conv2d conv1{ nullptr }; torch::nn::BatchNorm2d bn1{ nullptr }; torch::nn::Conv2d conv2{ nullptr }; torch::nn::BatchNorm2d bn2{ nullptr }; torch::nn::Conv2d conv3{ nullptr }; torch::nn::BatchNorm2d bn3{ nullptr }; }; TORCH_MODULE(Block); \u53ef\u4ee5\u53d1\u73b0\uff0c\u5176\u5b9e\u662f\u76f4\u63a5\u58f0\u660e\u4e86\u4e09\u4e2aconv\u7ed3\u6784\u548c\u4e00\u4e2ais_basic\u6807\u5fd7\u4f4d\u5224\u65ad\u5b9a\u4e49\u65f6\u8fdb\u884cBasicBlock\u5b9a\u4e49\u8fd8\u662fBottleNeck\u5b9a\u4e49\u3002\u4e0b\u9762\u662f\u5176\u5b9a\u4e49: // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 BlockImpl::BlockImpl(int64_t inplanes, int64_t planes, int64_t stride_, torch::nn::Sequential downsample_, int groups, int base_width, bool _is_basic) { downsample = downsample_; stride = stride_; int width = int(planes * (base_width / 64.)) * groups; conv1 = torch::nn::Conv2d(conv_options(inplanes, width, 3, stride_, 1, groups, false)); bn1 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(width)); conv2 = torch::nn::Conv2d(conv_options(width, width, 3, 1, 1, groups, false)); bn2 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(width)); is_basic = _is_basic; if (!is_basic) { conv1 = torch::nn::Conv2d(conv_options(inplanes, width, 1, 1, 0, 1, false)); conv2 = torch::nn::Conv2d(conv_options(width, width, 3, stride_, 1, groups, false)); conv3 = torch::nn::Conv2d(conv_options(width, planes * 4, 1, 1, 0, 1, false)); bn3 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(planes * 4)); } register_module(\"conv1\", conv1); register_module(\"bn1\", bn1); register_module(\"conv2\", conv2); register_module(\"bn2\", bn2); if (!is_basic) { register_module(\"conv3\", conv3); register_module(\"bn3\", bn3); } if (!downsample->is_empty()) { register_module(\"downsample\", downsample); } } torch::Tensor BlockImpl::forward(torch::Tensor x) { torch::Tensor residual = x.clone(); x = conv1->forward(x); x = bn1->forward(x); x = torch::relu(x); x = conv2->forward(x); x = bn2->forward(x); if (!is_basic) { x = torch::relu(x); x = conv3->forward(x); x = bn3->forward(x); } if (!downsample->is_empty()) { residual = downsample->forward(residual); } x += residual; x = torch::relu(x); return x; } \u7136\u540e\u4e0d\u8981\u5fd8\u4e86\u719f\u6089\u7684conv_options\u51fd\u6570\uff0c\u5b9a\u4e49\u5982\u4e0b\uff1a inline torch::nn::Conv2dOptions conv_options(int64_t in_planes, int64_t out_planes, int64_t kerner_size, int64_t stride = 1, int64_t padding = 0, int groups = 1, bool with_bias = true) { torch::nn::Conv2dOptions conv_options = torch::nn::Conv2dOptions(in_planes, out_planes, kerner_size); conv_options.stride(stride); conv_options.padding(padding); conv_options.bias(with_bias); conv_options.groups(groups); return conv_options; } \u548c\u4e4b\u524d\u7ae0\u8282\u4e2d\u7684\u76f8\u6bd4\uff0c\u589e\u52a0\u4e86groups\u53c2\u6570\uff0c\u540c\u65f6with_bias\u9ed8\u8ba4\u6253\u5f00\uff0c\u4f7f\u7528\u9700\u8981\u6ce8\u610f\u4fee\u6539\u3002 2.ResNet\u4e3b\u4f53\u642d\u5efa \u5b9a\u4e49\u597dBlock\u6a21\u5757\u540e\u5c31\u53ef\u4ee5\u8bbe\u8ba1ResNet\u4e86\uff0cc++\u4e2dResNet\u6a21\u578b\u58f0\u660e\u7c7b\u4f3cpytorch\u4e2d\u7684ResNet\u3002\u4f46\u662f\u521d\u59cb\u5316\u53c2\u6570\u589e\u52a0\u4e00\u4e2amodel_type\uff0c\u8f85\u52a9\u5224\u65ad\u91c7\u7528\u54ea\u79cdBlock\u3002 class ResNetImpl : public torch::nn::Module { public: ResNetImpl(std::vector<int> layers, int num_classes = 1000, std::string model_type = \"resnet18\", int groups = 1, int width_per_group = 64); torch::Tensor forward(torch::Tensor x); std::vector<torch::Tensor> features(torch::Tensor x); torch::nn::Sequential _make_layer(int64_t planes, int64_t blocks, int64_t stride = 1); private: int expansion = 1; bool is_basic = true; int64_t inplanes = 64; int groups = 1; int base_width = 64; torch::nn::Conv2d conv1{ nullptr }; torch::nn::BatchNorm2d bn1{ nullptr }; torch::nn::Sequential layer1{ nullptr }; torch::nn::Sequential layer2{ nullptr }; torch::nn::Sequential layer3{ nullptr }; torch::nn::Sequential layer4{ nullptr }; torch::nn::Linear fc{nullptr}; }; TORCH_MODULE(ResNet); \u5728\u5b9e\u73b0\u521d\u59cb\u5316\u51fd\u6570\u4e4b\u524d\uff0c\u9700\u8981\u5b9e\u73b0_make_layer\u51fd\u6570\u3002\u5b9e\u73b0\u597d_make_layer\u51fd\u6570\u540e\u518d\u5b9e\u73b0ResNet\u521d\u59cb\u5316\u51fd\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a torch::nn::Sequential ResNetImpl::_make_layer(int64_t planes, int64_t blocks, int64_t stride) { torch::nn::Sequential downsample; if (stride != 1 || inplanes != planes * expansion) { downsample = torch::nn::Sequential( torch::nn::Conv2d(conv_options(inplanes, planes * expansion, 1, stride, 0, 1, false)), torch::nn::BatchNorm2d(planes * expansion) ); } torch::nn::Sequential layers; layers->push_back(Block(inplanes, planes, stride, downsample, groups, base_width, is_basic)); inplanes = planes * expansion; for (int64_t i = 1; i < blocks; i++) { layers->push_back(Block(inplanes, planes, 1, torch::nn::Sequential(), groups, base_width,is_basic)); } return layers; } ResNetImpl::ResNetImpl(std::vector<int> layers, int num_classes, std::string model_type, int _groups, int _width_per_group) { if (model_type != \"resnet18\" && model_type != \"resnet34\") { expansion = 4; is_basic = false; } groups = _groups; base_width = _width_per_group; conv1 = torch::nn::Conv2d(conv_options(3, 64, 7, 2, 3, 1, false)); bn1 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(64)); layer1 = torch::nn::Sequential(_make_layer(64, layers[0])); layer2 = torch::nn::Sequential(_make_layer(128, layers[1], 2)); layer3 = torch::nn::Sequential(_make_layer(256, layers[2], 2)); layer4 = torch::nn::Sequential(_make_layer(512, layers[3], 2)); fc = torch::nn::Linear(512 * expansion, num_classes); register_module(\"conv1\", conv1); register_module(\"bn1\", bn1); register_module(\"layer1\", layer1); register_module(\"layer2\", layer2); register_module(\"layer3\", layer3); register_module(\"layer4\", layer4); register_module(\"fc\", fc); } 3.\u524d\u5411\u4f20\u64ad\u53ca\u7279\u5f81\u63d0\u53d6 \u524d\u5411\u4f20\u64ad\u76f8\u5bf9\u7b80\u5355\uff0c\u76f4\u63a5\u6839\u636e\u5b9a\u4e49\u597d\u7684\u5c42\u5f80\u4e0b\u4f20\u64ad\u5373\u53ef\u3002 // \u5b9e\u73b0ResNetImpl\u7684forward\u7684\u6210\u5458\u65b9\u6cd5 torch::Tensor ResNetImpl::forward(torch::Tensor x) { x = conv1->forward(x); x = bn1->forward(x); x = torch::relu(x); x = torch::max_pool2d(x, 3, 2, 1); x = layer1->forward(x); x = layer2->forward(x); x = layer3->forward(x); x = layer4->forward(x); x = torch::avg_pool2d(x, 7, 1); x = x.view({ x.sizes()[0], -1 }); x = fc->forward(x); return torch::log_softmax(x, 1); } \u4f46\u662f\u672c\u6587\u662f\u4ecb\u7ecd\u5206\u5272\u7528\u7684\uff0c\u6240\u4ee5\u9700\u8981\u5bf9\u4e0d\u540c\u7684\u7279\u5f81\u5c42\u8fdb\u884c\u63d0\u53d6\uff0c\u5b58\u50a8\u5230 std::vector<torch::Tensor> \u4e2d\u3002 std::vector<torch::Tensor> ResNetImpl::features(torch::Tensor x){ std::vector<torch::Tensor> features; features.push_back(x); //push_back \u5b58\u653e\u6570\u636e x = conv1->forward(x); x = bn1->forward(x); x = torch::relu(x); features.push_back(x); x = torch::max_pool2d(x, 3, 2, 1); x = layer1->forward(x); features.push_back(x); x = layer2->forward(x); features.push_back(x); x = layer3->forward(x); features.push_back(x); x = layer4->forward(x); features.push_back(x); return features; } 3.U-Net\u89e3\u7801 \u4e0a\u9762\u7684ResNet\u90e8\u5206\u5176\u5b9e\u53ef\u4ee5\u5f00\u5355\u7ae0\u8be6\u7ec6\u8bb2\u89e3\uff0c\u4f46\u662f\u53c2\u7167\u6e90\u7801\u8bfb\u8005\u5e94\u8be5\u5bb9\u6613\u7406\u89e3\uff0c\u5c31\u76f4\u63a5\u653e\u4e00\u8d77\u3002\u5982\u679c\u4e0a\u9762\u7684\u5185\u5bb9\u662f\u5bf9torchvision\u5728libtorch\u4e2d\u7684\u4f18\u5316\uff0c\u4e0b\u9762\u7684\u90e8\u5206\u53ef\u4ee5\u770b\u6210\u76f4\u63a5\u5bf9SMP\u4e2dU-Net\u89e3\u7801\u7684c++\u590d\u5236\u3002 \u76f4\u63a5\u4e0a\u58f0\u660e\uff1a //.h\u4e2d\u7684\u5185\u5bb9 //attention and basic class SCSEModuleImpl: public torch::nn::Module{ public: SCSEModuleImpl(int in_channels, int reduction=16, bool use_attention = false); torch::Tensor forward(torch::Tensor x); private: bool use_attention = false; torch::nn::Sequential cSE{nullptr}; torch::nn::Sequential sSE{nullptr}; };TORCH_MODULE(SCSEModule); class Conv2dReLUImpl: public torch::nn::Module{ public: Conv2dReLUImpl(int in_channels, int out_channels, int kernel_size = 3, int padding = 1); torch::Tensor forward(torch::Tensor x); private: torch::nn::Conv2d conv2d{nullptr}; torch::nn::BatchNorm2d bn{nullptr}; };TORCH_MODULE(Conv2dReLU); //decoderblock and center block class DecoderBlockImpl: public torch::nn::Module{ public: DecoderBlockImpl(int in_channels, int skip_channels, int out_channels, bool skip = true, bool attention = false); torch::Tensor forward(torch::Tensor x, torch::Tensor skip); private: Conv2dReLU conv1{nullptr}; Conv2dReLU conv2{nullptr}; SCSEModule attention1{nullptr}; SCSEModule attention2{nullptr}; torch::nn::Upsample upsample{nullptr}; bool is_skip = true; };TORCH_MODULE(DecoderBlock); torch::nn::Sequential CenterBlock(int in_channels, int out_channels); class UNetDecoderImpl:public torch::nn::Module { public: UNetDecoderImpl(std::vector<int> encoder_channels, std::vector<int> decoder_channels, int n_blocks = 5, bool use_attention = false, bool use_center=false); torch::Tensor forward(std::vector<torch::Tensor> features); private: torch::nn::Sequential center{nullptr}; torch::nn::ModuleList blocks = torch::nn::ModuleList(); };TORCH_MODULE(UNetDecoder); #endif // UNETDECODER_H \u76f4\u63a5\u4e0a\u5b9a\u4e49\uff1a // \u58f0\u660e\u7684\u7c7b\u4e2d\u7684\u65b9\u6cd5\u7684\u5b9e\u73b0 SCSEModuleImpl::SCSEModuleImpl(int in_channels, int reduction, bool _use_attention){ use_attention = _use_attention; cSE = torch::nn::Sequential( torch::nn::AdaptiveAvgPool2d(torch::nn::AdaptiveAvgPool2dOptions(1)), torch::nn::Conv2d(conv_options(in_channels, in_channels / reduction, 1)), torch::nn::ReLU(torch::nn::ReLUOptions(true)), torch::nn::Conv2d(conv_options(in_channels / reduction, in_channels, 1)), torch::nn::Sigmoid()); sSE = torch::nn::Sequential(torch::nn::Conv2d(conv_options(in_channels, 1, 1)), torch::nn::Sigmoid()); register_module(\"cSE\",cSE); register_module(\"sSE\",sSE); } torch::Tensor SCSEModuleImpl::forward(torch::Tensor x){ if(!use_attention) return x; return x * cSE->forward(x) + x * sSE->forward(x); } Conv2dReLUImpl::Conv2dReLUImpl(int in_channels, int out_channels, int kernel_size, int padding){ conv2d = torch::nn::Conv2d(conv_options(in_channels,out_channels,kernel_size,1,padding)); bn = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(out_channels)); register_module(\"conv2d\", conv2d); register_module(\"bn\", bn); } torch::Tensor Conv2dReLUImpl::forward(torch::Tensor x){ x = conv2d->forward(x); x = bn->forward(x); return x; } DecoderBlockImpl::DecoderBlockImpl(int in_channels, int skip_channels, int out_channels, bool skip, bool attention){ conv1 = Conv2dReLU(in_channels + skip_channels, out_channels, 3, 1); conv2 = Conv2dReLU(out_channels, out_channels, 3, 1); register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); upsample = torch::nn::Upsample(torch::nn::UpsampleOptions().scale_factor(std::vector<double>({2,2})).mode(torch::kNearest)); attention1 = SCSEModule(in_channels + skip_channels, 16, attention); attention2 = SCSEModule(out_channels, 16, attention); register_module(\"attention1\", attention1); register_module(\"attention2\", attention2); is_skip = skip; } torch::Tensor DecoderBlockImpl::forward(torch::Tensor x, torch::Tensor skip){ x = upsample->forward(x); if (is_skip){ x = torch::cat({x, skip}, 1); x = attention1->forward(x); } x = conv1->forward(x); x = conv2->forward(x); x = attention2->forward(x); return x; } torch::nn::Sequential CenterBlock(int in_channels, int out_channels){ return torch::nn::Sequential(Conv2dReLU(in_channels, out_channels, 3, 1), Conv2dReLU(out_channels, out_channels, 3, 1)); } UNetDecoderImpl::UNetDecoderImpl(std::vector<int> encoder_channels, std::vector<int> decoder_channels, int n_blocks, bool use_attention, bool use_center) { if (n_blocks != decoder_channels.size()) throw \"Model depth not equal to your provided `decoder_channels`\"; std::reverse(std::begin(encoder_channels),std::end(encoder_channels)); // computing blocks input and output channels int head_channels = encoder_channels[0]; std::vector<int> out_channels = decoder_channels; decoder_channels.pop_back(); decoder_channels.insert(decoder_channels.begin(),head_channels); std::vector<int> in_channels = decoder_channels; encoder_channels.erase(encoder_channels.begin()); std::vector<int> skip_channels = encoder_channels; skip_channels[skip_channels.size()-1] = 0; if(use_center) center = CenterBlock(head_channels, head_channels); else center = torch::nn::Sequential(torch::nn::Identity()); //the last DecoderBlock of blocks need no skip tensor for (int i = 0; i< in_channels.size()-1; i++) { blocks->push_back(DecoderBlock(in_channels[i], skip_channels[i], out_channels[i], true, use_attention)); } blocks->push_back(DecoderBlock(in_channels[in_channels.size()-1], skip_channels[in_channels.size()-1], out_channels[in_channels.size()-1], false, use_attention)); register_module(\"center\", center); register_module(\"blocks\", blocks); } torch::Tensor UNetDecoderImpl::forward(std::vector<torch::Tensor> features){ std::reverse(std::begin(features),std::end(features)); torch::Tensor head = features[0]; features.erase(features.begin()); auto x = center->forward(head); for (int i = 0; i<blocks->size(); i++) { x = blocks[i]->as<DecoderBlock>()->forward(x, features[i]); } return x; } \u4e0d\u5c55\u5f00\u8bf4\u4e86\uff0c\u5185\u5bb9\u8f83\u591a\u3002\u540e\u7eed\u8fd8\u6709U-Net\u6574\u4f53\u548c\u5c01\u88c5\u2026 4.U-Net\u6574\u4f53\u8bbe\u8ba1 \u8fd9\u662fU-Net\u7684\u58f0\u660e\uff0c\u5206\u4e3a\u7f16\u7801\u5668\uff0c\u89e3\u7801\u5668\u548c\u5206\u5272\u5934\u3002 // .h class UNetImpl : public torch::nn::Module { public: UNetImpl(int num_classes, std::string encoder_name = \"resnet18\", std::string pretrained_path = \"\", int encoder_depth = 5, std::vector<int> decoder_channels={256, 128, 64, 32, 16}, bool use_attention = false); torch::Tensor forward(torch::Tensor x); private: ResNet encoder{nullptr}; UNetDecoder decoder{nullptr}; SegmentationHead segmentation_head{nullptr}; int num_classes = 1; std::vector<int> BasicChannels = {3, 64, 64, 128, 256, 512}; std::vector<int> BottleChannels = {3, 64, 256, 512, 1024, 2048}; std::map<std::string, std::vector<int>> name2layers = getParams(); };TORCH_MODULE(UNet); \u8fd9\u662f\u5b9e\u73b0\uff1a UNetImpl::UNetImpl(int _num_classes, std::string encoder_name, std::string pretrained_path, int encoder_depth, std::vector<int> decoder_channels, bool use_attention){ num_classes = _num_classes; std::vector<int> encoder_channels = BasicChannels; if(!name2layers.count(encoder_name)) throw \"encoder name must in {resnet18, resnet34, resnet50, resnet101}\"; if(encoder_name!=\"resnet18\" && encoder_name!=\"resnet34\"){ encoder_channels = BottleChannels; } encoder = pretrained_resnet(1000, encoder_name, pretrained_path); decoder = UNetDecoder(encoder_channels,decoder_channels, encoder_depth, use_attention, false); segmentation_head = SegmentationHead(decoder_channels[decoder_channels.size()-1], num_classes, 1, 1); register_module(\"encoder\",encoder); register_module(\"decoder\",decoder); register_module(\"segmentation_head\",segmentation_head); } torch::Tensor UNetImpl::forward(torch::Tensor x){ std::vector<torch::Tensor> features = encoder->features(x); x = decoder->forward(features); x = segmentation_head->forward(x); return x; } \u5206\u5272\u5934\uff1a // segmentation head\u7684\u5b9e\u73b0 class SegmentationHeadImpl: public torch::nn::Module{ public: SegmentationHeadImpl(int in_channels, int out_channels, int kernel_size=3, double upsampling=1); torch::Tensor forward(torch::Tensor x); private: torch::nn::Conv2d conv2d{nullptr}; torch::nn::Upsample upsampling{nullptr}; };TORCH_MODULE(SegmentationHead); SegmentationHeadImpl::SegmentationHeadImpl(int in_channels, int out_channels, int kernel_size, double _upsampling){ conv2d = torch::nn::Conv2d(conv_options(in_channels, out_channels, kernel_size, 1, kernel_size / 2)); upsampling = torch::nn::Upsample(upsample_options(std::vector<double>{_upsampling,_upsampling})); register_module(\"conv2d\",conv2d); } torch::Tensor SegmentationHeadImpl::forward(torch::Tensor x){ x = conv2d->forward(x); x = upsampling->forward(x); return x; } \u5185\u5bb9\u8fc7\u4e8e\u591a\uff0c\u535a\u5ba2\u5199\u5f97\u6bd4\u8f83\u8d39\u52b2\u3002\u76f4\u63a5\u5c06\u5c01\u88c5\u548c\u6d4b\u8bd5\u4ee3\u7801\u653e\u5230GitHub\u4e0a\u4e86\uff0c\u5728\u8fd9\u91cc\u3002\u91cc\u9762\u96c6\u6210\u4e86\u5305\u62ecResNet\uff0cResNext\u548c\u53ef\u80fd\u7684ResNest\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u76ee\u524d\u7f51\u7edc\u67b6\u6784\u5b9e\u73b0\u4e86FPN\u548cU-Net\u3002\u5982\u679c\u9879\u76ee\u5185\u5bb9\u6709\u5e2e\u5230\u4f60\u8bf7\u52a1\u5fc5\u7ed9\u4e2astar\uff0c\u4f5c\u8005\u9700\u8981\u8fd9\u4efd\u652f\u6301\u4f5c\u4e3a\u52a8\u529b\uff01\uff01\uff01 \u5b9e\u9645\u6d4b\u8bd5U-Net\u5728c++\u4ee3\u7801\u6267\u884c\u6548\u7387\uff0c\u53d1\u73b0\u4e0epython\u5728cpu\u4e0b\u901f\u5ea6\u4e00\u81f4\uff0cGPU\u4e0b\u5feb35%+\u3002c++\u771f\u9999\u3002 AllentDan\u5927\u4f6c\u7684\u4ee3\u7801\u5730\u5740: https://github.com/AllentDan/LibtorchTutorials/tree/main/lesson6-Segmentation","title":"\u7b2c\u516d\u7ae0 \u5206\u5272\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b"},{"location":"chapter6/#_1","text":"\u672c\u7ae0\u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u5982\u4f55\u7528C++\u5b9e\u73b0\u4e00\u4e2a\u8bed\u4e49\u5206\u5272\u5668\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u8bad\u7ec3\u548c\u9884\u6d4b\u7684\u529f\u80fd\u3002\u672c\u6587\u7684\u5206\u5272\u6a21\u578b\u67b6\u6784\u4f7f\u7528\u7b80\u5355\u7684U-Net\u7ed3\u6784\uff0c\u4ee3\u7801\u7ed3\u6784\u53c2\u8003\u4e86 qubvel segmentation \u4e2d\u7684U-Net\u90e8\u5206\uff0c\u8be5\u9879\u76ee\u7b80\u79f0SMP\uff0c\u662f\u57fa\u4e8epytorch\u5b9e\u73b0\u7684\u5f00\u6e90\u8bed\u4e49\u5206\u5272\u9879\u76ee\u3002\u672c\u6587\u5206\u4eab\u7684c++\u6a21\u578b\u51e0\u4e4e\u5b8c\u7f8e\u590d\u73b0\u4e86python\u7684\u7248\u672c\u3002","title":"\u7b2c\u516d\u7ae0 \u5206\u5272\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b"},{"location":"chapter6/#1","text":"\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0bU-Net\u6a21\u578b\u3002U-Net\u6a21\u578b\u7684\u63d0\u51fa\u662f\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u76f8\u6bd4\u4e8e\u5f53\u65f6\u7684\u5176\u4ed6\u6a21\u578b\u7ed3\u6784\uff0cU-Net\u7684\u5206\u5272\u80fd\u529b\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002\u4e00\u4e2a\u7ecf\u5178\u7684U-Net\u7ed3\u6784\u56fe\u5982\u4e0b\uff1a U-Net\u6a21\u578b\u91c7\u7528\u5178\u578b\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5de6\u8fb9\u7684\u7f16\u7801\u90e8\u5206\u7c7b\u4f3cVGG\u6a21\u578b\uff0c\u662f\u53cc\u5377\u79ef+\u4e0b\u91c7\u6837\u7684\u591a\u6b21\u5806\u53e0\u3002U-Net\u6a21\u578b\u53f3\u8fb9\u7684\u89e3\u7801\u90e8\u5206\u540c\u6837\u662f\u53cc\u5377\u79ef\uff0c\u4f46\u662f\u4e3a\u4e86\u5f97\u5230\u63a5\u8fd1\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\u7684\u8f93\u51fa\u56fe\u50cf\uff0c\u9488\u5bf9\u7f16\u7801\u7684\u4e0b\u91c7\u6837\u5b9e\u65bd\u4e86\u5bf9\u5e94\u7684\u4e0a\u91c7\u6837\u3002\u6700\u91cd\u8981\u7684\u662f\uff0cU-Net\u4e4b\u6240\u4ee5\u6548\u679c\u7a81\u51fa\uff0c\u91cd\u8981\u539f\u56e0\u5728\u4e8e\u5176\u5728\u89e3\u7801\u90e8\u5206\u5229\u7528\u4e86\u7f16\u7801\u73af\u8282\u7684\u7279\u5f81\u56fe\uff0c\u62fc\u63a5\u7f16\u7801\u548c\u89e3\u7801\u7684\u7279\u5f81\u56fe\uff0c\u518d\u5bf9\u62fc\u63a5\u540e\u7279\u5f81\u56fe\u5377\u79ef\u4e0a\u91c7\u6837\uff0c\u91cd\u590d\u591a\u6b21\u5f97\u5230\u89e3\u7801\u8f93\u51fa\u3002","title":"1.\u6a21\u578b\u7b80\u4ecb"},{"location":"chapter6/#2resnet","text":"\u672c\u6587\u4ecb\u7ecd\u7684\u7f16\u7801\u5668\u4f7f\u7528ResNet\u7f51\u7edc\uff0c\u540c\u65f6\u53ef\u4ee5\u50cf\u7b2c\u4e94\u7ae0\u4e00\u6837\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u5373\u9aa8\u5e72\u7f51\u7edc\u4e3aImageNet\u9884\u8bad\u7ec3\u7684ResNet\u3002\u8bdd\u4e0d\u591a\u8bf4\uff0c\u76f4\u63a5\u4e0ac++\u7684ResNet\u4ee3\u7801\u3002 1.Block\u642d\u5efa \u5efa\u8bae\u770b\u672c\u6587\u4ee3\u7801\u65f6\u6253\u5f00pytorch\u7684torchvision\u4e2d\u7684resnet.py\uff0c\u5bf9\u6bd4\u9605\u8bfb\u3002 \u9996\u5148\u662f\u57fa\u7840\u6a21\u5757\uff0cpytorch\u9488\u5bf9resnet18\uff0cresne34\u548cresnet50\uff0cresnet101\uff0cresnet152\u8fdb\u884c\u5206\u7c7b\uff0cresnet18\u4e0eresnet34\u5747\u4f7f\u7528BasicBlock\uff0c\u800c\u66f4\u6df1\u7684\u7f51\u7edc\u4f7f\u7528BottleNeck\u3002\u6211\u4e0d\u60f3\u4f7f\u7528\u6a21\u677f\u7c7b\u7f16\u7a0b\uff0c\u5c31\u76f4\u63a5\u5c06\u4e24\u4e2a\u6a21\u5757\u5408\u4e3a\u4e00\u4f53\u3002\u58f0\u660e\u5982\u4e0b\uff1a class BlockImpl : public torch::nn::Module { public: BlockImpl(int64_t inplanes, int64_t planes, int64_t stride_ = 1, torch::nn::Sequential downsample_ = nullptr, int groups = 1, int base_width = 64, bool is_basic = true); torch::Tensor forward(torch::Tensor x); torch::nn::Sequential downsample{ nullptr }; private: bool is_basic = true; int64_t stride = 1; torch::nn::Conv2d conv1{ nullptr }; torch::nn::BatchNorm2d bn1{ nullptr }; torch::nn::Conv2d conv2{ nullptr }; torch::nn::BatchNorm2d bn2{ nullptr }; torch::nn::Conv2d conv3{ nullptr }; torch::nn::BatchNorm2d bn3{ nullptr }; }; TORCH_MODULE(Block); \u53ef\u4ee5\u53d1\u73b0\uff0c\u5176\u5b9e\u662f\u76f4\u63a5\u58f0\u660e\u4e86\u4e09\u4e2aconv\u7ed3\u6784\u548c\u4e00\u4e2ais_basic\u6807\u5fd7\u4f4d\u5224\u65ad\u5b9a\u4e49\u65f6\u8fdb\u884cBasicBlock\u5b9a\u4e49\u8fd8\u662fBottleNeck\u5b9a\u4e49\u3002\u4e0b\u9762\u662f\u5176\u5b9a\u4e49: // \u6784\u9020\u51fd\u6570\u7684\u5b9e\u73b0 BlockImpl::BlockImpl(int64_t inplanes, int64_t planes, int64_t stride_, torch::nn::Sequential downsample_, int groups, int base_width, bool _is_basic) { downsample = downsample_; stride = stride_; int width = int(planes * (base_width / 64.)) * groups; conv1 = torch::nn::Conv2d(conv_options(inplanes, width, 3, stride_, 1, groups, false)); bn1 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(width)); conv2 = torch::nn::Conv2d(conv_options(width, width, 3, 1, 1, groups, false)); bn2 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(width)); is_basic = _is_basic; if (!is_basic) { conv1 = torch::nn::Conv2d(conv_options(inplanes, width, 1, 1, 0, 1, false)); conv2 = torch::nn::Conv2d(conv_options(width, width, 3, stride_, 1, groups, false)); conv3 = torch::nn::Conv2d(conv_options(width, planes * 4, 1, 1, 0, 1, false)); bn3 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(planes * 4)); } register_module(\"conv1\", conv1); register_module(\"bn1\", bn1); register_module(\"conv2\", conv2); register_module(\"bn2\", bn2); if (!is_basic) { register_module(\"conv3\", conv3); register_module(\"bn3\", bn3); } if (!downsample->is_empty()) { register_module(\"downsample\", downsample); } } torch::Tensor BlockImpl::forward(torch::Tensor x) { torch::Tensor residual = x.clone(); x = conv1->forward(x); x = bn1->forward(x); x = torch::relu(x); x = conv2->forward(x); x = bn2->forward(x); if (!is_basic) { x = torch::relu(x); x = conv3->forward(x); x = bn3->forward(x); } if (!downsample->is_empty()) { residual = downsample->forward(residual); } x += residual; x = torch::relu(x); return x; } \u7136\u540e\u4e0d\u8981\u5fd8\u4e86\u719f\u6089\u7684conv_options\u51fd\u6570\uff0c\u5b9a\u4e49\u5982\u4e0b\uff1a inline torch::nn::Conv2dOptions conv_options(int64_t in_planes, int64_t out_planes, int64_t kerner_size, int64_t stride = 1, int64_t padding = 0, int groups = 1, bool with_bias = true) { torch::nn::Conv2dOptions conv_options = torch::nn::Conv2dOptions(in_planes, out_planes, kerner_size); conv_options.stride(stride); conv_options.padding(padding); conv_options.bias(with_bias); conv_options.groups(groups); return conv_options; } \u548c\u4e4b\u524d\u7ae0\u8282\u4e2d\u7684\u76f8\u6bd4\uff0c\u589e\u52a0\u4e86groups\u53c2\u6570\uff0c\u540c\u65f6with_bias\u9ed8\u8ba4\u6253\u5f00\uff0c\u4f7f\u7528\u9700\u8981\u6ce8\u610f\u4fee\u6539\u3002 2.ResNet\u4e3b\u4f53\u642d\u5efa \u5b9a\u4e49\u597dBlock\u6a21\u5757\u540e\u5c31\u53ef\u4ee5\u8bbe\u8ba1ResNet\u4e86\uff0cc++\u4e2dResNet\u6a21\u578b\u58f0\u660e\u7c7b\u4f3cpytorch\u4e2d\u7684ResNet\u3002\u4f46\u662f\u521d\u59cb\u5316\u53c2\u6570\u589e\u52a0\u4e00\u4e2amodel_type\uff0c\u8f85\u52a9\u5224\u65ad\u91c7\u7528\u54ea\u79cdBlock\u3002 class ResNetImpl : public torch::nn::Module { public: ResNetImpl(std::vector<int> layers, int num_classes = 1000, std::string model_type = \"resnet18\", int groups = 1, int width_per_group = 64); torch::Tensor forward(torch::Tensor x); std::vector<torch::Tensor> features(torch::Tensor x); torch::nn::Sequential _make_layer(int64_t planes, int64_t blocks, int64_t stride = 1); private: int expansion = 1; bool is_basic = true; int64_t inplanes = 64; int groups = 1; int base_width = 64; torch::nn::Conv2d conv1{ nullptr }; torch::nn::BatchNorm2d bn1{ nullptr }; torch::nn::Sequential layer1{ nullptr }; torch::nn::Sequential layer2{ nullptr }; torch::nn::Sequential layer3{ nullptr }; torch::nn::Sequential layer4{ nullptr }; torch::nn::Linear fc{nullptr}; }; TORCH_MODULE(ResNet); \u5728\u5b9e\u73b0\u521d\u59cb\u5316\u51fd\u6570\u4e4b\u524d\uff0c\u9700\u8981\u5b9e\u73b0_make_layer\u51fd\u6570\u3002\u5b9e\u73b0\u597d_make_layer\u51fd\u6570\u540e\u518d\u5b9e\u73b0ResNet\u521d\u59cb\u5316\u51fd\u6570\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a torch::nn::Sequential ResNetImpl::_make_layer(int64_t planes, int64_t blocks, int64_t stride) { torch::nn::Sequential downsample; if (stride != 1 || inplanes != planes * expansion) { downsample = torch::nn::Sequential( torch::nn::Conv2d(conv_options(inplanes, planes * expansion, 1, stride, 0, 1, false)), torch::nn::BatchNorm2d(planes * expansion) ); } torch::nn::Sequential layers; layers->push_back(Block(inplanes, planes, stride, downsample, groups, base_width, is_basic)); inplanes = planes * expansion; for (int64_t i = 1; i < blocks; i++) { layers->push_back(Block(inplanes, planes, 1, torch::nn::Sequential(), groups, base_width,is_basic)); } return layers; } ResNetImpl::ResNetImpl(std::vector<int> layers, int num_classes, std::string model_type, int _groups, int _width_per_group) { if (model_type != \"resnet18\" && model_type != \"resnet34\") { expansion = 4; is_basic = false; } groups = _groups; base_width = _width_per_group; conv1 = torch::nn::Conv2d(conv_options(3, 64, 7, 2, 3, 1, false)); bn1 = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(64)); layer1 = torch::nn::Sequential(_make_layer(64, layers[0])); layer2 = torch::nn::Sequential(_make_layer(128, layers[1], 2)); layer3 = torch::nn::Sequential(_make_layer(256, layers[2], 2)); layer4 = torch::nn::Sequential(_make_layer(512, layers[3], 2)); fc = torch::nn::Linear(512 * expansion, num_classes); register_module(\"conv1\", conv1); register_module(\"bn1\", bn1); register_module(\"layer1\", layer1); register_module(\"layer2\", layer2); register_module(\"layer3\", layer3); register_module(\"layer4\", layer4); register_module(\"fc\", fc); } 3.\u524d\u5411\u4f20\u64ad\u53ca\u7279\u5f81\u63d0\u53d6 \u524d\u5411\u4f20\u64ad\u76f8\u5bf9\u7b80\u5355\uff0c\u76f4\u63a5\u6839\u636e\u5b9a\u4e49\u597d\u7684\u5c42\u5f80\u4e0b\u4f20\u64ad\u5373\u53ef\u3002 // \u5b9e\u73b0ResNetImpl\u7684forward\u7684\u6210\u5458\u65b9\u6cd5 torch::Tensor ResNetImpl::forward(torch::Tensor x) { x = conv1->forward(x); x = bn1->forward(x); x = torch::relu(x); x = torch::max_pool2d(x, 3, 2, 1); x = layer1->forward(x); x = layer2->forward(x); x = layer3->forward(x); x = layer4->forward(x); x = torch::avg_pool2d(x, 7, 1); x = x.view({ x.sizes()[0], -1 }); x = fc->forward(x); return torch::log_softmax(x, 1); } \u4f46\u662f\u672c\u6587\u662f\u4ecb\u7ecd\u5206\u5272\u7528\u7684\uff0c\u6240\u4ee5\u9700\u8981\u5bf9\u4e0d\u540c\u7684\u7279\u5f81\u5c42\u8fdb\u884c\u63d0\u53d6\uff0c\u5b58\u50a8\u5230 std::vector<torch::Tensor> \u4e2d\u3002 std::vector<torch::Tensor> ResNetImpl::features(torch::Tensor x){ std::vector<torch::Tensor> features; features.push_back(x); //push_back \u5b58\u653e\u6570\u636e x = conv1->forward(x); x = bn1->forward(x); x = torch::relu(x); features.push_back(x); x = torch::max_pool2d(x, 3, 2, 1); x = layer1->forward(x); features.push_back(x); x = layer2->forward(x); features.push_back(x); x = layer3->forward(x); features.push_back(x); x = layer4->forward(x); features.push_back(x); return features; }","title":"2.\u7f16\u7801\u5668\u2014ResNet"},{"location":"chapter6/#3u-net","text":"\u4e0a\u9762\u7684ResNet\u90e8\u5206\u5176\u5b9e\u53ef\u4ee5\u5f00\u5355\u7ae0\u8be6\u7ec6\u8bb2\u89e3\uff0c\u4f46\u662f\u53c2\u7167\u6e90\u7801\u8bfb\u8005\u5e94\u8be5\u5bb9\u6613\u7406\u89e3\uff0c\u5c31\u76f4\u63a5\u653e\u4e00\u8d77\u3002\u5982\u679c\u4e0a\u9762\u7684\u5185\u5bb9\u662f\u5bf9torchvision\u5728libtorch\u4e2d\u7684\u4f18\u5316\uff0c\u4e0b\u9762\u7684\u90e8\u5206\u53ef\u4ee5\u770b\u6210\u76f4\u63a5\u5bf9SMP\u4e2dU-Net\u89e3\u7801\u7684c++\u590d\u5236\u3002 \u76f4\u63a5\u4e0a\u58f0\u660e\uff1a //.h\u4e2d\u7684\u5185\u5bb9 //attention and basic class SCSEModuleImpl: public torch::nn::Module{ public: SCSEModuleImpl(int in_channels, int reduction=16, bool use_attention = false); torch::Tensor forward(torch::Tensor x); private: bool use_attention = false; torch::nn::Sequential cSE{nullptr}; torch::nn::Sequential sSE{nullptr}; };TORCH_MODULE(SCSEModule); class Conv2dReLUImpl: public torch::nn::Module{ public: Conv2dReLUImpl(int in_channels, int out_channels, int kernel_size = 3, int padding = 1); torch::Tensor forward(torch::Tensor x); private: torch::nn::Conv2d conv2d{nullptr}; torch::nn::BatchNorm2d bn{nullptr}; };TORCH_MODULE(Conv2dReLU); //decoderblock and center block class DecoderBlockImpl: public torch::nn::Module{ public: DecoderBlockImpl(int in_channels, int skip_channels, int out_channels, bool skip = true, bool attention = false); torch::Tensor forward(torch::Tensor x, torch::Tensor skip); private: Conv2dReLU conv1{nullptr}; Conv2dReLU conv2{nullptr}; SCSEModule attention1{nullptr}; SCSEModule attention2{nullptr}; torch::nn::Upsample upsample{nullptr}; bool is_skip = true; };TORCH_MODULE(DecoderBlock); torch::nn::Sequential CenterBlock(int in_channels, int out_channels); class UNetDecoderImpl:public torch::nn::Module { public: UNetDecoderImpl(std::vector<int> encoder_channels, std::vector<int> decoder_channels, int n_blocks = 5, bool use_attention = false, bool use_center=false); torch::Tensor forward(std::vector<torch::Tensor> features); private: torch::nn::Sequential center{nullptr}; torch::nn::ModuleList blocks = torch::nn::ModuleList(); };TORCH_MODULE(UNetDecoder); #endif // UNETDECODER_H \u76f4\u63a5\u4e0a\u5b9a\u4e49\uff1a // \u58f0\u660e\u7684\u7c7b\u4e2d\u7684\u65b9\u6cd5\u7684\u5b9e\u73b0 SCSEModuleImpl::SCSEModuleImpl(int in_channels, int reduction, bool _use_attention){ use_attention = _use_attention; cSE = torch::nn::Sequential( torch::nn::AdaptiveAvgPool2d(torch::nn::AdaptiveAvgPool2dOptions(1)), torch::nn::Conv2d(conv_options(in_channels, in_channels / reduction, 1)), torch::nn::ReLU(torch::nn::ReLUOptions(true)), torch::nn::Conv2d(conv_options(in_channels / reduction, in_channels, 1)), torch::nn::Sigmoid()); sSE = torch::nn::Sequential(torch::nn::Conv2d(conv_options(in_channels, 1, 1)), torch::nn::Sigmoid()); register_module(\"cSE\",cSE); register_module(\"sSE\",sSE); } torch::Tensor SCSEModuleImpl::forward(torch::Tensor x){ if(!use_attention) return x; return x * cSE->forward(x) + x * sSE->forward(x); } Conv2dReLUImpl::Conv2dReLUImpl(int in_channels, int out_channels, int kernel_size, int padding){ conv2d = torch::nn::Conv2d(conv_options(in_channels,out_channels,kernel_size,1,padding)); bn = torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(out_channels)); register_module(\"conv2d\", conv2d); register_module(\"bn\", bn); } torch::Tensor Conv2dReLUImpl::forward(torch::Tensor x){ x = conv2d->forward(x); x = bn->forward(x); return x; } DecoderBlockImpl::DecoderBlockImpl(int in_channels, int skip_channels, int out_channels, bool skip, bool attention){ conv1 = Conv2dReLU(in_channels + skip_channels, out_channels, 3, 1); conv2 = Conv2dReLU(out_channels, out_channels, 3, 1); register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); upsample = torch::nn::Upsample(torch::nn::UpsampleOptions().scale_factor(std::vector<double>({2,2})).mode(torch::kNearest)); attention1 = SCSEModule(in_channels + skip_channels, 16, attention); attention2 = SCSEModule(out_channels, 16, attention); register_module(\"attention1\", attention1); register_module(\"attention2\", attention2); is_skip = skip; } torch::Tensor DecoderBlockImpl::forward(torch::Tensor x, torch::Tensor skip){ x = upsample->forward(x); if (is_skip){ x = torch::cat({x, skip}, 1); x = attention1->forward(x); } x = conv1->forward(x); x = conv2->forward(x); x = attention2->forward(x); return x; } torch::nn::Sequential CenterBlock(int in_channels, int out_channels){ return torch::nn::Sequential(Conv2dReLU(in_channels, out_channels, 3, 1), Conv2dReLU(out_channels, out_channels, 3, 1)); } UNetDecoderImpl::UNetDecoderImpl(std::vector<int> encoder_channels, std::vector<int> decoder_channels, int n_blocks, bool use_attention, bool use_center) { if (n_blocks != decoder_channels.size()) throw \"Model depth not equal to your provided `decoder_channels`\"; std::reverse(std::begin(encoder_channels),std::end(encoder_channels)); // computing blocks input and output channels int head_channels = encoder_channels[0]; std::vector<int> out_channels = decoder_channels; decoder_channels.pop_back(); decoder_channels.insert(decoder_channels.begin(),head_channels); std::vector<int> in_channels = decoder_channels; encoder_channels.erase(encoder_channels.begin()); std::vector<int> skip_channels = encoder_channels; skip_channels[skip_channels.size()-1] = 0; if(use_center) center = CenterBlock(head_channels, head_channels); else center = torch::nn::Sequential(torch::nn::Identity()); //the last DecoderBlock of blocks need no skip tensor for (int i = 0; i< in_channels.size()-1; i++) { blocks->push_back(DecoderBlock(in_channels[i], skip_channels[i], out_channels[i], true, use_attention)); } blocks->push_back(DecoderBlock(in_channels[in_channels.size()-1], skip_channels[in_channels.size()-1], out_channels[in_channels.size()-1], false, use_attention)); register_module(\"center\", center); register_module(\"blocks\", blocks); } torch::Tensor UNetDecoderImpl::forward(std::vector<torch::Tensor> features){ std::reverse(std::begin(features),std::end(features)); torch::Tensor head = features[0]; features.erase(features.begin()); auto x = center->forward(head); for (int i = 0; i<blocks->size(); i++) { x = blocks[i]->as<DecoderBlock>()->forward(x, features[i]); } return x; } \u4e0d\u5c55\u5f00\u8bf4\u4e86\uff0c\u5185\u5bb9\u8f83\u591a\u3002\u540e\u7eed\u8fd8\u6709U-Net\u6574\u4f53\u548c\u5c01\u88c5\u2026","title":"3.U-Net\u89e3\u7801"},{"location":"chapter6/#4u-net","text":"\u8fd9\u662fU-Net\u7684\u58f0\u660e\uff0c\u5206\u4e3a\u7f16\u7801\u5668\uff0c\u89e3\u7801\u5668\u548c\u5206\u5272\u5934\u3002 // .h class UNetImpl : public torch::nn::Module { public: UNetImpl(int num_classes, std::string encoder_name = \"resnet18\", std::string pretrained_path = \"\", int encoder_depth = 5, std::vector<int> decoder_channels={256, 128, 64, 32, 16}, bool use_attention = false); torch::Tensor forward(torch::Tensor x); private: ResNet encoder{nullptr}; UNetDecoder decoder{nullptr}; SegmentationHead segmentation_head{nullptr}; int num_classes = 1; std::vector<int> BasicChannels = {3, 64, 64, 128, 256, 512}; std::vector<int> BottleChannels = {3, 64, 256, 512, 1024, 2048}; std::map<std::string, std::vector<int>> name2layers = getParams(); };TORCH_MODULE(UNet); \u8fd9\u662f\u5b9e\u73b0\uff1a UNetImpl::UNetImpl(int _num_classes, std::string encoder_name, std::string pretrained_path, int encoder_depth, std::vector<int> decoder_channels, bool use_attention){ num_classes = _num_classes; std::vector<int> encoder_channels = BasicChannels; if(!name2layers.count(encoder_name)) throw \"encoder name must in {resnet18, resnet34, resnet50, resnet101}\"; if(encoder_name!=\"resnet18\" && encoder_name!=\"resnet34\"){ encoder_channels = BottleChannels; } encoder = pretrained_resnet(1000, encoder_name, pretrained_path); decoder = UNetDecoder(encoder_channels,decoder_channels, encoder_depth, use_attention, false); segmentation_head = SegmentationHead(decoder_channels[decoder_channels.size()-1], num_classes, 1, 1); register_module(\"encoder\",encoder); register_module(\"decoder\",decoder); register_module(\"segmentation_head\",segmentation_head); } torch::Tensor UNetImpl::forward(torch::Tensor x){ std::vector<torch::Tensor> features = encoder->features(x); x = decoder->forward(features); x = segmentation_head->forward(x); return x; } \u5206\u5272\u5934\uff1a // segmentation head\u7684\u5b9e\u73b0 class SegmentationHeadImpl: public torch::nn::Module{ public: SegmentationHeadImpl(int in_channels, int out_channels, int kernel_size=3, double upsampling=1); torch::Tensor forward(torch::Tensor x); private: torch::nn::Conv2d conv2d{nullptr}; torch::nn::Upsample upsampling{nullptr}; };TORCH_MODULE(SegmentationHead); SegmentationHeadImpl::SegmentationHeadImpl(int in_channels, int out_channels, int kernel_size, double _upsampling){ conv2d = torch::nn::Conv2d(conv_options(in_channels, out_channels, kernel_size, 1, kernel_size / 2)); upsampling = torch::nn::Upsample(upsample_options(std::vector<double>{_upsampling,_upsampling})); register_module(\"conv2d\",conv2d); } torch::Tensor SegmentationHeadImpl::forward(torch::Tensor x){ x = conv2d->forward(x); x = upsampling->forward(x); return x; } \u5185\u5bb9\u8fc7\u4e8e\u591a\uff0c\u535a\u5ba2\u5199\u5f97\u6bd4\u8f83\u8d39\u52b2\u3002\u76f4\u63a5\u5c06\u5c01\u88c5\u548c\u6d4b\u8bd5\u4ee3\u7801\u653e\u5230GitHub\u4e0a\u4e86\uff0c\u5728\u8fd9\u91cc\u3002\u91cc\u9762\u96c6\u6210\u4e86\u5305\u62ecResNet\uff0cResNext\u548c\u53ef\u80fd\u7684ResNest\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u76ee\u524d\u7f51\u7edc\u67b6\u6784\u5b9e\u73b0\u4e86FPN\u548cU-Net\u3002\u5982\u679c\u9879\u76ee\u5185\u5bb9\u6709\u5e2e\u5230\u4f60\u8bf7\u52a1\u5fc5\u7ed9\u4e2astar\uff0c\u4f5c\u8005\u9700\u8981\u8fd9\u4efd\u652f\u6301\u4f5c\u4e3a\u52a8\u529b\uff01\uff01\uff01 \u5b9e\u9645\u6d4b\u8bd5U-Net\u5728c++\u4ee3\u7801\u6267\u884c\u6548\u7387\uff0c\u53d1\u73b0\u4e0epython\u5728cpu\u4e0b\u901f\u5ea6\u4e00\u81f4\uff0cGPU\u4e0b\u5feb35%+\u3002c++\u771f\u9999\u3002 AllentDan\u5927\u4f6c\u7684\u4ee3\u7801\u5730\u5740: https://github.com/AllentDan/LibtorchTutorials/tree/main/lesson6-Segmentation","title":"4.U-Net\u6574\u4f53\u8bbe\u8ba1"},{"location":"chapter7/","text":"\u7b2c\u4e03\u7ae0 \u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b \u9605\u8bfb\u672c\u6587\u9700\u8981\u6709\u57fa\u7840\u7684pytorch\u7f16\u7a0b\u7ecf\u9a8c\uff0c\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u76f8\u5173\u77e5\u8bc6\uff0c\u4e0d\u7528\u5f88\u6df1\u5165\uff0c\u5927\u81f4\u4e86\u89e3\u6982\u5ff5\u5373\u53ef\u3002 \u672c\u7ae0\u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u5982\u4f55\u7528C++\u5b9e\u73b0\u4e00\u4e2a\u76ee\u6807\u68c0\u6d4b\u5668\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u8bad\u7ec3\u548c\u9884\u6d4b\u7684\u529f\u80fd\u3002\u672c\u6587\u7684\u5206\u5272\u6a21\u578b\u67b6\u6784\u4f7f\u7528yolov4-tiny\u7ed3\u6784\uff0c\u4ee3\u7801\u7ed3\u6784\u53c2\u8003\u4e86 bubbliiiing yolov4-tiny \uff0c\u672c\u6587\u5206\u4eab\u7684c++\u6a21\u578b\u51e0\u4e4e\u5b8c\u7f8e\u590d\u73b0\u4e86pytorch\u7684\u7248\u672c\uff0c\u4e14\u5177\u6709\u901f\u5ea6\u4f18\u52bf\uff0c30-40%\u7684\u901f\u5ea6\u63d0\u5347\u3002 1.\u6a21\u578b\u7b80\u4ecb \u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0byolov4-tiny\u6a21\u578b\u3002yolov4-tiny\u6a21\u578b\u662fYOLO(you only look once)\u7cfb\u5217\u6a21\u578b\u4e2d\uff0cversion 4\u7684\u8f7b\u5de7\u7248\uff0c\u76f8\u6bd4\u4e8eyolov4\uff0c\u5b83\u727a\u7272\u4e86\u90e8\u5206\u7cbe\u5ea6\u4ee5\u5b9e\u73b0\u901f\u5ea6\u4e0a\u7684\u5927\u5e45\u63d0\u5347\u3002yolov4_tiny\u6a21\u578b\u7ed3\u6784\u5982\u56fe\uff08\u56fe\u7247\u6765\u6e90\u81ea\u8fd9\uff09\uff1a \u53ef\u4ee5\u53d1\u73b0\u6a21\u578b\u7ed3\u6784\u975e\u5e38\u7b80\u5355\uff0c\u4ee5CSPDarknet53-tiny\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0cFPN\u4e3a\u9888\u90e8(neck)\uff0cYolo head\u4e3a\u5934\u90e8\u3002\u6700\u540e\u8f93\u51fa\u4e24\u4e2a\u7279\u5f81\u5c42\uff0c\u5206\u522b\u662f\u539f\u56fe\u4e0b\u91c7\u683732\u500d\u548c\u4e0b\u91c7\u683716\u500d\u7684\u7279\u5f81\u56fe\u3002\u8bad\u7ec3\u65f6\uff0c\u4ee5\u8fd9\u4e24\u4e2a\u7279\u5f81\u56fe\u5206\u522b\u8f93\u5165\u635f\u5931\u8ba1\u7b97\u4e2d\u8ba1\u7b97\u635f\u5931\uff0c\u518d\u5c06\u635f\u5931\u6c42\u548c(\u6216\u5e73\u5747\uff0c\u600e\u4e48\u90fd\u597d)\uff0c\u540e\u505a\u53cd\u5411\u4f20\u64ad\uff0c\u9884\u6d4b\u65f6\u5c06\u4e24\u4e2a\u7279\u5f81\u56fe\u89e3\u7801\u51fa\u7684\u7ed3\u679c\u505a\u5e76\u96c6\u518d\u505aNMS(\u975e\u6781\u5927\u503c\u6291\u5236)\u3002 2.\u9aa8\u5e72\u7f51\u7edc CSPDarknet53-tiny\u662f CSPNet \u7684\u4e00\u79cd\uff0cCSPNet\u53d1\u8868\u4e8eCVPR2019\uff0c\u662f\u7528\u4e8e\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u68c0\u6d4b\u6027\u80fd\u7684\u4e00\u79cd\u9aa8\u5e72\u7f51\u7edc,\u4f46\u5bf9\u4e8e\u5206\u7c7b\u6548\u679c\u63d0\u5347\u6709\u9650\uff0c\u4f46\u5728\u901f\u5ea6\u4e0a\u6709\u63d0\u5347\u3002\u611f\u5174\u8da3\u7684\u540c\u5b66\u53ef\u4ee5\u53bb\u770b\u539f\u6587\uff0c\u7b80\u5355\u7406\u89e3\u8be5\u8bba\u6587\u8d21\u732e\uff0c\u5c31\u662f\u5c06\u7279\u5f81\u5c42\u6cbf\u7740\u901a\u9053\u7ef4\u5ea6\u5207\u6210\u4e24\u7247\uff0c\u4e24\u7247\u5206\u522b\u505a\u4e0d\u540c\u7684\u5377\u79ef\uff0c\u7136\u540e\u518d\u62fc\u63a5\u8d77\u6765\uff0c\u8fd9\u6837\u505a\u76f8\u6bd4\u4e8e\u76f4\u63a5\u5bf9\u539f\u56fe\u505a\u7279\u5f81\u63d0\u53d6\uff0c\u80fd\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002 \u9ed8\u8ba4\u770b\u8fc7\u6211\u7684libtorch\u7cfb\u5217\u6559\u7a0b\u7684\u524d\u90e8\u5206,\u76f4\u63a5\u4e0a\u4ee3\u7801\u3002\u9996\u5148\u662f\u57fa\u672c\u5355\u5143\uff0c\u7531Conv2d + BatchNorm2d + LeakyReLU\u6784\u6210\u3002 //Conv2d + BatchNorm2d + LeakyReLU class BasicConvImpl : public torch::nn::Module { public: BasicConvImpl(int in_channels, int out_channels, int kernel_size, int stride = 1); torch::Tensor forward(torch::Tensor x); private: // Declare layers torch::nn::Conv2d conv{ nullptr }; torch::nn::BatchNorm2d bn{ nullptr }; torch::nn::LeakyReLU acitivation{ nullptr }; }; TORCH_MODULE(BasicConv); BasicConvImpl::BasicConvImpl(int in_channels, int out_channels, int kernel_size, int stride) : conv(conv_options(in_channels, out_channels, kernel_size, stride, int(kernel_size / 2), 1, false)), bn(torch::nn::BatchNorm2d(out_channels)), acitivation(torch::nn::LeakyReLU(torch::nn::LeakyReLUOptions().negative_slope(0.1))) { register_module(\"conv\", conv); register_module(\"bn\", bn); } torch::Tensor BasicConvImpl::forward(torch::Tensor x) { x = conv->forward(x); x = bn->forward(x); x = acitivation(x); return x; } \u8be5\u5c42\u4f5c\u4e3a\u57fa\u672c\u6a21\u5757\uff0c\u5c06\u5728\u540e\u671f\u4f5c\u4e3a\u642d\u79ef\u6728\u7684\u57fa\u672c\u5757\uff0c\u642d\u5efayolo4_tiny\u3002 \u7136\u540e\u662fResblock_body\u6a21\u5757\uff0c class Resblock_bodyImpl : public torch::nn::Module { public: Resblock_bodyImpl(int in_channels, int out_channels); std::vector<torch::Tensor> forward(torch::Tensor x); private: int out_channels; BasicConv conv1{ nullptr }; BasicConv conv2{ nullptr }; BasicConv conv3{ nullptr }; BasicConv conv4{ nullptr }; torch::nn::MaxPool2d maxpool{ nullptr }; }; TORCH_MODULE(Resblock_body); Resblock_bodyImpl::Resblock_bodyImpl(int in_channels, int out_channels) { this->out_channels = out_channels; conv1 = BasicConv(in_channels, out_channels, 3); conv2 = BasicConv(out_channels / 2, out_channels / 2, 3); conv3 = BasicConv(out_channels / 2, out_channels / 2, 3); conv4 = BasicConv(out_channels, out_channels, 1); maxpool = torch::nn::MaxPool2d(maxpool_options(2, 2)); register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); register_module(\"conv3\", conv3); register_module(\"conv4\", conv4); } std::vector<torch::Tensor> Resblock_bodyImpl::forward(torch::Tensor x) { auto c = out_channels; x = conv1->forward(x); auto route = x; x = torch::split(x, c / 2, 1)[1]; x = conv2->forward(x); auto route1 = x; x = conv3->forward(x); x = torch::cat({ x, route1 }, 1); x = conv4->forward(x); auto feat = x; x = torch::cat({ route, x }, 1); x = maxpool->forward(x); return std::vector<torch::Tensor>({ x,feat }); } \u6700\u540e\u662f\u9aa8\u5e72\u7f51\u7edc\u4e3b\u4f53 class CSPdarknet53_tinyImpl : public torch::nn::Module { public: CSPdarknet53_tinyImpl(); std::vector<torch::Tensor> forward(torch::Tensor x); private: BasicConv conv1{ nullptr }; BasicConv conv2{ nullptr }; Resblock_body resblock_body1{ nullptr }; Resblock_body resblock_body2{ nullptr }; Resblock_body resblock_body3{ nullptr }; BasicConv conv3{ nullptr }; int num_features = 1; }; TORCH_MODULE(CSPdarknet53_tiny); CSPdarknet53_tinyImpl::CSPdarknet53_tinyImpl() { conv1 = BasicConv(3, 32, 3, 2); conv2 = BasicConv(32, 64, 3, 2); resblock_body1 = Resblock_body(64, 64); resblock_body2 = Resblock_body(128, 128); resblock_body3 = Resblock_body(256, 256); conv3 = BasicConv(512, 512, 3); register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); register_module(\"resblock_body1\", resblock_body1); register_module(\"resblock_body2\", resblock_body2); register_module(\"resblock_body3\", resblock_body3); register_module(\"conv3\", conv3); } std::vector<torch::Tensor> CSPdarknet53_tinyImpl::forward(torch::Tensor x) { // 416, 416, 3 -> 208, 208, 32 -> 104, 104, 64 x = conv1(x); x = conv2(x); // 104, 104, 64 -> 52, 52, 128 x = resblock_body1->forward(x)[0]; // 52, 52, 128 -> 26, 26, 256 x = resblock_body2->forward(x)[0]; // 26, 26, 256->x\u03aa13, 13, 512 # // -> feat1\u03aa26,26,256 auto res_out = resblock_body3->forward(x); x = res_out[0]; auto feat1 = res_out[1]; // 13, 13, 512 -> 13, 13, 512 x = conv3->forward(x); auto feat2 = x; return std::vector<torch::Tensor>({ feat1, feat2 }); } \u81f3\u6b64\uff0cyolo4_tiny\u4e2d\u7684\u9aa8\u5e72\u7f51\u7edc\u5df2\u7ecf\u642d\u5efa\u597d\u3002\u63a5\u4e0b\u6765\u5c06\u642d\u5efayolo4_tiny\u6a21\u578b\u3002 3.yolov4_tiny \u9aa8\u5e72\u7f51\u7edc\u5f97\u5230\u7684\u7279\u5f81\u56fe\uff0c\u5c06\u7ecf\u8fc7FPN\uff0c\u9700\u8981\u4e0a\u91c7\u6837\u6a21\u5757\u3002 //conv+upsample class UpsampleImpl : public torch::nn::Module { public: UpsampleImpl(int in_channels, int out_channels); torch::Tensor forward(torch::Tensor x); private: // Declare layers torch::nn::Sequential upsample = torch::nn::Sequential(); }; TORCH_MODULE(Upsample); UpsampleImpl::UpsampleImpl(int in_channels, int out_channels) { upsample = torch::nn::Sequential( BasicConv(in_channels, out_channels, 1) //torch::nn::Upsample(torch::nn::UpsampleOptions().scale_factor(std::vector<double>({ 2 })).mode(torch::kNearest).align_corners(false)) ); register_module(\"upsample\", upsample); } torch::Tensor UpsampleImpl::forward(torch::Tensor x) { x = upsample->forward(x); x = at::upsample_nearest2d(x, { x.sizes()[2] * 2 , x.sizes()[3] * 2 }); return x; } \u7136\u540e\u662fyolo_head\u6a21\u5757 torch::nn::Sequential yolo_head(std::vector<int> filters_list, int in_filters); torch::nn::Sequential yolo_head(std::vector<int> filters_list, int in_filters) { auto m = torch::nn::Sequential(BasicConv(in_filters, filters_list[0], 3), torch::nn::Conv2d(conv_options(filters_list[0], filters_list[1], 1))); return m; } \u4ee5\u53cayolo_body class YoloBody_tinyImpl : public torch::nn::Module { public: YoloBody_tinyImpl(int num_anchors, int num_classes); std::vector<torch::Tensor> forward(torch::Tensor x); private: // Declare layers CSPdarknet53_tiny backbone{ nullptr }; BasicConv conv_for_P5{ nullptr }; Upsample upsample{ nullptr }; torch::nn::Sequential yolo_headP5{ nullptr }; torch::nn::Sequential yolo_headP4{ nullptr }; }; TORCH_MODULE(YoloBody_tiny); YoloBody_tinyImpl::YoloBody_tinyImpl(int num_anchors, int num_classes) { backbone = CSPdarknet53_tiny(); conv_for_P5 = BasicConv(512, 256, 1); yolo_headP5 = yolo_head({ 512, num_anchors * (5 + num_classes) }, 256); upsample = Upsample(256, 128); yolo_headP4 = yolo_head({ 256, num_anchors * (5 + num_classes) }, 384); register_module(\"backbone\", backbone); register_module(\"conv_for_P5\", conv_for_P5); register_module(\"yolo_headP5\", yolo_headP5); register_module(\"upsample\", upsample); register_module(\"yolo_headP4\", yolo_headP4); } std::vector<torch::Tensor> YoloBody_tinyImpl::forward(torch::Tensor x) { //return feat1 with shape of {26,26,256} and feat2 of {13, 13, 512} auto backbone_out = backbone->forward(x); auto feat1 = backbone_out[0]; auto feat2 = backbone_out[1]; //13,13,512 -> 13,13,256 auto P5 = conv_for_P5->forward(feat2); //13, 13, 256 -> 13, 13, 512 -> 13, 13, 255 auto out0 = yolo_headP5->forward(P5); //13,13,256 -> 13,13,128 -> 26,26,128 auto P5_Upsample = upsample->forward(P5); //26, 26, 256 + 26, 26, 128 -> 26, 26, 384 auto P4 = torch::cat({ P5_Upsample, feat1 }, 1); //26, 26, 384 -> 26, 26, 256 -> 26, 26, 255 auto out1 = yolo_headP4->forward(P4); return std::vector<torch::Tensor>({ out0, out1 }); } \u4ee3\u7801\u5199\u5230\u8fd9\u4e00\u6b65\uff0c\u5176\u5b9e\u53ea\u8981\u7ec6\u5fc3\u5c31\u4f1a\u53d1\u73b0\u57fa\u672c\u662f\u5bf9pytorch\u4ee3\u7801\u5230libtorch\u7684\u8fc1\u79fb\uff0c\u9664\u4e86\u5c11\u6570bug\u9700\u8981\u8c03\u8bd5\uff0c\u5927\u90e8\u5206\u7b80\u5355\u8fc1\u79fb\u5230c++\u5373\u53ef\u3002\u53ef\u4ee5\u8bf4\u662f\u975e\u5e38\u7b80\u4fbf\u4e86\u3002 \u50cf\u524d\u9762\u7ae0\u8282\u4e2d\u4e00\u6837\uff0c\u751f\u6210torchscript\u6a21\u578b\u3002bubbliiiing yolov4-tiny\u4e2d\u6709\u63d0\u4f9b\u4e00\u4e2acoco\u8bad\u7ec3\u7248\u672c\uff0c\u901a\u8fc7\u4e0b\u8ff0\u4ee3\u7801\u751f\u6210.pt\u6587\u4ef6\uff1a import torch from torchsummary import summary import numpy as np from nets.yolo4_tiny import YoloBody from train import get_anchors, get_classes,YOLOLoss device = torch.device('cpu') model = YoloBody(3,80).to(device) model_path = \"model_data/yolov4_tiny_weights_coco.pth\" print('Loading weights into state dict...') model_dict = model.state_dict() pretrained_dict = torch.load(model_path, map_location=torch.device(\"cpu\")) pretrained_dict = {k: v for k, v in pretrained_dict.items() if np.shape(model_dict[k]) == np.shape(v)} model_dict.update(pretrained_dict) model.load_state_dict(model_dict) print('Finished!') #\u751f\u6210pt\u6a21\u578b\uff0c\u6309\u7167\u5b98\u7f51\u6765\u5373\u53ef model=model.to(torch.device(\"cpu\")) model.eval() var=torch.ones((1,3,416,416)) traced_script_module = torch.jit.trace(model, var) traced_script_module.save(\"yolo4_tiny.pt\") \u7136\u540e\u5728c++\u4e2d\u4f7f\u7528\u4e0b\u8ff0\u4ee3\u7801\u6d4b\u8bd5\u662f\u5426\u80fd\u591f\u6b63\u786e\u52a0\u8f7d: auto model = YoloBody_tiny(3, 80); torch::load(model, \"weights/yolo4_tiny.pt\"); \u6267\u884c\u901a\u8fc7\u5373\u8868\u660e\u52a0\u8f7d\u6210\u529f\u3002 4.\u9884\u6d4b \u9884\u6d4b\u9700\u8981\u5c06YOLO4_tiny\u6a21\u578b\u8f93\u51fa\u7684\u5f20\u91cf\u8fdb\u884c\u89e3\u7801\uff0c\u6839\u636e\u6e90\u4ee3\u7801\u89e3\u7801\u51fd\u6570\uff0c\u5199\u51fac++\u7248\u672c\u7684\u89e3\u7801\u51fd\u6570\uff0c\u6b64\u65f6\u5c06\u53d1\u73b0\uff0clibtorch\u6559\u7a0b\u7b2c\u4e8c\u7ae0\u7684\u91cd\u8981\u6027\u4e86\u3002 torch::Tensor DecodeBox(torch::Tensor input, torch::Tensor anchors, int num_classes, int img_size[]) { int num_anchors = anchors.sizes()[0]; int bbox_attrs = 5 + num_classes; int batch_size = input.sizes()[0]; int input_height = input.sizes()[2]; int input_width = input.sizes()[3]; //\u8ba1\u7b97\u6b65\u957f //\u6bcf\u4e00\u4e2a\u7279\u5f81\u70b9\u5bf9\u5e94\u539f\u6765\u7684\u56fe\u7247\u4e0a\u591a\u5c11\u4e2a\u50cf\u7d20\u70b9 //\u5982\u679c\u7279\u5f81\u5c42\u4e3a13x13\u7684\u8bdd\uff0c\u4e00\u4e2a\u7279\u5f81\u70b9\u5c31\u5bf9\u5e94\u539f\u6765\u7684\u56fe\u7247\u4e0a\u768432\u4e2a\u50cf\u7d20\u70b9 //416 / 13 = 32 auto stride_h = img_size[1] / input_height; auto stride_w = img_size[0] / input_width; //\u628a\u5148\u9a8c\u6846\u7684\u5c3a\u5bf8\u8c03\u6574\u6210\u7279\u5f81\u5c42\u5927\u5c0f\u7684\u5f62\u5f0f //\u8ba1\u7b97\u51fa\u5148\u9a8c\u6846\u5728\u7279\u5f81\u5c42\u4e0a\u5bf9\u5e94\u7684\u5bbd\u9ad8 auto scaled_anchors = anchors.clone(); scaled_anchors.select(1, 0) = scaled_anchors.select(1, 0) / stride_w; scaled_anchors.select(1, 1) = scaled_anchors.select(1, 1) / stride_h; //bs, 3 * (5 + num_classes), 13, 13->bs, 3, 13, 13, (5 + num_classes) //cout << \"begin view\"<<input.sizes()<<endl; auto prediction = input.view({ batch_size, num_anchors,bbox_attrs, input_height, input_width }).permute({ 0, 1, 3, 4, 2 }).contiguous(); //cout << \"end view\" << endl; //\u5148\u9a8c\u6846\u7684\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u8c03\u6574\u53c2\u6570 auto x = torch::sigmoid(prediction.select(-1, 0)); auto y = torch::sigmoid(prediction.select(-1, 1)); //\u5148\u9a8c\u6846\u7684\u5bbd\u9ad8\u8c03\u6574\u53c2\u6570 auto w = prediction.select(-1, 2); // Width auto h = prediction.select(-1, 3); // Height //\u83b7\u5f97\u7f6e\u4fe1\u5ea6\uff0c\u662f\u5426\u6709\u7269\u4f53 auto conf = torch::sigmoid(prediction.select(-1, 4)); //\u79cd\u7c7b\u7f6e\u4fe1\u5ea6 auto pred_cls = torch::sigmoid(prediction.narrow(-1, 5, num_classes));// Cls pred. auto LongType = x.clone().to(torch::kLong).options(); auto FloatType = x.options(); //\u751f\u6210\u7f51\u683c\uff0c\u5148\u9a8c\u6846\u4e2d\u5fc3\uff0c\u7f51\u683c\u5de6\u4e0a\u89d2 batch_size, 3, 13, 13 auto grid_x = torch::linspace(0, input_width - 1, input_width).repeat({ input_height, 1 }).repeat( { batch_size * num_anchors, 1, 1 }).view(x.sizes()).to(FloatType); auto grid_y = torch::linspace(0, input_height - 1, input_height).repeat({ input_width, 1 }).t().repeat( { batch_size * num_anchors, 1, 1 }).view(y.sizes()).to(FloatType); //\u751f\u6210\u5148\u9a8c\u6846\u7684\u5bbd\u9ad8 auto anchor_w = scaled_anchors.to(FloatType).narrow(1, 0, 1); auto anchor_h = scaled_anchors.to(FloatType).narrow(1, 1, 1); anchor_w = anchor_w.repeat({ batch_size, 1 }).repeat({ 1, 1, input_height * input_width }).view(w.sizes()); anchor_h = anchor_h.repeat({ batch_size, 1 }).repeat({ 1, 1, input_height * input_width }).view(h.sizes()); //\u8ba1\u7b97\u8c03\u6574\u540e\u7684\u5148\u9a8c\u6846\u4e2d\u5fc3\u4e0e\u5bbd\u9ad8 auto pred_boxes = torch::randn_like(prediction.narrow(-1, 0, 4)).to(FloatType); pred_boxes.select(-1, 0) = x + grid_x; pred_boxes.select(-1, 1) = y + grid_y; pred_boxes.select(-1, 2) = w.exp() * anchor_w; pred_boxes.select(-1, 3) = h.exp() * anchor_h; //\u7528\u4e8e\u5c06\u8f93\u51fa\u8c03\u6574\u4e3a\u76f8\u5bf9\u4e8e416x416\u7684\u5927\u5c0f std::vector<int> scales{ stride_w, stride_h, stride_w, stride_h }; auto _scale = torch::tensor(scales).to(FloatType); //cout << pred_boxes << endl; //cout << conf << endl; //cout << pred_cls << endl; pred_boxes = pred_boxes.view({ batch_size, -1, 4 }) * _scale; conf = conf.view({ batch_size, -1, 1 }); pred_cls = pred_cls.view({ batch_size, -1, num_classes }); auto output = torch::cat({ pred_boxes, conf, pred_cls }, -1); return output; } \u6b64\u5916\uff0c\u8fd8\u9700\u8981\u5c06\u8f93\u51fa\u8fdb\u884c\u975e\u6781\u5927\u503c\u6291\u5236\u3002\u53c2\u8003\u6211\u7684 NMS\u7684\u51e0\u79cd\u5199\u6cd5 \u5199\u51fa\u975e\u6781\u5927\u503c\u6291\u5236\u51fd\u6570\uff1a std::vector<int> nms_libtorch(torch::Tensor bboxes, torch::Tensor scores, float thresh) { auto x1 = bboxes.select(-1, 0); auto y1 = bboxes.select(-1, 1); auto x2 = bboxes.select(-1, 2); auto y2 = bboxes.select(-1, 3); auto areas = (x2 - x1)*(y2 - y1); //[N, ] \u6bcf\u4e2abbox\u7684\u9762\u79ef auto tuple_sorted = scores.sort(0, true); //\u964d\u5e8f\u6392\u5217 auto order = std::get<1>(tuple_sorted); std::vector<int> keep; while (order.numel() > 0) {// torch.numel()\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u4e2a\u6570 if (order.numel() == 1) {// \u4fdd\u7559\u6846\u53ea\u5269\u4e00\u4e2a auto i = order.item(); keep.push_back(i.toInt()); break; } else { auto i = order[0].item();// \u4fdd\u7559scores\u6700\u5927\u7684\u90a3\u4e2a\u6846box[i] keep.push_back(i.toInt()); } //\u8ba1\u7b97box[i]\u4e0e\u5176\u4f59\u5404\u6846\u7684IOU(\u601d\u8def\u5f88\u597d) auto order_mask = order.narrow(0, 1, order.size(-1) - 1); x1.index({ order_mask }); x1.index({ order_mask }).clamp(x1[keep.back()].item().toFloat(), 1e10); auto xx1 = x1.index({ order_mask }).clamp(x1[keep.back()].item().toFloat(), 1e10);// [N - 1, ] auto yy1 = y1.index({ order_mask }).clamp(y1[keep.back()].item().toFloat(), 1e10); auto xx2 = x2.index({ order_mask }).clamp(0, x2[keep.back()].item().toFloat()); auto yy2 = y2.index({ order_mask }).clamp(0, y2[keep.back()].item().toFloat()); auto inter = (xx2 - xx1).clamp(0, 1e10) * (yy2 - yy1).clamp(0, 1e10);// [N - 1, ] auto iou = inter / (areas[keep.back()] + areas.index({ order.narrow(0,1,order.size(-1) - 1) }) - inter);//[N - 1, ] auto idx = (iou <= thresh).nonzero().squeeze();//\u6ce8\u610f\u6b64\u65f6idx\u4e3a[N - 1, ] \u800corder\u4e3a[N, ] if (idx.numel() == 0) { break; } order = order.index({ idx + 1 }); //\u4fee\u8865\u7d22\u5f15\u4e4b\u95f4\u7684\u5dee\u503c } return keep; } std::vector<torch::Tensor> non_maximum_suppression(torch::Tensor prediction, int num_classes, float conf_thres, float nms_thres) { prediction.select(-1, 0) -= prediction.select(-1, 2) / 2; prediction.select(-1, 1) -= prediction.select(-1, 3) / 2; prediction.select(-1, 2) += prediction.select(-1, 0); prediction.select(-1, 3) += prediction.select(-1, 1); std::vector<torch::Tensor> output; for (int image_id = 0; image_id < prediction.sizes()[0]; image_id++) { auto image_pred = prediction[image_id]; auto max_out_tuple = torch::max(image_pred.narrow(-1, 5, num_classes), -1, true); auto class_conf = std::get<0>(max_out_tuple); auto class_pred = std::get<1>(max_out_tuple); auto conf_mask = (image_pred.select(-1, 4) * class_conf.select(-1, 0) >= conf_thres).squeeze(); image_pred = image_pred.index({ conf_mask }).to(torch::kFloat); class_conf = class_conf.index({ conf_mask }).to(torch::kFloat); class_pred = class_pred.index({ conf_mask }).to(torch::kFloat); if (!image_pred.sizes()[0]) { output.push_back(torch::full({ 1, 7 }, 0)); continue; } //\u83b7\u5f97\u7684\u5185\u5bb9\u4e3a(x1, y1, x2, y2, obj_conf, class_conf, class_pred) auto detections = torch::cat({ image_pred.narrow(-1,0,5), class_conf, class_pred }, 1); //\u83b7\u5f97\u79cd\u7c7b std::vector<torch::Tensor> img_classes; for (int m = 0, len = detections.size(0); m < len; m++) { bool found = false; for (size_t n = 0; n < img_classes.size(); n++) { auto ret = (detections[m][6] == img_classes[n]); if (torch::nonzero(ret).size(0) > 0) { found = true; break; } } if (!found) img_classes.push_back(detections[m][6]); } std::vector<torch::Tensor> temp_class_detections; for (auto c : img_classes) { auto detections_class = detections.index({ detections.select(-1,-1) == c }); auto keep = nms_libtorch(detections_class.narrow(-1, 0, 4), detections_class.select(-1, 4)*detections_class.select(-1, 5), nms_thres); std::vector<torch::Tensor> temp_max_detections; for (auto v : keep) { temp_max_detections.push_back(detections_class[v]); } auto max_detections = torch::cat(temp_max_detections, 0); temp_class_detections.push_back(max_detections); } auto class_detections = torch::cat(temp_class_detections, 0); output.push_back(class_detections); } return output; } \u8fd9\u4e9b\u51fd\u6570\u51c6\u5907\u597d\u540e\uff0c\u5199\u51fa\u9884\u6d4b\u51fd\u6570\uff1a void show_bbox_coco(cv::Mat image, torch::Tensor bboxes, int nums) { //\u8bbe\u7f6e\u7ed8\u5236\u6587\u672c\u7684\u76f8\u5173\u53c2\u6570 int font_face = cv::FONT_HERSHEY_COMPLEX; double font_scale = 0.4; int thickness = 1; float* bbox = new float[bboxes.size(0)](); std::cout << bboxes << std::endl; memcpy(bbox, bboxes.cpu().data_ptr(), bboxes.size(0) * sizeof(float)); for (int i = 0; i < bboxes.size(0); i = i + 7) { cv::rectangle(image, cv::Rect(bbox[i + 0], bbox[i + 1], bbox[i + 2] - bbox[i + 0], bbox[i + 3] - bbox[i + 1]), cv::Scalar(0, 0, 255)); //\u5c06\u6587\u672c\u6846\u5c45\u4e2d\u7ed8\u5236 cv::Point origin; origin.x = bbox[i + 0]; origin.y = bbox[i + 1] + 8; cv::putText(image, std::to_string(int(bbox[i + 6])), origin, font_face, font_scale, cv::Scalar(0, 0, 255), thickness, 1, 0); } delete bbox; cv::imshow(\"test\", image); cv::waitKey(0); cv::destroyAllWindows(); } void Predict(YoloBody_tiny detector, cv::Mat image, bool show, float conf_thresh, float nms_thresh) { int origin_width = image.cols; int origin_height = image.rows; cv::resize(image, image, { 416,416 }); auto img_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte); img_tensor = img_tensor.permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat) / 255.0; float anchor[12] = { 10,14, 23,27, 37,58, 81,82, 135,169, 344,319 }; auto anchors_ = torch::from_blob(anchor, { 6,2 }, torch::TensorOptions(torch::kFloat32)); int image_size[2] = { 416,416 }; img_tensor = img_tensor.cuda(); auto outputs = detector->forward(img_tensor); std::vector<torch::Tensor> output_list = {}; auto tensor_input = outputs[1]; auto output_decoded = DecodeBox(tensor_input, anchors_.narrow(0, 0, 3), 80, image_size); output_list.push_back(output_decoded); tensor_input = outputs[0]; output_decoded = DecodeBox(tensor_input, anchors_.narrow(0, 3, 3), 80, image_size); output_list.push_back(output_decoded); //std::cout << tensor_input << anchors_.narrow(0, 3, 3); auto output = torch::cat(output_list, 1); auto detection = non_maximum_suppression(output, 80, conf_thresh, nms_thresh); float w_scale = float(origin_width) / 416; float h_scale = float(origin_height) / 416; for (int i = 0; i < detection.size(); i++) { for (int j = 0; j < detection[i].size(0) / 7; j++) { detection[i].select(0, 7 * j + 0) *= w_scale; detection[i].select(0, 7 * j + 1) *= h_scale; detection[i].select(0, 7 * j + 2) *= w_scale; detection[i].select(0, 7 * j + 3) *= h_scale; } } cv::resize(image, image, { origin_width,origin_height }); if (show) show_bbox_coco(image, detection[0], 80); return; } \u4f7f\u7528VOC\u6570\u636e\u96c6\u4e2d\u4e00\u5f20\u56fe\u7247\uff0c\u6d4b\u8bd5\u4e00\u4e0b\u51fd\u6570\u51c6\u786e\u6027\u3002\u76f4\u63a5\u5c06\u4e0a\u8ff0\u4ee3\u7801\u7528\u4e8e\u6d4b\u8bd5.pt\u6587\u4ef6\uff0c\u5982\u8f93\u5165\u4e0b\u8ff0\u4ee3\u7801\uff1a cv::Mat image = cv::imread(\"2007_005331.jpg\"); auto model = YoloBody_tiny(3, 80); torch::load(model, \"weights/yolo4_tiny.pt\"); model->to(torch::kCUDA); Predict(model, image, true, 0.1, 0.3); \u4f7f\u7528\u7684\u56fe\u7247\u5982\u4e0b\u56fe \u5c06\u4f1a\u53d1\u73b0\uff0c\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\uff1a \u7ed3\u679c\u5206\u6790\u6709\u4ee5\u4e0b\u4e24\u70b9\u7ed3\u8bba\uff1a \u8f93\u51fa\u4e86\u68c0\u6d4b\u6846\uff0c\u9884\u6d4b\u51fd\u6570\u5927\u6982\u7387\u6b63\u786e\uff1b \u5b58\u5728\u90e8\u5206\u8bef\u68c0\uff0c\u63d0\u9ad8\u7f6e\u4fe1\u5ea6\u9608\u503c\u53ef\u80fd\u6539\u5584\uff0c\u4f46\u662f\u4f1a\u6f0f\u68c0\u3002\u8fd9\u662f\u7531\u4e8e.pt\u6587\u4ef6\u8bad\u7ec3\u65f6\u91c7\u7528\u7684\u9884\u5904\u7406\u7b56\u7565\uff0c\u548c\u672c\u6587\u4ee3\u7801\u9884\u6d4b\u65f6\u91c7\u7528\u7684\u9884\u5904\u7406\u7b56\u7565\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u3002 \u4f7f\u7528\u8bad\u7ec3\u548c\u9884\u6d4b\u4e00\u81f4\u7684\u9884\u5904\u7406\u65b9\u5f0f\u5904\u7406\u56fe\u7247\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u5e94\u8be5\u8981\u597d\u5f88\u591a\u3002\u4e0b\u9762\u65f6\u4e00\u5f20\uff0c\u4ee5coco\u9884\u8bad\u7ec3\u6743\u91cd\u505a\u8fc1\u79fb\u5b66\u4e60\uff0c\u53ea\u8bad\u7ec3yolo_head\uff0c\u8bad\u7ec3voc\u6570\u636e\u96c6\u4e00\u4e2a\u5468\u671f\u540e\uff0c\u9884\u6d4b\u8be5\u56fe\u7684\u6548\u679c\uff1a \u7ee7\u7eed\u8bad\u7ec3\uff0c\u6570\u636e\u589e\u5f3a\uff0c\u8bad\u7ec3\u5168\u90e8\u6743\u91cd\u5e94\u8be5\u53ef\u4ee5\u5c06\u7ed3\u679c\u63d0\u5347\u66f4\u591a\u3002 5.\u8bad\u7ec3 \u8bad\u7ec3\u4ee3\u7801\u6bd4\u8f83\u591a\uff0c\u535a\u5ba2\u5c31\u4e0d\u518d\u4ecb\u7ecd\u3002\u53ef\u4ee5\u79fb\u6b65\u5230 LibtorchTutorials \u4e2d\u3002\u540c\u65f6\uff0cLibtorchTutorials\u4e2d\u7684\u4ee3\u7801\u5b9e\u73b0\u7684\u529f\u80fd\u90fd\u6bd4\u8f83\u57fa\u7840\uff0c\u6211\u5c06\u5206\u5f00\u5728LibtorchSegment\u9879\u76ee\u548cLibtorchDetection\u4e2d\u5c06\u529f\u80fd\u63d0\u5347\u5b8c\u5584\u3002\u6709\u5e2e\u52a9\u5230\u7684\u8bdd\u8bf7\u70b9\u4e2astar\u8d44\u74f7\u4e0b\u3002","title":"\u7b2c\u4e03\u7ae0 \u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b"},{"location":"chapter7/#_1","text":"\u9605\u8bfb\u672c\u6587\u9700\u8981\u6709\u57fa\u7840\u7684pytorch\u7f16\u7a0b\u7ecf\u9a8c\uff0c\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u76f8\u5173\u77e5\u8bc6\uff0c\u4e0d\u7528\u5f88\u6df1\u5165\uff0c\u5927\u81f4\u4e86\u89e3\u6982\u5ff5\u5373\u53ef\u3002 \u672c\u7ae0\u7b80\u8981\u4ecb\u7ecd\u5982\u4f55\u5982\u4f55\u7528C++\u5b9e\u73b0\u4e00\u4e2a\u76ee\u6807\u68c0\u6d4b\u5668\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u8bad\u7ec3\u548c\u9884\u6d4b\u7684\u529f\u80fd\u3002\u672c\u6587\u7684\u5206\u5272\u6a21\u578b\u67b6\u6784\u4f7f\u7528yolov4-tiny\u7ed3\u6784\uff0c\u4ee3\u7801\u7ed3\u6784\u53c2\u8003\u4e86 bubbliiiing yolov4-tiny \uff0c\u672c\u6587\u5206\u4eab\u7684c++\u6a21\u578b\u51e0\u4e4e\u5b8c\u7f8e\u590d\u73b0\u4e86pytorch\u7684\u7248\u672c\uff0c\u4e14\u5177\u6709\u901f\u5ea6\u4f18\u52bf\uff0c30-40%\u7684\u901f\u5ea6\u63d0\u5347\u3002","title":"\u7b2c\u4e03\u7ae0 \u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u642d\u5efa\uff0c\u8bad\u7ec3\uff0c\u9884\u6d4b"},{"location":"chapter7/#1","text":"\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0byolov4-tiny\u6a21\u578b\u3002yolov4-tiny\u6a21\u578b\u662fYOLO(you only look once)\u7cfb\u5217\u6a21\u578b\u4e2d\uff0cversion 4\u7684\u8f7b\u5de7\u7248\uff0c\u76f8\u6bd4\u4e8eyolov4\uff0c\u5b83\u727a\u7272\u4e86\u90e8\u5206\u7cbe\u5ea6\u4ee5\u5b9e\u73b0\u901f\u5ea6\u4e0a\u7684\u5927\u5e45\u63d0\u5347\u3002yolov4_tiny\u6a21\u578b\u7ed3\u6784\u5982\u56fe\uff08\u56fe\u7247\u6765\u6e90\u81ea\u8fd9\uff09\uff1a \u53ef\u4ee5\u53d1\u73b0\u6a21\u578b\u7ed3\u6784\u975e\u5e38\u7b80\u5355\uff0c\u4ee5CSPDarknet53-tiny\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0cFPN\u4e3a\u9888\u90e8(neck)\uff0cYolo head\u4e3a\u5934\u90e8\u3002\u6700\u540e\u8f93\u51fa\u4e24\u4e2a\u7279\u5f81\u5c42\uff0c\u5206\u522b\u662f\u539f\u56fe\u4e0b\u91c7\u683732\u500d\u548c\u4e0b\u91c7\u683716\u500d\u7684\u7279\u5f81\u56fe\u3002\u8bad\u7ec3\u65f6\uff0c\u4ee5\u8fd9\u4e24\u4e2a\u7279\u5f81\u56fe\u5206\u522b\u8f93\u5165\u635f\u5931\u8ba1\u7b97\u4e2d\u8ba1\u7b97\u635f\u5931\uff0c\u518d\u5c06\u635f\u5931\u6c42\u548c(\u6216\u5e73\u5747\uff0c\u600e\u4e48\u90fd\u597d)\uff0c\u540e\u505a\u53cd\u5411\u4f20\u64ad\uff0c\u9884\u6d4b\u65f6\u5c06\u4e24\u4e2a\u7279\u5f81\u56fe\u89e3\u7801\u51fa\u7684\u7ed3\u679c\u505a\u5e76\u96c6\u518d\u505aNMS(\u975e\u6781\u5927\u503c\u6291\u5236)\u3002","title":"1.\u6a21\u578b\u7b80\u4ecb"},{"location":"chapter7/#2","text":"CSPDarknet53-tiny\u662f CSPNet \u7684\u4e00\u79cd\uff0cCSPNet\u53d1\u8868\u4e8eCVPR2019\uff0c\u662f\u7528\u4e8e\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u68c0\u6d4b\u6027\u80fd\u7684\u4e00\u79cd\u9aa8\u5e72\u7f51\u7edc,\u4f46\u5bf9\u4e8e\u5206\u7c7b\u6548\u679c\u63d0\u5347\u6709\u9650\uff0c\u4f46\u5728\u901f\u5ea6\u4e0a\u6709\u63d0\u5347\u3002\u611f\u5174\u8da3\u7684\u540c\u5b66\u53ef\u4ee5\u53bb\u770b\u539f\u6587\uff0c\u7b80\u5355\u7406\u89e3\u8be5\u8bba\u6587\u8d21\u732e\uff0c\u5c31\u662f\u5c06\u7279\u5f81\u5c42\u6cbf\u7740\u901a\u9053\u7ef4\u5ea6\u5207\u6210\u4e24\u7247\uff0c\u4e24\u7247\u5206\u522b\u505a\u4e0d\u540c\u7684\u5377\u79ef\uff0c\u7136\u540e\u518d\u62fc\u63a5\u8d77\u6765\uff0c\u8fd9\u6837\u505a\u76f8\u6bd4\u4e8e\u76f4\u63a5\u5bf9\u539f\u56fe\u505a\u7279\u5f81\u63d0\u53d6\uff0c\u80fd\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002 \u9ed8\u8ba4\u770b\u8fc7\u6211\u7684libtorch\u7cfb\u5217\u6559\u7a0b\u7684\u524d\u90e8\u5206,\u76f4\u63a5\u4e0a\u4ee3\u7801\u3002\u9996\u5148\u662f\u57fa\u672c\u5355\u5143\uff0c\u7531Conv2d + BatchNorm2d + LeakyReLU\u6784\u6210\u3002 //Conv2d + BatchNorm2d + LeakyReLU class BasicConvImpl : public torch::nn::Module { public: BasicConvImpl(int in_channels, int out_channels, int kernel_size, int stride = 1); torch::Tensor forward(torch::Tensor x); private: // Declare layers torch::nn::Conv2d conv{ nullptr }; torch::nn::BatchNorm2d bn{ nullptr }; torch::nn::LeakyReLU acitivation{ nullptr }; }; TORCH_MODULE(BasicConv); BasicConvImpl::BasicConvImpl(int in_channels, int out_channels, int kernel_size, int stride) : conv(conv_options(in_channels, out_channels, kernel_size, stride, int(kernel_size / 2), 1, false)), bn(torch::nn::BatchNorm2d(out_channels)), acitivation(torch::nn::LeakyReLU(torch::nn::LeakyReLUOptions().negative_slope(0.1))) { register_module(\"conv\", conv); register_module(\"bn\", bn); } torch::Tensor BasicConvImpl::forward(torch::Tensor x) { x = conv->forward(x); x = bn->forward(x); x = acitivation(x); return x; } \u8be5\u5c42\u4f5c\u4e3a\u57fa\u672c\u6a21\u5757\uff0c\u5c06\u5728\u540e\u671f\u4f5c\u4e3a\u642d\u79ef\u6728\u7684\u57fa\u672c\u5757\uff0c\u642d\u5efayolo4_tiny\u3002 \u7136\u540e\u662fResblock_body\u6a21\u5757\uff0c class Resblock_bodyImpl : public torch::nn::Module { public: Resblock_bodyImpl(int in_channels, int out_channels); std::vector<torch::Tensor> forward(torch::Tensor x); private: int out_channels; BasicConv conv1{ nullptr }; BasicConv conv2{ nullptr }; BasicConv conv3{ nullptr }; BasicConv conv4{ nullptr }; torch::nn::MaxPool2d maxpool{ nullptr }; }; TORCH_MODULE(Resblock_body); Resblock_bodyImpl::Resblock_bodyImpl(int in_channels, int out_channels) { this->out_channels = out_channels; conv1 = BasicConv(in_channels, out_channels, 3); conv2 = BasicConv(out_channels / 2, out_channels / 2, 3); conv3 = BasicConv(out_channels / 2, out_channels / 2, 3); conv4 = BasicConv(out_channels, out_channels, 1); maxpool = torch::nn::MaxPool2d(maxpool_options(2, 2)); register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); register_module(\"conv3\", conv3); register_module(\"conv4\", conv4); } std::vector<torch::Tensor> Resblock_bodyImpl::forward(torch::Tensor x) { auto c = out_channels; x = conv1->forward(x); auto route = x; x = torch::split(x, c / 2, 1)[1]; x = conv2->forward(x); auto route1 = x; x = conv3->forward(x); x = torch::cat({ x, route1 }, 1); x = conv4->forward(x); auto feat = x; x = torch::cat({ route, x }, 1); x = maxpool->forward(x); return std::vector<torch::Tensor>({ x,feat }); } \u6700\u540e\u662f\u9aa8\u5e72\u7f51\u7edc\u4e3b\u4f53 class CSPdarknet53_tinyImpl : public torch::nn::Module { public: CSPdarknet53_tinyImpl(); std::vector<torch::Tensor> forward(torch::Tensor x); private: BasicConv conv1{ nullptr }; BasicConv conv2{ nullptr }; Resblock_body resblock_body1{ nullptr }; Resblock_body resblock_body2{ nullptr }; Resblock_body resblock_body3{ nullptr }; BasicConv conv3{ nullptr }; int num_features = 1; }; TORCH_MODULE(CSPdarknet53_tiny); CSPdarknet53_tinyImpl::CSPdarknet53_tinyImpl() { conv1 = BasicConv(3, 32, 3, 2); conv2 = BasicConv(32, 64, 3, 2); resblock_body1 = Resblock_body(64, 64); resblock_body2 = Resblock_body(128, 128); resblock_body3 = Resblock_body(256, 256); conv3 = BasicConv(512, 512, 3); register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); register_module(\"resblock_body1\", resblock_body1); register_module(\"resblock_body2\", resblock_body2); register_module(\"resblock_body3\", resblock_body3); register_module(\"conv3\", conv3); } std::vector<torch::Tensor> CSPdarknet53_tinyImpl::forward(torch::Tensor x) { // 416, 416, 3 -> 208, 208, 32 -> 104, 104, 64 x = conv1(x); x = conv2(x); // 104, 104, 64 -> 52, 52, 128 x = resblock_body1->forward(x)[0]; // 52, 52, 128 -> 26, 26, 256 x = resblock_body2->forward(x)[0]; // 26, 26, 256->x\u03aa13, 13, 512 # // -> feat1\u03aa26,26,256 auto res_out = resblock_body3->forward(x); x = res_out[0]; auto feat1 = res_out[1]; // 13, 13, 512 -> 13, 13, 512 x = conv3->forward(x); auto feat2 = x; return std::vector<torch::Tensor>({ feat1, feat2 }); } \u81f3\u6b64\uff0cyolo4_tiny\u4e2d\u7684\u9aa8\u5e72\u7f51\u7edc\u5df2\u7ecf\u642d\u5efa\u597d\u3002\u63a5\u4e0b\u6765\u5c06\u642d\u5efayolo4_tiny\u6a21\u578b\u3002","title":"2.\u9aa8\u5e72\u7f51\u7edc"},{"location":"chapter7/#3yolov4_tiny","text":"\u9aa8\u5e72\u7f51\u7edc\u5f97\u5230\u7684\u7279\u5f81\u56fe\uff0c\u5c06\u7ecf\u8fc7FPN\uff0c\u9700\u8981\u4e0a\u91c7\u6837\u6a21\u5757\u3002 //conv+upsample class UpsampleImpl : public torch::nn::Module { public: UpsampleImpl(int in_channels, int out_channels); torch::Tensor forward(torch::Tensor x); private: // Declare layers torch::nn::Sequential upsample = torch::nn::Sequential(); }; TORCH_MODULE(Upsample); UpsampleImpl::UpsampleImpl(int in_channels, int out_channels) { upsample = torch::nn::Sequential( BasicConv(in_channels, out_channels, 1) //torch::nn::Upsample(torch::nn::UpsampleOptions().scale_factor(std::vector<double>({ 2 })).mode(torch::kNearest).align_corners(false)) ); register_module(\"upsample\", upsample); } torch::Tensor UpsampleImpl::forward(torch::Tensor x) { x = upsample->forward(x); x = at::upsample_nearest2d(x, { x.sizes()[2] * 2 , x.sizes()[3] * 2 }); return x; } \u7136\u540e\u662fyolo_head\u6a21\u5757 torch::nn::Sequential yolo_head(std::vector<int> filters_list, int in_filters); torch::nn::Sequential yolo_head(std::vector<int> filters_list, int in_filters) { auto m = torch::nn::Sequential(BasicConv(in_filters, filters_list[0], 3), torch::nn::Conv2d(conv_options(filters_list[0], filters_list[1], 1))); return m; } \u4ee5\u53cayolo_body class YoloBody_tinyImpl : public torch::nn::Module { public: YoloBody_tinyImpl(int num_anchors, int num_classes); std::vector<torch::Tensor> forward(torch::Tensor x); private: // Declare layers CSPdarknet53_tiny backbone{ nullptr }; BasicConv conv_for_P5{ nullptr }; Upsample upsample{ nullptr }; torch::nn::Sequential yolo_headP5{ nullptr }; torch::nn::Sequential yolo_headP4{ nullptr }; }; TORCH_MODULE(YoloBody_tiny); YoloBody_tinyImpl::YoloBody_tinyImpl(int num_anchors, int num_classes) { backbone = CSPdarknet53_tiny(); conv_for_P5 = BasicConv(512, 256, 1); yolo_headP5 = yolo_head({ 512, num_anchors * (5 + num_classes) }, 256); upsample = Upsample(256, 128); yolo_headP4 = yolo_head({ 256, num_anchors * (5 + num_classes) }, 384); register_module(\"backbone\", backbone); register_module(\"conv_for_P5\", conv_for_P5); register_module(\"yolo_headP5\", yolo_headP5); register_module(\"upsample\", upsample); register_module(\"yolo_headP4\", yolo_headP4); } std::vector<torch::Tensor> YoloBody_tinyImpl::forward(torch::Tensor x) { //return feat1 with shape of {26,26,256} and feat2 of {13, 13, 512} auto backbone_out = backbone->forward(x); auto feat1 = backbone_out[0]; auto feat2 = backbone_out[1]; //13,13,512 -> 13,13,256 auto P5 = conv_for_P5->forward(feat2); //13, 13, 256 -> 13, 13, 512 -> 13, 13, 255 auto out0 = yolo_headP5->forward(P5); //13,13,256 -> 13,13,128 -> 26,26,128 auto P5_Upsample = upsample->forward(P5); //26, 26, 256 + 26, 26, 128 -> 26, 26, 384 auto P4 = torch::cat({ P5_Upsample, feat1 }, 1); //26, 26, 384 -> 26, 26, 256 -> 26, 26, 255 auto out1 = yolo_headP4->forward(P4); return std::vector<torch::Tensor>({ out0, out1 }); } \u4ee3\u7801\u5199\u5230\u8fd9\u4e00\u6b65\uff0c\u5176\u5b9e\u53ea\u8981\u7ec6\u5fc3\u5c31\u4f1a\u53d1\u73b0\u57fa\u672c\u662f\u5bf9pytorch\u4ee3\u7801\u5230libtorch\u7684\u8fc1\u79fb\uff0c\u9664\u4e86\u5c11\u6570bug\u9700\u8981\u8c03\u8bd5\uff0c\u5927\u90e8\u5206\u7b80\u5355\u8fc1\u79fb\u5230c++\u5373\u53ef\u3002\u53ef\u4ee5\u8bf4\u662f\u975e\u5e38\u7b80\u4fbf\u4e86\u3002 \u50cf\u524d\u9762\u7ae0\u8282\u4e2d\u4e00\u6837\uff0c\u751f\u6210torchscript\u6a21\u578b\u3002bubbliiiing yolov4-tiny\u4e2d\u6709\u63d0\u4f9b\u4e00\u4e2acoco\u8bad\u7ec3\u7248\u672c\uff0c\u901a\u8fc7\u4e0b\u8ff0\u4ee3\u7801\u751f\u6210.pt\u6587\u4ef6\uff1a import torch from torchsummary import summary import numpy as np from nets.yolo4_tiny import YoloBody from train import get_anchors, get_classes,YOLOLoss device = torch.device('cpu') model = YoloBody(3,80).to(device) model_path = \"model_data/yolov4_tiny_weights_coco.pth\" print('Loading weights into state dict...') model_dict = model.state_dict() pretrained_dict = torch.load(model_path, map_location=torch.device(\"cpu\")) pretrained_dict = {k: v for k, v in pretrained_dict.items() if np.shape(model_dict[k]) == np.shape(v)} model_dict.update(pretrained_dict) model.load_state_dict(model_dict) print('Finished!') #\u751f\u6210pt\u6a21\u578b\uff0c\u6309\u7167\u5b98\u7f51\u6765\u5373\u53ef model=model.to(torch.device(\"cpu\")) model.eval() var=torch.ones((1,3,416,416)) traced_script_module = torch.jit.trace(model, var) traced_script_module.save(\"yolo4_tiny.pt\") \u7136\u540e\u5728c++\u4e2d\u4f7f\u7528\u4e0b\u8ff0\u4ee3\u7801\u6d4b\u8bd5\u662f\u5426\u80fd\u591f\u6b63\u786e\u52a0\u8f7d: auto model = YoloBody_tiny(3, 80); torch::load(model, \"weights/yolo4_tiny.pt\"); \u6267\u884c\u901a\u8fc7\u5373\u8868\u660e\u52a0\u8f7d\u6210\u529f\u3002","title":"3.yolov4_tiny"},{"location":"chapter7/#4","text":"\u9884\u6d4b\u9700\u8981\u5c06YOLO4_tiny\u6a21\u578b\u8f93\u51fa\u7684\u5f20\u91cf\u8fdb\u884c\u89e3\u7801\uff0c\u6839\u636e\u6e90\u4ee3\u7801\u89e3\u7801\u51fd\u6570\uff0c\u5199\u51fac++\u7248\u672c\u7684\u89e3\u7801\u51fd\u6570\uff0c\u6b64\u65f6\u5c06\u53d1\u73b0\uff0clibtorch\u6559\u7a0b\u7b2c\u4e8c\u7ae0\u7684\u91cd\u8981\u6027\u4e86\u3002 torch::Tensor DecodeBox(torch::Tensor input, torch::Tensor anchors, int num_classes, int img_size[]) { int num_anchors = anchors.sizes()[0]; int bbox_attrs = 5 + num_classes; int batch_size = input.sizes()[0]; int input_height = input.sizes()[2]; int input_width = input.sizes()[3]; //\u8ba1\u7b97\u6b65\u957f //\u6bcf\u4e00\u4e2a\u7279\u5f81\u70b9\u5bf9\u5e94\u539f\u6765\u7684\u56fe\u7247\u4e0a\u591a\u5c11\u4e2a\u50cf\u7d20\u70b9 //\u5982\u679c\u7279\u5f81\u5c42\u4e3a13x13\u7684\u8bdd\uff0c\u4e00\u4e2a\u7279\u5f81\u70b9\u5c31\u5bf9\u5e94\u539f\u6765\u7684\u56fe\u7247\u4e0a\u768432\u4e2a\u50cf\u7d20\u70b9 //416 / 13 = 32 auto stride_h = img_size[1] / input_height; auto stride_w = img_size[0] / input_width; //\u628a\u5148\u9a8c\u6846\u7684\u5c3a\u5bf8\u8c03\u6574\u6210\u7279\u5f81\u5c42\u5927\u5c0f\u7684\u5f62\u5f0f //\u8ba1\u7b97\u51fa\u5148\u9a8c\u6846\u5728\u7279\u5f81\u5c42\u4e0a\u5bf9\u5e94\u7684\u5bbd\u9ad8 auto scaled_anchors = anchors.clone(); scaled_anchors.select(1, 0) = scaled_anchors.select(1, 0) / stride_w; scaled_anchors.select(1, 1) = scaled_anchors.select(1, 1) / stride_h; //bs, 3 * (5 + num_classes), 13, 13->bs, 3, 13, 13, (5 + num_classes) //cout << \"begin view\"<<input.sizes()<<endl; auto prediction = input.view({ batch_size, num_anchors,bbox_attrs, input_height, input_width }).permute({ 0, 1, 3, 4, 2 }).contiguous(); //cout << \"end view\" << endl; //\u5148\u9a8c\u6846\u7684\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u8c03\u6574\u53c2\u6570 auto x = torch::sigmoid(prediction.select(-1, 0)); auto y = torch::sigmoid(prediction.select(-1, 1)); //\u5148\u9a8c\u6846\u7684\u5bbd\u9ad8\u8c03\u6574\u53c2\u6570 auto w = prediction.select(-1, 2); // Width auto h = prediction.select(-1, 3); // Height //\u83b7\u5f97\u7f6e\u4fe1\u5ea6\uff0c\u662f\u5426\u6709\u7269\u4f53 auto conf = torch::sigmoid(prediction.select(-1, 4)); //\u79cd\u7c7b\u7f6e\u4fe1\u5ea6 auto pred_cls = torch::sigmoid(prediction.narrow(-1, 5, num_classes));// Cls pred. auto LongType = x.clone().to(torch::kLong).options(); auto FloatType = x.options(); //\u751f\u6210\u7f51\u683c\uff0c\u5148\u9a8c\u6846\u4e2d\u5fc3\uff0c\u7f51\u683c\u5de6\u4e0a\u89d2 batch_size, 3, 13, 13 auto grid_x = torch::linspace(0, input_width - 1, input_width).repeat({ input_height, 1 }).repeat( { batch_size * num_anchors, 1, 1 }).view(x.sizes()).to(FloatType); auto grid_y = torch::linspace(0, input_height - 1, input_height).repeat({ input_width, 1 }).t().repeat( { batch_size * num_anchors, 1, 1 }).view(y.sizes()).to(FloatType); //\u751f\u6210\u5148\u9a8c\u6846\u7684\u5bbd\u9ad8 auto anchor_w = scaled_anchors.to(FloatType).narrow(1, 0, 1); auto anchor_h = scaled_anchors.to(FloatType).narrow(1, 1, 1); anchor_w = anchor_w.repeat({ batch_size, 1 }).repeat({ 1, 1, input_height * input_width }).view(w.sizes()); anchor_h = anchor_h.repeat({ batch_size, 1 }).repeat({ 1, 1, input_height * input_width }).view(h.sizes()); //\u8ba1\u7b97\u8c03\u6574\u540e\u7684\u5148\u9a8c\u6846\u4e2d\u5fc3\u4e0e\u5bbd\u9ad8 auto pred_boxes = torch::randn_like(prediction.narrow(-1, 0, 4)).to(FloatType); pred_boxes.select(-1, 0) = x + grid_x; pred_boxes.select(-1, 1) = y + grid_y; pred_boxes.select(-1, 2) = w.exp() * anchor_w; pred_boxes.select(-1, 3) = h.exp() * anchor_h; //\u7528\u4e8e\u5c06\u8f93\u51fa\u8c03\u6574\u4e3a\u76f8\u5bf9\u4e8e416x416\u7684\u5927\u5c0f std::vector<int> scales{ stride_w, stride_h, stride_w, stride_h }; auto _scale = torch::tensor(scales).to(FloatType); //cout << pred_boxes << endl; //cout << conf << endl; //cout << pred_cls << endl; pred_boxes = pred_boxes.view({ batch_size, -1, 4 }) * _scale; conf = conf.view({ batch_size, -1, 1 }); pred_cls = pred_cls.view({ batch_size, -1, num_classes }); auto output = torch::cat({ pred_boxes, conf, pred_cls }, -1); return output; } \u6b64\u5916\uff0c\u8fd8\u9700\u8981\u5c06\u8f93\u51fa\u8fdb\u884c\u975e\u6781\u5927\u503c\u6291\u5236\u3002\u53c2\u8003\u6211\u7684 NMS\u7684\u51e0\u79cd\u5199\u6cd5 \u5199\u51fa\u975e\u6781\u5927\u503c\u6291\u5236\u51fd\u6570\uff1a std::vector<int> nms_libtorch(torch::Tensor bboxes, torch::Tensor scores, float thresh) { auto x1 = bboxes.select(-1, 0); auto y1 = bboxes.select(-1, 1); auto x2 = bboxes.select(-1, 2); auto y2 = bboxes.select(-1, 3); auto areas = (x2 - x1)*(y2 - y1); //[N, ] \u6bcf\u4e2abbox\u7684\u9762\u79ef auto tuple_sorted = scores.sort(0, true); //\u964d\u5e8f\u6392\u5217 auto order = std::get<1>(tuple_sorted); std::vector<int> keep; while (order.numel() > 0) {// torch.numel()\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u4e2a\u6570 if (order.numel() == 1) {// \u4fdd\u7559\u6846\u53ea\u5269\u4e00\u4e2a auto i = order.item(); keep.push_back(i.toInt()); break; } else { auto i = order[0].item();// \u4fdd\u7559scores\u6700\u5927\u7684\u90a3\u4e2a\u6846box[i] keep.push_back(i.toInt()); } //\u8ba1\u7b97box[i]\u4e0e\u5176\u4f59\u5404\u6846\u7684IOU(\u601d\u8def\u5f88\u597d) auto order_mask = order.narrow(0, 1, order.size(-1) - 1); x1.index({ order_mask }); x1.index({ order_mask }).clamp(x1[keep.back()].item().toFloat(), 1e10); auto xx1 = x1.index({ order_mask }).clamp(x1[keep.back()].item().toFloat(), 1e10);// [N - 1, ] auto yy1 = y1.index({ order_mask }).clamp(y1[keep.back()].item().toFloat(), 1e10); auto xx2 = x2.index({ order_mask }).clamp(0, x2[keep.back()].item().toFloat()); auto yy2 = y2.index({ order_mask }).clamp(0, y2[keep.back()].item().toFloat()); auto inter = (xx2 - xx1).clamp(0, 1e10) * (yy2 - yy1).clamp(0, 1e10);// [N - 1, ] auto iou = inter / (areas[keep.back()] + areas.index({ order.narrow(0,1,order.size(-1) - 1) }) - inter);//[N - 1, ] auto idx = (iou <= thresh).nonzero().squeeze();//\u6ce8\u610f\u6b64\u65f6idx\u4e3a[N - 1, ] \u800corder\u4e3a[N, ] if (idx.numel() == 0) { break; } order = order.index({ idx + 1 }); //\u4fee\u8865\u7d22\u5f15\u4e4b\u95f4\u7684\u5dee\u503c } return keep; } std::vector<torch::Tensor> non_maximum_suppression(torch::Tensor prediction, int num_classes, float conf_thres, float nms_thres) { prediction.select(-1, 0) -= prediction.select(-1, 2) / 2; prediction.select(-1, 1) -= prediction.select(-1, 3) / 2; prediction.select(-1, 2) += prediction.select(-1, 0); prediction.select(-1, 3) += prediction.select(-1, 1); std::vector<torch::Tensor> output; for (int image_id = 0; image_id < prediction.sizes()[0]; image_id++) { auto image_pred = prediction[image_id]; auto max_out_tuple = torch::max(image_pred.narrow(-1, 5, num_classes), -1, true); auto class_conf = std::get<0>(max_out_tuple); auto class_pred = std::get<1>(max_out_tuple); auto conf_mask = (image_pred.select(-1, 4) * class_conf.select(-1, 0) >= conf_thres).squeeze(); image_pred = image_pred.index({ conf_mask }).to(torch::kFloat); class_conf = class_conf.index({ conf_mask }).to(torch::kFloat); class_pred = class_pred.index({ conf_mask }).to(torch::kFloat); if (!image_pred.sizes()[0]) { output.push_back(torch::full({ 1, 7 }, 0)); continue; } //\u83b7\u5f97\u7684\u5185\u5bb9\u4e3a(x1, y1, x2, y2, obj_conf, class_conf, class_pred) auto detections = torch::cat({ image_pred.narrow(-1,0,5), class_conf, class_pred }, 1); //\u83b7\u5f97\u79cd\u7c7b std::vector<torch::Tensor> img_classes; for (int m = 0, len = detections.size(0); m < len; m++) { bool found = false; for (size_t n = 0; n < img_classes.size(); n++) { auto ret = (detections[m][6] == img_classes[n]); if (torch::nonzero(ret).size(0) > 0) { found = true; break; } } if (!found) img_classes.push_back(detections[m][6]); } std::vector<torch::Tensor> temp_class_detections; for (auto c : img_classes) { auto detections_class = detections.index({ detections.select(-1,-1) == c }); auto keep = nms_libtorch(detections_class.narrow(-1, 0, 4), detections_class.select(-1, 4)*detections_class.select(-1, 5), nms_thres); std::vector<torch::Tensor> temp_max_detections; for (auto v : keep) { temp_max_detections.push_back(detections_class[v]); } auto max_detections = torch::cat(temp_max_detections, 0); temp_class_detections.push_back(max_detections); } auto class_detections = torch::cat(temp_class_detections, 0); output.push_back(class_detections); } return output; } \u8fd9\u4e9b\u51fd\u6570\u51c6\u5907\u597d\u540e\uff0c\u5199\u51fa\u9884\u6d4b\u51fd\u6570\uff1a void show_bbox_coco(cv::Mat image, torch::Tensor bboxes, int nums) { //\u8bbe\u7f6e\u7ed8\u5236\u6587\u672c\u7684\u76f8\u5173\u53c2\u6570 int font_face = cv::FONT_HERSHEY_COMPLEX; double font_scale = 0.4; int thickness = 1; float* bbox = new float[bboxes.size(0)](); std::cout << bboxes << std::endl; memcpy(bbox, bboxes.cpu().data_ptr(), bboxes.size(0) * sizeof(float)); for (int i = 0; i < bboxes.size(0); i = i + 7) { cv::rectangle(image, cv::Rect(bbox[i + 0], bbox[i + 1], bbox[i + 2] - bbox[i + 0], bbox[i + 3] - bbox[i + 1]), cv::Scalar(0, 0, 255)); //\u5c06\u6587\u672c\u6846\u5c45\u4e2d\u7ed8\u5236 cv::Point origin; origin.x = bbox[i + 0]; origin.y = bbox[i + 1] + 8; cv::putText(image, std::to_string(int(bbox[i + 6])), origin, font_face, font_scale, cv::Scalar(0, 0, 255), thickness, 1, 0); } delete bbox; cv::imshow(\"test\", image); cv::waitKey(0); cv::destroyAllWindows(); } void Predict(YoloBody_tiny detector, cv::Mat image, bool show, float conf_thresh, float nms_thresh) { int origin_width = image.cols; int origin_height = image.rows; cv::resize(image, image, { 416,416 }); auto img_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte); img_tensor = img_tensor.permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat) / 255.0; float anchor[12] = { 10,14, 23,27, 37,58, 81,82, 135,169, 344,319 }; auto anchors_ = torch::from_blob(anchor, { 6,2 }, torch::TensorOptions(torch::kFloat32)); int image_size[2] = { 416,416 }; img_tensor = img_tensor.cuda(); auto outputs = detector->forward(img_tensor); std::vector<torch::Tensor> output_list = {}; auto tensor_input = outputs[1]; auto output_decoded = DecodeBox(tensor_input, anchors_.narrow(0, 0, 3), 80, image_size); output_list.push_back(output_decoded); tensor_input = outputs[0]; output_decoded = DecodeBox(tensor_input, anchors_.narrow(0, 3, 3), 80, image_size); output_list.push_back(output_decoded); //std::cout << tensor_input << anchors_.narrow(0, 3, 3); auto output = torch::cat(output_list, 1); auto detection = non_maximum_suppression(output, 80, conf_thresh, nms_thresh); float w_scale = float(origin_width) / 416; float h_scale = float(origin_height) / 416; for (int i = 0; i < detection.size(); i++) { for (int j = 0; j < detection[i].size(0) / 7; j++) { detection[i].select(0, 7 * j + 0) *= w_scale; detection[i].select(0, 7 * j + 1) *= h_scale; detection[i].select(0, 7 * j + 2) *= w_scale; detection[i].select(0, 7 * j + 3) *= h_scale; } } cv::resize(image, image, { origin_width,origin_height }); if (show) show_bbox_coco(image, detection[0], 80); return; } \u4f7f\u7528VOC\u6570\u636e\u96c6\u4e2d\u4e00\u5f20\u56fe\u7247\uff0c\u6d4b\u8bd5\u4e00\u4e0b\u51fd\u6570\u51c6\u786e\u6027\u3002\u76f4\u63a5\u5c06\u4e0a\u8ff0\u4ee3\u7801\u7528\u4e8e\u6d4b\u8bd5.pt\u6587\u4ef6\uff0c\u5982\u8f93\u5165\u4e0b\u8ff0\u4ee3\u7801\uff1a cv::Mat image = cv::imread(\"2007_005331.jpg\"); auto model = YoloBody_tiny(3, 80); torch::load(model, \"weights/yolo4_tiny.pt\"); model->to(torch::kCUDA); Predict(model, image, true, 0.1, 0.3); \u4f7f\u7528\u7684\u56fe\u7247\u5982\u4e0b\u56fe \u5c06\u4f1a\u53d1\u73b0\uff0c\u9884\u6d4b\u7ed3\u679c\u5982\u4e0b\uff1a \u7ed3\u679c\u5206\u6790\u6709\u4ee5\u4e0b\u4e24\u70b9\u7ed3\u8bba\uff1a \u8f93\u51fa\u4e86\u68c0\u6d4b\u6846\uff0c\u9884\u6d4b\u51fd\u6570\u5927\u6982\u7387\u6b63\u786e\uff1b \u5b58\u5728\u90e8\u5206\u8bef\u68c0\uff0c\u63d0\u9ad8\u7f6e\u4fe1\u5ea6\u9608\u503c\u53ef\u80fd\u6539\u5584\uff0c\u4f46\u662f\u4f1a\u6f0f\u68c0\u3002\u8fd9\u662f\u7531\u4e8e.pt\u6587\u4ef6\u8bad\u7ec3\u65f6\u91c7\u7528\u7684\u9884\u5904\u7406\u7b56\u7565\uff0c\u548c\u672c\u6587\u4ee3\u7801\u9884\u6d4b\u65f6\u91c7\u7528\u7684\u9884\u5904\u7406\u7b56\u7565\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u3002 \u4f7f\u7528\u8bad\u7ec3\u548c\u9884\u6d4b\u4e00\u81f4\u7684\u9884\u5904\u7406\u65b9\u5f0f\u5904\u7406\u56fe\u7247\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u5e94\u8be5\u8981\u597d\u5f88\u591a\u3002\u4e0b\u9762\u65f6\u4e00\u5f20\uff0c\u4ee5coco\u9884\u8bad\u7ec3\u6743\u91cd\u505a\u8fc1\u79fb\u5b66\u4e60\uff0c\u53ea\u8bad\u7ec3yolo_head\uff0c\u8bad\u7ec3voc\u6570\u636e\u96c6\u4e00\u4e2a\u5468\u671f\u540e\uff0c\u9884\u6d4b\u8be5\u56fe\u7684\u6548\u679c\uff1a \u7ee7\u7eed\u8bad\u7ec3\uff0c\u6570\u636e\u589e\u5f3a\uff0c\u8bad\u7ec3\u5168\u90e8\u6743\u91cd\u5e94\u8be5\u53ef\u4ee5\u5c06\u7ed3\u679c\u63d0\u5347\u66f4\u591a\u3002","title":"4.\u9884\u6d4b"},{"location":"chapter7/#5","text":"\u8bad\u7ec3\u4ee3\u7801\u6bd4\u8f83\u591a\uff0c\u535a\u5ba2\u5c31\u4e0d\u518d\u4ecb\u7ecd\u3002\u53ef\u4ee5\u79fb\u6b65\u5230 LibtorchTutorials \u4e2d\u3002\u540c\u65f6\uff0cLibtorchTutorials\u4e2d\u7684\u4ee3\u7801\u5b9e\u73b0\u7684\u529f\u80fd\u90fd\u6bd4\u8f83\u57fa\u7840\uff0c\u6211\u5c06\u5206\u5f00\u5728LibtorchSegment\u9879\u76ee\u548cLibtorchDetection\u4e2d\u5c06\u529f\u80fd\u63d0\u5347\u5b8c\u5584\u3002\u6709\u5e2e\u52a9\u5230\u7684\u8bdd\u8bf7\u70b9\u4e2astar\u8d44\u74f7\u4e0b\u3002","title":"5.\u8bad\u7ec3"},{"location":"chapter8/","text":"\u7b2c\u4e5d\u7ae0 \u603b\u7ed3\u548c\u5c55\u671b \u9996\u9009\u8981\u8bf4\u660e\u7684\u662f\u8be5\u6559\u7a0b\u662f\u5bf9 Allent Dan\u5927\u4f6c\u5f00\u6e90\u7684libtorch\u6559\u7a0b\u7684\u4e00\u4e2a\u6574\u7406\uff0c\u5176\u5f00\u6e90\u4e86\u5927\u91cf\u7684libtorch\u5728C++\u4e0b\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u65b9\u6cd5\u3002\u5176GitHub\u5730\u5740\u4e3a\uff1a https://github.com/AllentDan \u5efa\u8bae\u5927\u5bb6\u770b\u5e76\u7ed9Star\uff1a https://github.com/AllentDan/LibtorchTutorials https://github.com/AllentDan/LibtorchDetection https://github.com/AllentDan/LibtorchSegmentation \u73b0\u5728\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u7684\u5c97\u4f4d\uff0c\u5c24\u5176\u662fCV\u5c97\uff0cpython\u7684HC\u771f\u7684\u4e0d\u591a\uff0c\u5927\u5382\u8fd8\u597d\uff0c\u6709\u4f46\u662f\u8981\u6c42\u9ad8\uff0c\u5c0f\u5382\u8981\u6c42\u4f4e\u4e9b\u4f46\u662f\u4e5f\u57fa\u672c\u8981\u6c42\u4f1a\u90e8\u7f72\u4e4b\u7c7b\u7684\u3002\u4e00\u822c\u5927\u5382\u7814\u7a76\u9662\u53ef\u4ee5\u641e\u8bad\u7ec3\u548c\u90e8\u7f72\u5206\u5f00\u641e\uff0c\u4e2d\u5c0f\u5382\u4e00\u822c\u8fd8\u662fc++\u548cpython\u90fd\u6709\u8981\u6c42\u7684\uff0c\u751a\u81f3\u5f88\u591a\u5927\u5382\u4e5f\u662f\u540c\u65f6\u8981\u6c42\u8bad\u7ec3\u90e8\u7f72\u4e00\u4e2a\u4eba\u5305\u3002\u6240\u4ee5\uff0c\u505a\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u7684\uff0c\u5bf9c++\u6709\u4e9b\u8981\u6c42\uff0c\u6216\u8005\u8bf4\u4f1a\u90e8\u7f72\u8c03\u4f18\u786e\u5b9e\u5f88\u91cd\u8981\u4e86\u3002 Allent Dan\u5927\u4f6c\u5173\u4e8elibtorch\u7684\u4e00\u4e9b\u7f16\u7a0b\u7ecf\u9a8c\uff0c\u603b\u7ed3\u4e0b\u5427\uff1a libtorch\u6709\u8bb8\u591a\u5751\uff0c\u6bd4\u5982sequential in sequential\u4f1a\u62a5\u9519\uff0c\u800c\u4e14\u89e3\u51b3\u4e0d\u4e86\uff0c\u9664\u975e\u53e6\u5199\u4e2astack\u7684sequential\u7c7b\u2026.\u6bd4\u5982CPU\u6bd4python\u6709\u65f6\u5019\u8fd8\u8981\u6162\u4e00\u4e9b\u2026\u867d\u7136GPU\u4e00\u822c\u4f1a\u5feb\u4e2a\u4e09\u6210\u3002 libtorch\u4e2d\u7684sequential\uff0c\u4e0d\u80fd\u5806\u53e0std::vector\\torch::Tensor\\\u4e3a\u8f93\u5165\u7684\u6a21\u5757\uff0c\u6bd4\u5982yolov5\u6a21\u578b\uff0c\u5c31\u6574\u4e86\u4e2aConCat\u6a21\u5757\uff0clibtorch\u91cc\u9762\u6ca1\u6cd5\u7528\u554a\u3002\u4e00\u65f6\u95f4\u4e0d\u77e5\u9053\u8be5\u602ayolov5\u4f5c\u8005\u4ee3\u7801\u89c4\u8303\u5dee\u597d\uff0c\u8fd8\u662f\u602alibtorch\u5783\u573e\u597d\uff0c\u8fd8\u662f\u602a\u81ea\u5df1\u6ca1\u8d44\u6e90\u597d\u3002\u6240\u4ee5\u6700\u540e\u6574\u4e86\u4e2ayolov4_tiny\u3002 libtorch\u8fd8\u662f\u6709\u8bb8\u591a\u5f85\u4f18\u5316\u7684\u4e1c\u897f\u7684\uff0c\u6bd4\u5982\u63d0\u901f\u5565\u7684\uff0c\u540e\u9762\u8981\u662f\u80fd\u6574\u4e2aint8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u4e00\u4e2aapi\u8c03\u7528\uff0c\u53ef\u80fdTensorRT\u7684\u5e02\u573a\u4efd\u989d\u53c8\u5f97\u7f29\u5c0f\u4e86\u3002\u5f53\u7136\uff0c\u5e94\u8be5\u6ca1\u6709TensorRT\u63d0\u901f\u90a3\u4e48\u591a\uff0c\u6bd5\u7adf\u4e0d\u53ef\u80fd\u6bd4\u4eba\u5bb6\u66f4\u4e86\u89e3Nvidia\u663e\u5361\u4e86\u2026 \u6709\u6761\u4ef6\uff0c\u8fd8\u662f\u81ea\u5df1\u6574\u4e2a\u914d\u5957\u7684python\u6a21\u578b\uff0c\u7136\u540e\u81ea\u5df1\u8bad\u7ec3\u4e2a\u9884\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u6bd4\u5230\u5904\u627e\u5408\u9002\u7684\u5f00\u6e90\u9879\u76ee\u53ca\u6743\u91cd\u9999\u591a\u4e86\u3002\u5f53\u7136\uff0c\u8981\u662f\u771f\u7684\u7cbe\u529b\u597d\uff0c\u81ea\u5df1\u4ece\u5934\u8bad\u7ec3libtorch\u7684\u6a21\u578b\u4e5f\u672a\u5c1d\u4e0d\u53ef\u554a\u7b52\u5b50\u4eec\u3002\u6216\u8bb8\u7b49\u6211\u9879\u76ee\u5b8c\u5584\u8d77\u6765\uff0c\u4e5f\u53ef\u4ee5\u5b9e\u73b0libtorch\u4ece\u5934\u8bad\u7ec3\u5427\uff0c\u9700\u8981\u52a0\u8bb8\u591a\u4e1c\u897f\uff0c\u52a0\u8bb8\u591a\u6570\u636e\u589e\u5f3a\uff0c\u4e00\u4e9b\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u4e5f\u8981\u81ea\u5df1\u5b9e\u73b0\uff0c\u52a0\u4e0a\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u5e94\u8be5\u8fd8\u662f\u53ef\u4ee5\u7684\u3002\u6bd5\u7adflibtorch\u63a5\u8fc7\u4e86caffe\u7684\u5927\u65d7\uff0c\u800c\u4e14\u6709\u8138\u4e66\u5927\u5382\u5728\u7ef4\u62a4\u3002 \u8bf4\u5230\u5e95\uff0c\u5c31\u662f\u8d44\u6e90\u7ed9\u591f\uff0c\u82b1\u94b1\u94fa\u4eba\uff0c\u5199\u4e2alibtorch++\u90fd\u53ef\u4ee5\u5b9e\u73b0\u2026\u60f3\u5fc5\u8bb8\u591a\u516c\u53f8\u5e94\u8be5\u5c31\u662f\u8fd9\u6837\u5e72\u7684\uff0c\u76ee\u524d\u4e5f\u5728\u6109\u5feb\u878d\u8d44\u5565\u7684\u4e86\u3002 \u6574\u4f53\u6765\u8bf4\u5982\u679c\u4f60\u662f\u7528Pytorch\u7684\u6a21\u578b\u5728C++\u90e8\u7f72\u65f6\u9009\u62e9\u7684\u65b9\u6848\u53ea\u80fd\u662flibtorch\u6216TensorRT,\u800c\u5bf9\u4e8elibtorch\uff0c Facebook\u5df2\u7ecf\u63d0\u4f9b\u4e86\u5f88\u597d\u7684\u9884\u7f16\u8bd1\u7684C++\u5e93\u7ed9\u6211\u4eec\uff0c\u4e0d\u50cfTensorFlow\uff0c\u4f60\u9700\u8981\u81ea\u5df1\u7f16\u8bd1C++\u7684\u5e93\uff0c\u8fd9\u6837\u5f88\u65b9\u4fbf\u5728C++\u4e2d\u4f7f\u7528\u3002","title":"\u7b2c\u4e5d\u7ae0 \u603b\u7ed3\u5c55\u671b"},{"location":"chapter8/#_1","text":"\u9996\u9009\u8981\u8bf4\u660e\u7684\u662f\u8be5\u6559\u7a0b\u662f\u5bf9 Allent Dan\u5927\u4f6c\u5f00\u6e90\u7684libtorch\u6559\u7a0b\u7684\u4e00\u4e2a\u6574\u7406\uff0c\u5176\u5f00\u6e90\u4e86\u5927\u91cf\u7684libtorch\u5728C++\u4e0b\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u65b9\u6cd5\u3002\u5176GitHub\u5730\u5740\u4e3a\uff1a https://github.com/AllentDan \u5efa\u8bae\u5927\u5bb6\u770b\u5e76\u7ed9Star\uff1a https://github.com/AllentDan/LibtorchTutorials https://github.com/AllentDan/LibtorchDetection https://github.com/AllentDan/LibtorchSegmentation \u73b0\u5728\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u7684\u5c97\u4f4d\uff0c\u5c24\u5176\u662fCV\u5c97\uff0cpython\u7684HC\u771f\u7684\u4e0d\u591a\uff0c\u5927\u5382\u8fd8\u597d\uff0c\u6709\u4f46\u662f\u8981\u6c42\u9ad8\uff0c\u5c0f\u5382\u8981\u6c42\u4f4e\u4e9b\u4f46\u662f\u4e5f\u57fa\u672c\u8981\u6c42\u4f1a\u90e8\u7f72\u4e4b\u7c7b\u7684\u3002\u4e00\u822c\u5927\u5382\u7814\u7a76\u9662\u53ef\u4ee5\u641e\u8bad\u7ec3\u548c\u90e8\u7f72\u5206\u5f00\u641e\uff0c\u4e2d\u5c0f\u5382\u4e00\u822c\u8fd8\u662fc++\u548cpython\u90fd\u6709\u8981\u6c42\u7684\uff0c\u751a\u81f3\u5f88\u591a\u5927\u5382\u4e5f\u662f\u540c\u65f6\u8981\u6c42\u8bad\u7ec3\u90e8\u7f72\u4e00\u4e2a\u4eba\u5305\u3002\u6240\u4ee5\uff0c\u505a\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u7684\uff0c\u5bf9c++\u6709\u4e9b\u8981\u6c42\uff0c\u6216\u8005\u8bf4\u4f1a\u90e8\u7f72\u8c03\u4f18\u786e\u5b9e\u5f88\u91cd\u8981\u4e86\u3002 Allent Dan\u5927\u4f6c\u5173\u4e8elibtorch\u7684\u4e00\u4e9b\u7f16\u7a0b\u7ecf\u9a8c\uff0c\u603b\u7ed3\u4e0b\u5427\uff1a libtorch\u6709\u8bb8\u591a\u5751\uff0c\u6bd4\u5982sequential in sequential\u4f1a\u62a5\u9519\uff0c\u800c\u4e14\u89e3\u51b3\u4e0d\u4e86\uff0c\u9664\u975e\u53e6\u5199\u4e2astack\u7684sequential\u7c7b\u2026.\u6bd4\u5982CPU\u6bd4python\u6709\u65f6\u5019\u8fd8\u8981\u6162\u4e00\u4e9b\u2026\u867d\u7136GPU\u4e00\u822c\u4f1a\u5feb\u4e2a\u4e09\u6210\u3002 libtorch\u4e2d\u7684sequential\uff0c\u4e0d\u80fd\u5806\u53e0std::vector\\torch::Tensor\\\u4e3a\u8f93\u5165\u7684\u6a21\u5757\uff0c\u6bd4\u5982yolov5\u6a21\u578b\uff0c\u5c31\u6574\u4e86\u4e2aConCat\u6a21\u5757\uff0clibtorch\u91cc\u9762\u6ca1\u6cd5\u7528\u554a\u3002\u4e00\u65f6\u95f4\u4e0d\u77e5\u9053\u8be5\u602ayolov5\u4f5c\u8005\u4ee3\u7801\u89c4\u8303\u5dee\u597d\uff0c\u8fd8\u662f\u602alibtorch\u5783\u573e\u597d\uff0c\u8fd8\u662f\u602a\u81ea\u5df1\u6ca1\u8d44\u6e90\u597d\u3002\u6240\u4ee5\u6700\u540e\u6574\u4e86\u4e2ayolov4_tiny\u3002 libtorch\u8fd8\u662f\u6709\u8bb8\u591a\u5f85\u4f18\u5316\u7684\u4e1c\u897f\u7684\uff0c\u6bd4\u5982\u63d0\u901f\u5565\u7684\uff0c\u540e\u9762\u8981\u662f\u80fd\u6574\u4e2aint8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u4e00\u4e2aapi\u8c03\u7528\uff0c\u53ef\u80fdTensorRT\u7684\u5e02\u573a\u4efd\u989d\u53c8\u5f97\u7f29\u5c0f\u4e86\u3002\u5f53\u7136\uff0c\u5e94\u8be5\u6ca1\u6709TensorRT\u63d0\u901f\u90a3\u4e48\u591a\uff0c\u6bd5\u7adf\u4e0d\u53ef\u80fd\u6bd4\u4eba\u5bb6\u66f4\u4e86\u89e3Nvidia\u663e\u5361\u4e86\u2026 \u6709\u6761\u4ef6\uff0c\u8fd8\u662f\u81ea\u5df1\u6574\u4e2a\u914d\u5957\u7684python\u6a21\u578b\uff0c\u7136\u540e\u81ea\u5df1\u8bad\u7ec3\u4e2a\u9884\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u6bd4\u5230\u5904\u627e\u5408\u9002\u7684\u5f00\u6e90\u9879\u76ee\u53ca\u6743\u91cd\u9999\u591a\u4e86\u3002\u5f53\u7136\uff0c\u8981\u662f\u771f\u7684\u7cbe\u529b\u597d\uff0c\u81ea\u5df1\u4ece\u5934\u8bad\u7ec3libtorch\u7684\u6a21\u578b\u4e5f\u672a\u5c1d\u4e0d\u53ef\u554a\u7b52\u5b50\u4eec\u3002\u6216\u8bb8\u7b49\u6211\u9879\u76ee\u5b8c\u5584\u8d77\u6765\uff0c\u4e5f\u53ef\u4ee5\u5b9e\u73b0libtorch\u4ece\u5934\u8bad\u7ec3\u5427\uff0c\u9700\u8981\u52a0\u8bb8\u591a\u4e1c\u897f\uff0c\u52a0\u8bb8\u591a\u6570\u636e\u589e\u5f3a\uff0c\u4e00\u4e9b\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u4e5f\u8981\u81ea\u5df1\u5b9e\u73b0\uff0c\u52a0\u4e0a\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u5e94\u8be5\u8fd8\u662f\u53ef\u4ee5\u7684\u3002\u6bd5\u7adflibtorch\u63a5\u8fc7\u4e86caffe\u7684\u5927\u65d7\uff0c\u800c\u4e14\u6709\u8138\u4e66\u5927\u5382\u5728\u7ef4\u62a4\u3002 \u8bf4\u5230\u5e95\uff0c\u5c31\u662f\u8d44\u6e90\u7ed9\u591f\uff0c\u82b1\u94b1\u94fa\u4eba\uff0c\u5199\u4e2alibtorch++\u90fd\u53ef\u4ee5\u5b9e\u73b0\u2026\u60f3\u5fc5\u8bb8\u591a\u516c\u53f8\u5e94\u8be5\u5c31\u662f\u8fd9\u6837\u5e72\u7684\uff0c\u76ee\u524d\u4e5f\u5728\u6109\u5feb\u878d\u8d44\u5565\u7684\u4e86\u3002 \u6574\u4f53\u6765\u8bf4\u5982\u679c\u4f60\u662f\u7528Pytorch\u7684\u6a21\u578b\u5728C++\u90e8\u7f72\u65f6\u9009\u62e9\u7684\u65b9\u6848\u53ea\u80fd\u662flibtorch\u6216TensorRT,\u800c\u5bf9\u4e8elibtorch\uff0c Facebook\u5df2\u7ecf\u63d0\u4f9b\u4e86\u5f88\u597d\u7684\u9884\u7f16\u8bd1\u7684C++\u5e93\u7ed9\u6211\u4eec\uff0c\u4e0d\u50cfTensorFlow\uff0c\u4f60\u9700\u8981\u81ea\u5df1\u7f16\u8bd1C++\u7684\u5e93\uff0c\u8fd9\u6837\u5f88\u65b9\u4fbf\u5728C++\u4e2d\u4f7f\u7528\u3002","title":"\u7b2c\u4e5d\u7ae0 \u603b\u7ed3\u548c\u5c55\u671b"},{"location":"chapter9/","text":"\u7b2c8\u7ae0 libtorch\u90e8\u7f72\u4f8b\u5b50 1.\u56fe\u50cf\u5206\u7c7b\u7684\u4f8b\u5b50 MobileNet v3 \u5173\u4e8eMobileNet V3\u53ef\u4ee5\u53c2\u8003 https://github.com/DataXujing/MobileNet_V3_pytorch \u9996\u5148\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8f6ctorchscript from __future__ import print_function, division import os import torch from torch import nn,optim import torch.nn.functional as F import pandas as pd import numpy as np from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils import torch from mobilenetv3 import * # model from data_pro import * import cv2 #----------------model define----------------- model = mobilenetv3(n_class=2, input_size=224, mode='large') state_dict = torch.load(\"./checkpoint/20201111/MobileNet_v3_20201111_300_0.9659863945578231.pth\") model.load_state_dict(state_dict) model.to(torch.device(\"cpu\")) model.eval() # ---------------------------------------- var=torch.ones((1,3,224,224)) traced_script_module = torch.jit.trace(model, var) traced_script_module.save(\"MobileNet_v3_large.pt\") libtorch\u8c03\u7528\u6a21\u578b\u8bc6\u522b // mobilenetv3.cpp #include <torch/torch.h> #include <torch/script.h> #include <opencv2/opencv.hpp> #include <iostream> using namespace cv; using namespace std; int main() { //\u5b9a\u4e49\u4f7f\u7528cuda //auto device = torch::Device(torch::kCUDA, 0); //\u8bfb\u53d6\u56fe\u7247 vector<float> m = { 0.485, 0.456, 0.406 }; vector<float> v = { 0.229, 0.224, 0.225 }; auto mean = torch::from_blob(m.data(), { 1,3 }, torch::kFloat32); auto var = torch::from_blob(v.data(), { 1,3 }, torch::kFloat32); auto image = cv::imread(\".\\\\test.jpg\"); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(224, 224)); //\u8f6c\u6210\u5f20\u91cf auto input_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat32) / 225.0; //auto input_tensor_norm = (input_tensor - mean) / var; //\u52a0\u8f7d\u6a21\u578b auto model = torch::jit::load(\".\\\\model\\\\MobileNet_v3_large.pt\"); //model.to(device); model.eval(); //\u524d\u5411\u4f20\u64ad //auto output = model.forward({ input_tensor.to(device) }).toTensor(); auto output = model.forward({ input_tensor }).toTensor(); output = torch::softmax(output, 1); auto class_id = torch::argmax(output); auto prob = output.max(); std::cout << \"MobileNet v3\u8bc6\u522b\u7684Label\u4e3a\" << class_id << \"\u6982\u7387\u4e3a\uff1a\" << prob << std::endl; return 0; } 2.\u8bed\u4e49\u5206\u5272\u7684\u4f8b\u5b50 DeepLab V3+ \u5173\u4e8edeeplab v3+\u7684Pytorch\u7248\u672c\u7684\u7684\u5b9e\u73b0\u6211\u4eec\u53c2\u8003\u4e86\uff1a https://github.com/VainF/DeepLabV3Plus-Pytorch \u6a21\u578b\u7ed3\u6784\u4e3a: \u4e0b\u8f7d\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u5c06\u6a21\u578b\u8f6c\u4e3atorchscipt import network import utils import os import random import argparse import numpy as np from torch.utils import data from datasets import VOCSegmentation, Cityscapes from utils import ext_transforms as et from metrics import StreamSegMetrics import torch import torch.nn as nn from utils.visualizer import Visualizer from torchvision import transforms import cv2 from PIL import Image import matplotlib import matplotlib.pyplot as plt # model # Set up model model_map = { 'deeplabv3_resnet50': network.deeplabv3_resnet50, 'deeplabv3plus_resnet50': network.deeplabv3plus_resnet50, 'deeplabv3_resnet101': network.deeplabv3_resnet101, 'deeplabv3plus_resnet101': network.deeplabv3plus_resnet101, 'deeplabv3_mobilenet': network.deeplabv3_mobilenet, 'deeplabv3plus_mobilenet': network.deeplabv3plus_mobilenet } model = model_map['deeplabv3_resnet50'](num_classes=21, output_stride=16) #network.convert_to_separable_conv(model.classifier) #utils.set_bn_momentum(model.backbone, momentum=0.01) model.load_state_dict( torch.load( \"../best_deeplabv3_resnet50_voc_os16.pth\", map_location='cpu' )['model_state'] ) model.eval() # print(model) val_transform = transforms.Compose([ transforms.Resize( (512,512) ), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) image = Image.open(\"../../test.jpg\").convert('RGB') image = val_transform(image).unsqueeze(0) outputs = model(image) preds = outputs.max(1)[1].detach().cpu().numpy() # model trace traced_script_module = torch.jit.trace(model, image) traced_script_module.save(\"deeplabv3.pt\") \u751f\u6210libtorch\u8c03\u7528\u7684\u6a21\u578b deeplabv3.pt \u7f16\u5199\u57fa\u4e8elibtorch\u7684\u6a21\u578b\u63a8\u65ad\u65b9\u6cd5 // deeplabv3plus.cpp : #include<opencv2/opencv.hpp> #include <torch/torch.h> #include <torch/script.h> #include <iostream> int main() { //\u5b9a\u4e49\u4f7f\u7528cuda //auto device = torch::Device(torch::kCUDA, 0); //\u52a0\u8f7d\u6a21\u578b auto model = torch::jit::load(\".\\\\model\\\\deeplabv3.pt\"); //model.to(device); model.eval(); std::cout << \"deeplab\u6a21\u578b\u52a0\u8f7d\u5b8c\u6bd5\uff01\" << std::endl; //\u8bfb\u53d6\u56fe\u7247 auto image = cv::imread(\".\\\\23_image.png\"); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(512, 512),cv::INTER_LINEAR); //\u8f6c\u6210\u5f20\u91cf auto input_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat32) / 225.0; // \u5f20\u91cf\u7684\u6807\u51c6\u5316 // todo std::cout << input_tensor.sizes() << std::endl; std::vector<float> m = { 0.485, 0.456, 0.406 }; std::vector<float> v = { 0.229, 0.224, 0.225 }; auto mean = torch::from_blob(m.data(), { 1,3,1,1 }, torch::kFloat32); auto var = torch::from_blob(v.data(), { 1,3,1,1 }, torch::kFloat32); input_tensor = (input_tensor - mean) / var; //\u524d\u5411\u4f20\u64ad //auto output = model.forward({ input_tensor.to(device) }).toTensor(); auto output = model.forward({ input_tensor }).toTensor(); std::tuple<at::Tensor,at::Tensor> preds = torch::max(output, 1); std::cout << output.sizes() << std::endl; std::cout << std::get<0>(preds).sizes() << std::endl; // tensor to cvmat auto pred_img = std::get<1>(preds).mul(255).to(torch::kU8); cv::Mat imgbin(cv::Size(512,512), CV_8U, pred_img.data_ptr()); cv::imwrite(\"seg_res.jpg\", imgbin); cv::namedWindow(\"segment-result\", 0); cv::imshow(\"segment-result\", imgbin); cv::waitKey(0); return 0; } \u6d4b\u8bd5\u7ed3\u679c\u5c55\u793a \u8bc6\u522b\u7ed3\u679c\u662f\u6b63\u786e\u7684\u3002 3.\u76ee\u6807\u68c0\u6d4b\u7684\u4f8b\u5b50 YOLOv5x \u6211\u4eec\u4f7f\u7528\u81ea\u8bad\u7ec3\u7684YOLOv5x 5.0\u7248\u672c\u7684\u4ee3\u7801\uff0c\u8be6\u7ec6\u7684\u4ee3\u7801\u53ef\u4ee5\u53c2\u8003 https://github.com/ultralytics/yolov5 \u751f\u6210torchscript\u6a21\u578b python ./model/export.py --grid \u524d\u5904\u7406 // pre process at::Tensor imagpro(std::string imgpath=\"./test.jpg\") { //\u8bfb\u53d6\u56fe\u7247 auto image = cv::imread(imgpath); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(640, 640)); cv::cvtColor(image, image, cv::COLOR_BGR2RGB); //\u8f6c\u6210\u5f20\u91cf at::Tensor imgTensor = torch::from_blob(image.data, { image.rows, image.cols,3 }, torch::kByte); imgTensor = imgTensor.permute({ 2,0,1 }); imgTensor = imgTensor.toType(torch::kFloat); imgTensor = imgTensor.div(255); imgTensor = imgTensor.unsqueeze(0); return imgTensor; } \u540e\u5904\u7406 // post process: NMS std::vector<torch::Tensor> non_max_suppression(torch::Tensor preds, float score_thresh = 0.25, float iou_thresh = 0.45) { std::vector<torch::Tensor> output; for (size_t i = 0; i < preds.sizes()[0]; ++i) { torch::Tensor pred = preds.select(0, i); // Filter by scores torch::Tensor scores = pred.select(1, 4) * std::get<0>(torch::max(pred.slice(1, 5, pred.sizes()[1]), 1)); pred = torch::index_select(pred, 0, torch::nonzero(scores > score_thresh).select(1, 0)); if (pred.sizes()[0] == 0) continue; // (center_x, center_y, w, h) to (left, top, right, bottom) pred.select(1, 0) = pred.select(1, 0) - pred.select(1, 2) / 2; pred.select(1, 1) = pred.select(1, 1) - pred.select(1, 3) / 2; pred.select(1, 2) = pred.select(1, 0) + pred.select(1, 2); pred.select(1, 3) = pred.select(1, 1) + pred.select(1, 3); // Computing scores and classes std::tuple<torch::Tensor, torch::Tensor> max_tuple = torch::max(pred.slice(1, 5, pred.sizes()[1]), 1); pred.select(1, 4) = pred.select(1, 4) * std::get<0>(max_tuple); pred.select(1, 5) = std::get<1>(max_tuple); torch::Tensor dets = pred.slice(1, 0, 6); torch::Tensor keep = torch::empty({ dets.sizes()[0] }); torch::Tensor areas = (dets.select(1, 3) - dets.select(1, 1)) * (dets.select(1, 2) - dets.select(1, 0)); std::tuple<torch::Tensor, torch::Tensor> indexes_tuple = torch::sort(dets.select(1, 4), 0, 1); torch::Tensor v = std::get<0>(indexes_tuple); torch::Tensor indexes = std::get<1>(indexes_tuple); int count = 0; while (indexes.sizes()[0] > 0) { keep[count] = (indexes[0].item().toInt()); count += 1; // Computing overlaps torch::Tensor lefts = torch::empty(indexes.sizes()[0] - 1); torch::Tensor tops = torch::empty(indexes.sizes()[0] - 1); torch::Tensor rights = torch::empty(indexes.sizes()[0] - 1); torch::Tensor bottoms = torch::empty(indexes.sizes()[0] - 1); torch::Tensor widths = torch::empty(indexes.sizes()[0] - 1); torch::Tensor heights = torch::empty(indexes.sizes()[0] - 1); for (size_t i = 0; i < indexes.sizes()[0] - 1; ++i) { lefts[i] = std::max(dets[indexes[0]][0].item().toFloat(), dets[indexes[i + 1]][0].item().toFloat()); tops[i] = std::max(dets[indexes[0]][1].item().toFloat(), dets[indexes[i + 1]][1].item().toFloat()); rights[i] = std::min(dets[indexes[0]][2].item().toFloat(), dets[indexes[i + 1]][2].item().toFloat()); bottoms[i] = std::min(dets[indexes[0]][3].item().toFloat(), dets[indexes[i + 1]][3].item().toFloat()); widths[i] = std::max(float(0), rights[i].item().toFloat() - lefts[i].item().toFloat()); heights[i] = std::max(float(0), bottoms[i].item().toFloat() - tops[i].item().toFloat()); } torch::Tensor overlaps = widths * heights; // FIlter by IOUs torch::Tensor ious = overlaps / (areas.select(0, indexes[0].item().toInt()) + torch::index_select(areas, 0, indexes.slice(0, 1, indexes.sizes()[0])) - overlaps); indexes = torch::index_select(indexes, 0, torch::nonzero(ious <= iou_thresh).select(1, 0) + 1); } keep = keep.toType(torch::kInt64); output.push_back(torch::index_select(dets, 0, keep.slice(0, 0, count))); } return output; } \u53ef\u89c6\u5316plot box // plot box void plotbox(std::vector<torch::Tensor> dets,cv::Mat image, std::vector<std::string> classnames) { if (dets.size() > 0) { // Visualize result for (size_t i = 0; i < dets[0].sizes()[0]; ++i) { float left = dets[0][i][0].item().toFloat() * image.cols / 640; float top = dets[0][i][1].item().toFloat() * image.rows / 640; float right = dets[0][i][2].item().toFloat() * image.cols / 640; float bottom = dets[0][i][3].item().toFloat() * image.rows / 640; float score = dets[0][i][4].item().toFloat(); int classID = dets[0][i][5].item().toInt(); cv::rectangle(image, cv::Rect(left, top, (right - left), (bottom - top)), cv::Scalar(0, 255, 0), 2); cv::putText(image, classnames[classID] + \": \" + cv::format(\"%.2f\", score), cv::Point(left, top), cv::FONT_HERSHEY_SIMPLEX, (right - left) / 200, cv::Scalar(0, 255, 0), 2); } } cv::imshow(\"YOLOv5-Res\", image); cv::waitKey(0); } \u4e3b\u51fd\u6570 int main() { // eus // Loading Model torch::jit::script::Module module = torch::jit::load(\"./model/best.torchscript.pt\"); std::cout << \"[info] \u6a21\u578b\u52a0\u8f7d\u5b8c\u6bd5\" << std::endl; //\u52a0\u8f7d\u7c7b\u522b std::vector<std::string> classnames; std::ifstream f(\"./model/coco.names\"); std::string name = \"\"; while (std::getline(f, name)) { classnames.push_back(name); } std::cout << \"[info] \u7c7b\u522b\u540d\u79f0: \" << std::endl; std::cout << classnames << std::endl; // \u524d\u5904\u7406 at::Tensor inputtensor = imagpro(\"./test.jpg\"); // yolov5 \u6a21\u578b\u8bc6\u522b torch::Tensor preds = module.forward({ inputtensor }).toTuple()->elements()[0].toTensor(); // \u540e\u5904\u7406 std::vector<torch::Tensor> dets = non_max_suppression(preds, 0.25, 0.45); //plotbox cv::Mat image = cv::imread(\"./test.jpg\"); plotbox(dets, image, classnames); return 0; } \u8bc6\u522b\u7ed3\u679c \u53ef\u4ee5\u770b\u5230\u8bc6\u522b\u662f\u6b63\u5e38\u7684\uff01 \u7efc\u4e0a\uff0c\u6211\u4eec\u5b8c\u6210\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5e38\u7528\u7684\u5206\u7c7b\uff0c\u5206\u5272\uff0c\u68c0\u6d4b\u7f51\u7edc\u7684libtorch\u7684C++\u90e8\u7f72\u3002","title":"\u7b2c\u516b\u7ae0 libtorch\u90e8\u7f72\u4f8b\u5b50"},{"location":"chapter9/#8-libtorch","text":"","title":"\u7b2c8\u7ae0 libtorch\u90e8\u7f72\u4f8b\u5b50"},{"location":"chapter9/#1-mobilenet-v3","text":"\u5173\u4e8eMobileNet V3\u53ef\u4ee5\u53c2\u8003 https://github.com/DataXujing/MobileNet_V3_pytorch \u9996\u5148\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8f6ctorchscript from __future__ import print_function, division import os import torch from torch import nn,optim import torch.nn.functional as F import pandas as pd import numpy as np from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils import torch from mobilenetv3 import * # model from data_pro import * import cv2 #----------------model define----------------- model = mobilenetv3(n_class=2, input_size=224, mode='large') state_dict = torch.load(\"./checkpoint/20201111/MobileNet_v3_20201111_300_0.9659863945578231.pth\") model.load_state_dict(state_dict) model.to(torch.device(\"cpu\")) model.eval() # ---------------------------------------- var=torch.ones((1,3,224,224)) traced_script_module = torch.jit.trace(model, var) traced_script_module.save(\"MobileNet_v3_large.pt\") libtorch\u8c03\u7528\u6a21\u578b\u8bc6\u522b // mobilenetv3.cpp #include <torch/torch.h> #include <torch/script.h> #include <opencv2/opencv.hpp> #include <iostream> using namespace cv; using namespace std; int main() { //\u5b9a\u4e49\u4f7f\u7528cuda //auto device = torch::Device(torch::kCUDA, 0); //\u8bfb\u53d6\u56fe\u7247 vector<float> m = { 0.485, 0.456, 0.406 }; vector<float> v = { 0.229, 0.224, 0.225 }; auto mean = torch::from_blob(m.data(), { 1,3 }, torch::kFloat32); auto var = torch::from_blob(v.data(), { 1,3 }, torch::kFloat32); auto image = cv::imread(\".\\\\test.jpg\"); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(224, 224)); //\u8f6c\u6210\u5f20\u91cf auto input_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat32) / 225.0; //auto input_tensor_norm = (input_tensor - mean) / var; //\u52a0\u8f7d\u6a21\u578b auto model = torch::jit::load(\".\\\\model\\\\MobileNet_v3_large.pt\"); //model.to(device); model.eval(); //\u524d\u5411\u4f20\u64ad //auto output = model.forward({ input_tensor.to(device) }).toTensor(); auto output = model.forward({ input_tensor }).toTensor(); output = torch::softmax(output, 1); auto class_id = torch::argmax(output); auto prob = output.max(); std::cout << \"MobileNet v3\u8bc6\u522b\u7684Label\u4e3a\" << class_id << \"\u6982\u7387\u4e3a\uff1a\" << prob << std::endl; return 0; }","title":"1.\u56fe\u50cf\u5206\u7c7b\u7684\u4f8b\u5b50 MobileNet v3"},{"location":"chapter9/#2-deeplab-v3","text":"\u5173\u4e8edeeplab v3+\u7684Pytorch\u7248\u672c\u7684\u7684\u5b9e\u73b0\u6211\u4eec\u53c2\u8003\u4e86\uff1a https://github.com/VainF/DeepLabV3Plus-Pytorch \u6a21\u578b\u7ed3\u6784\u4e3a: \u4e0b\u8f7d\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u5c06\u6a21\u578b\u8f6c\u4e3atorchscipt import network import utils import os import random import argparse import numpy as np from torch.utils import data from datasets import VOCSegmentation, Cityscapes from utils import ext_transforms as et from metrics import StreamSegMetrics import torch import torch.nn as nn from utils.visualizer import Visualizer from torchvision import transforms import cv2 from PIL import Image import matplotlib import matplotlib.pyplot as plt # model # Set up model model_map = { 'deeplabv3_resnet50': network.deeplabv3_resnet50, 'deeplabv3plus_resnet50': network.deeplabv3plus_resnet50, 'deeplabv3_resnet101': network.deeplabv3_resnet101, 'deeplabv3plus_resnet101': network.deeplabv3plus_resnet101, 'deeplabv3_mobilenet': network.deeplabv3_mobilenet, 'deeplabv3plus_mobilenet': network.deeplabv3plus_mobilenet } model = model_map['deeplabv3_resnet50'](num_classes=21, output_stride=16) #network.convert_to_separable_conv(model.classifier) #utils.set_bn_momentum(model.backbone, momentum=0.01) model.load_state_dict( torch.load( \"../best_deeplabv3_resnet50_voc_os16.pth\", map_location='cpu' )['model_state'] ) model.eval() # print(model) val_transform = transforms.Compose([ transforms.Resize( (512,512) ), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) image = Image.open(\"../../test.jpg\").convert('RGB') image = val_transform(image).unsqueeze(0) outputs = model(image) preds = outputs.max(1)[1].detach().cpu().numpy() # model trace traced_script_module = torch.jit.trace(model, image) traced_script_module.save(\"deeplabv3.pt\") \u751f\u6210libtorch\u8c03\u7528\u7684\u6a21\u578b deeplabv3.pt \u7f16\u5199\u57fa\u4e8elibtorch\u7684\u6a21\u578b\u63a8\u65ad\u65b9\u6cd5 // deeplabv3plus.cpp : #include<opencv2/opencv.hpp> #include <torch/torch.h> #include <torch/script.h> #include <iostream> int main() { //\u5b9a\u4e49\u4f7f\u7528cuda //auto device = torch::Device(torch::kCUDA, 0); //\u52a0\u8f7d\u6a21\u578b auto model = torch::jit::load(\".\\\\model\\\\deeplabv3.pt\"); //model.to(device); model.eval(); std::cout << \"deeplab\u6a21\u578b\u52a0\u8f7d\u5b8c\u6bd5\uff01\" << std::endl; //\u8bfb\u53d6\u56fe\u7247 auto image = cv::imread(\".\\\\23_image.png\"); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(512, 512),cv::INTER_LINEAR); //\u8f6c\u6210\u5f20\u91cf auto input_tensor = torch::from_blob(image.data, { image.rows, image.cols, 3 }, torch::kByte).permute({ 2, 0, 1 }).unsqueeze(0).to(torch::kFloat32) / 225.0; // \u5f20\u91cf\u7684\u6807\u51c6\u5316 // todo std::cout << input_tensor.sizes() << std::endl; std::vector<float> m = { 0.485, 0.456, 0.406 }; std::vector<float> v = { 0.229, 0.224, 0.225 }; auto mean = torch::from_blob(m.data(), { 1,3,1,1 }, torch::kFloat32); auto var = torch::from_blob(v.data(), { 1,3,1,1 }, torch::kFloat32); input_tensor = (input_tensor - mean) / var; //\u524d\u5411\u4f20\u64ad //auto output = model.forward({ input_tensor.to(device) }).toTensor(); auto output = model.forward({ input_tensor }).toTensor(); std::tuple<at::Tensor,at::Tensor> preds = torch::max(output, 1); std::cout << output.sizes() << std::endl; std::cout << std::get<0>(preds).sizes() << std::endl; // tensor to cvmat auto pred_img = std::get<1>(preds).mul(255).to(torch::kU8); cv::Mat imgbin(cv::Size(512,512), CV_8U, pred_img.data_ptr()); cv::imwrite(\"seg_res.jpg\", imgbin); cv::namedWindow(\"segment-result\", 0); cv::imshow(\"segment-result\", imgbin); cv::waitKey(0); return 0; } \u6d4b\u8bd5\u7ed3\u679c\u5c55\u793a \u8bc6\u522b\u7ed3\u679c\u662f\u6b63\u786e\u7684\u3002","title":"2.\u8bed\u4e49\u5206\u5272\u7684\u4f8b\u5b50 DeepLab V3+"},{"location":"chapter9/#3-yolov5x","text":"\u6211\u4eec\u4f7f\u7528\u81ea\u8bad\u7ec3\u7684YOLOv5x 5.0\u7248\u672c\u7684\u4ee3\u7801\uff0c\u8be6\u7ec6\u7684\u4ee3\u7801\u53ef\u4ee5\u53c2\u8003 https://github.com/ultralytics/yolov5 \u751f\u6210torchscript\u6a21\u578b python ./model/export.py --grid \u524d\u5904\u7406 // pre process at::Tensor imagpro(std::string imgpath=\"./test.jpg\") { //\u8bfb\u53d6\u56fe\u7247 auto image = cv::imread(imgpath); //\u7f29\u653e\u81f3\u6307\u5b9a\u5927\u5c0f cv::resize(image, image, cv::Size(640, 640)); cv::cvtColor(image, image, cv::COLOR_BGR2RGB); //\u8f6c\u6210\u5f20\u91cf at::Tensor imgTensor = torch::from_blob(image.data, { image.rows, image.cols,3 }, torch::kByte); imgTensor = imgTensor.permute({ 2,0,1 }); imgTensor = imgTensor.toType(torch::kFloat); imgTensor = imgTensor.div(255); imgTensor = imgTensor.unsqueeze(0); return imgTensor; } \u540e\u5904\u7406 // post process: NMS std::vector<torch::Tensor> non_max_suppression(torch::Tensor preds, float score_thresh = 0.25, float iou_thresh = 0.45) { std::vector<torch::Tensor> output; for (size_t i = 0; i < preds.sizes()[0]; ++i) { torch::Tensor pred = preds.select(0, i); // Filter by scores torch::Tensor scores = pred.select(1, 4) * std::get<0>(torch::max(pred.slice(1, 5, pred.sizes()[1]), 1)); pred = torch::index_select(pred, 0, torch::nonzero(scores > score_thresh).select(1, 0)); if (pred.sizes()[0] == 0) continue; // (center_x, center_y, w, h) to (left, top, right, bottom) pred.select(1, 0) = pred.select(1, 0) - pred.select(1, 2) / 2; pred.select(1, 1) = pred.select(1, 1) - pred.select(1, 3) / 2; pred.select(1, 2) = pred.select(1, 0) + pred.select(1, 2); pred.select(1, 3) = pred.select(1, 1) + pred.select(1, 3); // Computing scores and classes std::tuple<torch::Tensor, torch::Tensor> max_tuple = torch::max(pred.slice(1, 5, pred.sizes()[1]), 1); pred.select(1, 4) = pred.select(1, 4) * std::get<0>(max_tuple); pred.select(1, 5) = std::get<1>(max_tuple); torch::Tensor dets = pred.slice(1, 0, 6); torch::Tensor keep = torch::empty({ dets.sizes()[0] }); torch::Tensor areas = (dets.select(1, 3) - dets.select(1, 1)) * (dets.select(1, 2) - dets.select(1, 0)); std::tuple<torch::Tensor, torch::Tensor> indexes_tuple = torch::sort(dets.select(1, 4), 0, 1); torch::Tensor v = std::get<0>(indexes_tuple); torch::Tensor indexes = std::get<1>(indexes_tuple); int count = 0; while (indexes.sizes()[0] > 0) { keep[count] = (indexes[0].item().toInt()); count += 1; // Computing overlaps torch::Tensor lefts = torch::empty(indexes.sizes()[0] - 1); torch::Tensor tops = torch::empty(indexes.sizes()[0] - 1); torch::Tensor rights = torch::empty(indexes.sizes()[0] - 1); torch::Tensor bottoms = torch::empty(indexes.sizes()[0] - 1); torch::Tensor widths = torch::empty(indexes.sizes()[0] - 1); torch::Tensor heights = torch::empty(indexes.sizes()[0] - 1); for (size_t i = 0; i < indexes.sizes()[0] - 1; ++i) { lefts[i] = std::max(dets[indexes[0]][0].item().toFloat(), dets[indexes[i + 1]][0].item().toFloat()); tops[i] = std::max(dets[indexes[0]][1].item().toFloat(), dets[indexes[i + 1]][1].item().toFloat()); rights[i] = std::min(dets[indexes[0]][2].item().toFloat(), dets[indexes[i + 1]][2].item().toFloat()); bottoms[i] = std::min(dets[indexes[0]][3].item().toFloat(), dets[indexes[i + 1]][3].item().toFloat()); widths[i] = std::max(float(0), rights[i].item().toFloat() - lefts[i].item().toFloat()); heights[i] = std::max(float(0), bottoms[i].item().toFloat() - tops[i].item().toFloat()); } torch::Tensor overlaps = widths * heights; // FIlter by IOUs torch::Tensor ious = overlaps / (areas.select(0, indexes[0].item().toInt()) + torch::index_select(areas, 0, indexes.slice(0, 1, indexes.sizes()[0])) - overlaps); indexes = torch::index_select(indexes, 0, torch::nonzero(ious <= iou_thresh).select(1, 0) + 1); } keep = keep.toType(torch::kInt64); output.push_back(torch::index_select(dets, 0, keep.slice(0, 0, count))); } return output; } \u53ef\u89c6\u5316plot box // plot box void plotbox(std::vector<torch::Tensor> dets,cv::Mat image, std::vector<std::string> classnames) { if (dets.size() > 0) { // Visualize result for (size_t i = 0; i < dets[0].sizes()[0]; ++i) { float left = dets[0][i][0].item().toFloat() * image.cols / 640; float top = dets[0][i][1].item().toFloat() * image.rows / 640; float right = dets[0][i][2].item().toFloat() * image.cols / 640; float bottom = dets[0][i][3].item().toFloat() * image.rows / 640; float score = dets[0][i][4].item().toFloat(); int classID = dets[0][i][5].item().toInt(); cv::rectangle(image, cv::Rect(left, top, (right - left), (bottom - top)), cv::Scalar(0, 255, 0), 2); cv::putText(image, classnames[classID] + \": \" + cv::format(\"%.2f\", score), cv::Point(left, top), cv::FONT_HERSHEY_SIMPLEX, (right - left) / 200, cv::Scalar(0, 255, 0), 2); } } cv::imshow(\"YOLOv5-Res\", image); cv::waitKey(0); } \u4e3b\u51fd\u6570 int main() { // eus // Loading Model torch::jit::script::Module module = torch::jit::load(\"./model/best.torchscript.pt\"); std::cout << \"[info] \u6a21\u578b\u52a0\u8f7d\u5b8c\u6bd5\" << std::endl; //\u52a0\u8f7d\u7c7b\u522b std::vector<std::string> classnames; std::ifstream f(\"./model/coco.names\"); std::string name = \"\"; while (std::getline(f, name)) { classnames.push_back(name); } std::cout << \"[info] \u7c7b\u522b\u540d\u79f0: \" << std::endl; std::cout << classnames << std::endl; // \u524d\u5904\u7406 at::Tensor inputtensor = imagpro(\"./test.jpg\"); // yolov5 \u6a21\u578b\u8bc6\u522b torch::Tensor preds = module.forward({ inputtensor }).toTuple()->elements()[0].toTensor(); // \u540e\u5904\u7406 std::vector<torch::Tensor> dets = non_max_suppression(preds, 0.25, 0.45); //plotbox cv::Mat image = cv::imread(\"./test.jpg\"); plotbox(dets, image, classnames); return 0; } \u8bc6\u522b\u7ed3\u679c \u53ef\u4ee5\u770b\u5230\u8bc6\u522b\u662f\u6b63\u5e38\u7684\uff01 \u7efc\u4e0a\uff0c\u6211\u4eec\u5b8c\u6210\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5e38\u7528\u7684\u5206\u7c7b\uff0c\u5206\u5272\uff0c\u68c0\u6d4b\u7f51\u7edc\u7684libtorch\u7684C++\u90e8\u7f72\u3002","title":"3.\u76ee\u6807\u68c0\u6d4b\u7684\u4f8b\u5b50 YOLOv5x"}]}